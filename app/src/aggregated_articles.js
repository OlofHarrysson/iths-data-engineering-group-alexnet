// File generated with app/src/generateJsonData.js
export default [
  {
    "unique_id": "0a181817-68b5-59ef-9db7-c21097798ebc",
    "title": "If art is how we express our humanity, where does AI fit in?",
    "description": "MIT postdoc Ziv Epstein SM ’19, PhD ’23 discusses issues arising from the use of generative AI to make art and other media.",
    "link": "https://news.mit.edu/2023/generative-ai-art-expression-0615",
    "blog_text": "The rapid advance of artificial intelligence has generated a lot of buzz, with some predicting it will lead to an idyllic utopia and others warning it will bring the end of humanity. But speculation about where AI technology is going, while important, can also drown out important conversations about how we should be handling the AI technologies available today.\nOne such technology is generative AI, which can create content including text, images, audio, and video. Popular generative AIs like the chatbot ChatGPT generate conversational text based on training data taken from the internet.\nToday a group of 14 researchers from a number of organizations including MIT published a commentary article in Science that helps set the stage for discussions about generative AI’s immediate impact on creative work and society more broadly. The paper’s MIT-affiliated co-authors include Media Lab postdoc Ziv Epstein SM ’19, PhD ’23; Matt Groh SM ’19, PhD ’23; PhD students Rob Mahari ’17 and Hope Schroeder; and Professor Alex \"Sandy\" Pentland.\nMIT News spoke with Epstein, the lead author of the paper.\nQ: Why did you write this paper?\nA: Generative AI tools are doing things that even a few years ago we never thought would be possible. This raises a lot of fundamental questions about the creative process and the human’s role in creative production. Are we going to get automated out of jobs? How are we going to preserve the human aspect of creativity with all of these new technologies?\nThe complexity of black-box AI systems can make it hard for researchers and the broader public to understand what’s happening under the hood, and what the impacts of these tools on society will be. Many discussions about AI anthropomorphize the technology, implicitly suggesting these systems exhibit human-like intent, agency, or self-awareness. Even the term “artificial intelligence” reinforces these beliefs: ChatGPT uses first-person pronouns, and we say AIs “hallucinate.” These agentic roles we give AIs can undermine the credit to creators whose labor underlies the system’s outputs, and can deflect responsibility from the developers and decision makers when the systems cause harm.\nWe’re trying to build coalitions across academia and beyond to help think about the interdisciplinary connections and research areas necessary to grapple with the immediate dangers to humans coming from the deployment of these tools, such as disinformation, job displacement, and changes to legal structures and culture.\nQ: What do you see as the gaps in research around generative AI and art today?\nA: The way we talk about AI is broken in many ways. We need to understand how perceptions of the generative process affect attitudes toward outputs and authors, and also design the interfaces and systems in a way that is really transparent about the generative process and avoids some of these misleading interpretations. How do we talk about AI and how do these narratives cut along lines of power? As we outline in the article, there are these themes around AI’s impact that are important to consider: aesthetics and culture; legal aspects of ownership and credit; labor; and the impacts to the media ecosystem. For each of those we highlight the big open questions.\nWith aesthetics and culture, we’re considering how past art technologies can inform how we think about AI. For example, when photography was invented, some painters said it was “the end of art.” But instead it ended up being its own medium and eventually liberated painting from realism, giving rise to Impressionism and the modern art movement. We’re saying generative AI is a medium with its own affordances. The nature of art will evolve with that. How will artists and creators express their intent and style through this new medium?\nIssues around ownership and credit are tricky because we need copyright law that benefits creators, users, and society at large. Today’s copyright laws might not adequately apportion rights to artists when these systems are training on their styles. When it comes to training data, what does it mean to copy? That’s a legal question, but also a technical question. We’re trying to understand if these systems are copying, and when.\nFor labor economics and creative work, the idea is these generative AI systems can accelerate the creative process in many ways, but they can also remove the ideation process that starts with a blank slate. Sometimes, there’s actually good that comes from starting with a blank page. We don’t know how it’s going to influence creativity, and we need a better understanding of how AI will affect the different stages of the creative process. We need to think carefully about how we use these tools to complement people’s work instead of replacing it.\nIn terms of generative AI’s effect on the media ecosystem, with the ability to produce synthetic media at scale, the risk of AI-generated misinformation must be considered. We need to safeguard the media ecosystem against the possibility of massive fraud on one hand, and people losing trust in real media on the other.\nQ: How do you hope this paper is received — and by whom?\nA: The conversation about AI has been very fragmented and frustrating. Because the technologies are moving so fast, it’s been hard to think deeply about these ideas. To ensure the beneficial use of these technologies, we need to build shared language and start to understand where to focus our attention. We’re hoping this paper can be a step in that direction. We’re trying to start a conversation that can help us build a roadmap toward understanding this fast-moving situation.\nArtists many times are at the vanguard of new technologies. They’re playing with the technology long before there are commercial applications. They’re exploring how it works, and they’re wrestling with the ethics of it. AI art has been going on for over a decade, and for as long these artists have been grappling with the questions we now face as a society. I think it is critical to uplift the voices of the artists and other creative laborers whose jobs will be impacted by these tools. Art is how we express our humanity. It’s a core human, emotional part of life. In that way we believe it’s at the center of broader questions about AI’s impact on society, and hopefully we can ground that discussion with this.\n",
    "published": "2023-06-15",
    "timestamp": "2023-08-30T13:26:06.993628"
  },
  {
    "unique_id": "10398e11-bcdc-5518-844a-78583082cef3",
    "title": "MIT researchers make language models scalable self-learners",
    "description": "The scientists used a natural language-based logical inference dataset to create smaller language models that outperformed much larger counterparts. ",
    "link": "https://news.mit.edu/2023/language-models-scalable-self-learners-0608",
    "blog_text": "Socrates once said: “It is not the size of a thing, but the quality that truly matters. For it is in the nature of substance, not its volume, that true value is found.”\nDoes size always matter for large language models (LLMs)? In a technological landscape bedazzled by LLMs taking center stage, a team of MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) researchers think smaller models shouldn’t be overlooked, especially for natural language understanding products widely deployed in the industry.\nTo that end, the researchers cooked up an approach to long-standing problems of inefficiency and privacy associated with big, text-based AI models — a logic-aware model that outperforms 500-times-bigger counterparts on some language understanding tasks without human-generated annotations, while preserving privacy and robustness with high performance.\nLLMs, which have shown some promising skills in generating language, art, and code, are computationally expensive, and their data requirements can risk privacy leaks when using application programming interfaces for data upload. Smaller models have been historically less capable, particularly in multitasking and weakly supervised tasks, compared to their larger counterparts.\nSo what’s helping these smaller models act so mighty, then? Something called “textual entailment,” a way to help these models understand a variety of language tasks, where if one sentence (the premise) is true, then the other sentence (the hypothesis) is likely to be true as well. For example, if the premise is, “all cats have tails” then the hypothesis “a tabby cat has a tail” would be entailed by the premise. This concept is used to train an “entailment model” that proved to be less biased than other language models, from the team’s previous research. They then created “prompts'' that the models can use to figure out if certain information is entailed by a given sentence or phrase according to different tasks. This method improved the model's ability to adapt to different tasks without any additional training, known as zero-shot adaptation.\nIn the realm of “natural language understanding,” there are various applications that hinge on determining the relationship between two pieces of text. For example, in sentiment classification, a statement like “I think the movie is good” can be inferred or entailed from a movie review that says, “I like the story and the acting is great,” indicating a positive sentiment. Another is news classification, where the topic of a news article can be inferred from its content. For example, a statement like “the news article is about sports” can be entailed if the main content of the article reports on an NBA game. The key insight was that many existing natural language understanding tasks could be recast as an entailment (i.e., logical inference in natural language) task. \n“Our research is about improving the ability of computer programs to understand and process natural language — the way humans speak and write. Our self-trained, 350-million-parameter entailment models, without human-generated labels, outperform supervised language models with 137 to 175 billion parameters,” says MIT CSAIL postdoc Hongyin Luo, lead author on a new paper about the study. “This has potential to reshape the landscape of AI and machine learning, providing a more scalable, trustworthy, and cost-effective solution to language modeling,” says Luo. “By proving that smaller models can perform at the same level as larger ones for language understanding, this work paves the way for more sustainable and privacy-preserving AI technologies.” \nThe team discovered that they could improve the model's performance even more by using a technique called “self-training,” where the model uses its own predictions to teach itself, effectively learning without human supervision and additional annotated training data.The self-training method significantly improved performance on a bunch of downstream tasks, including sentiment analysis, question-answering, and news classification. It outperformed both Google's LaMDA and FLAN in zero-shot capabilities, GPT models, and other supervised algorithms. \nHowever, one challenge with self-training is that the model can sometimes generate incorrect or noisy labels that harm performance. To overcome this, they developed a new algorithm called 'SimPLE' (Simple Pseudo-Label Editing), a process to review and modify the pseudo-labels made in initial rounds of learning. By correcting any mislabeled instances, it improved the overall quality of the self-generated labels. This not only made the models more effective at understanding language, but more robust when faced with adversarial data. \nAs with most research, there are some limitations. The self-training on multi-class classification tasks didn't perform as well as on binary natural language understanding tasks, indicating the challenge of applying entailment models to multi-choice tasks.\n\n“This research presents an efficient and effective way to train large language models (LLMs) by formulating natural language understanding tasks as contextual entailment problems and employing a pseudo-labeling self-training mechanism to incorporate large quantities of unlabelled text data in the training process,” adds CSAIL Senior Research Scientist James Glass, who is also an author on the paper. “While the field of LLMs is undergoing rapid and dramatic changes, this research shows that it is possible to produce relatively compact language models that perform very well on benchmark understanding tasks compared to their peers of roughly the same size, or even much larger language models.”\n“Entailment task is a popular proxy to evaluate “understanding” of a given context by an AI model,” says Leonid Karlinsky, research staff member at the MIT-IBM Watson AI Lab. “It is used in many areas analyzing models with unimodal, like LLMs, and and multi-modal, like VLMs [visual language models] inputs, simplifying the task of question-answering about a given input context to a binary classification problem — does this context entail a certain (e.g., text) conclusion or not? This paper makes two contributions in this space. First, it proposes a way to improve the zero-shot (without additional tuning) NLU performance and robustness to adversarial attacks via tuning with synthesized (specialized) entailment tasks generated for the primal NLU task. Second, it offers a self-supervised SimPLE method including pseudo-labeling and confidence-based filtering to further improve large LLMs' NLU performance.”\nLuo and Glass wrote the paper with Yoon Kim, a CSAIL member and assistant professor in MIT’s Department of Electrical Engineering and Computer Science, and Jiaxin Ge of Peking University. Their work will be presented at the meeting of the Association for Computational Linguistics in Toronto, Ontario this July. This research was supported by a grant from the Hong Kong Innovation AI program.\n",
    "published": "2023-06-08",
    "timestamp": "2023-08-30T13:26:07.011345"
  },
  {
    "unique_id": "15e2d905-a216-56cc-8aa9-803a14bbe2cb",
    "title": "SMART launches research group to advance AI, automation, and the future of work",
    "description": "Mens, Manus and Machina (M3S) will design technology, training programs, and institutions for successful human-machine collaboration.",
    "link": "https://news.mit.edu/2023/smart-launches-m3s-research-group-advance-ai-automation-future-work-0823",
    "blog_text": "The Singapore MIT-Alliance for Research and Technology (SMART), MIT’s research enterprise in Singapore, has launched a new interdisciplinary research group aimed at tackling key social and institutional challenges around the rise of artificial intelligence and other new technologies. The group, known as Mens, Manus and Machina: How AI Empowers People, Institutions and the City in Singapore (M3S), aims to advance knowledge in these fields and foster collaborative research that generates positive impact for society in Singapore and the world.\nSeeking to redefine the boundaries of AI, automation, and robotics through interdisciplinary research, knowledge sharing, and impactful collaborations, SMART M3S endeavors to design inclusive, resilient, and innovative solutions that empower individuals, institutions, and cities. By exploring the intricate relationship between human capabilities, emerging technologies, and societal structures, it is envisioned that SMART M3S will drive scientific, societal, and commercial impact in Singapore and beyond.\nIn line with Singapore’s Smart Nation initiative and its National AI Strategy, the project will embark on an ambitious five-year endeavor supported by a multimillion-dollar grant from the National Research Foundation of Singapore under its Campus for Research Excellence And Technological Enterprise program. \nBringing together a diverse team of 17 professors from MIT and institutions in Singapore, SMART M3S will draw expertise from local researchers from Singapore Management University (SMU), Singapore University of Technology and Design, the National University of Singapore, and the National Robotics Program of Singapore. M3S will be guided by lead principal investigator Jinhua Zhao of MIT, co-lead principal investigator Daniela Rus of MIT, and co-lead principal investigator Archan Misra of SMU.\nRanked No. 1 in the 2023 Smart City Index, Singapore has facilitated the integration of AI, automation, and robotics by strategic use of data analytics, internet-of-things technologies, and smart infrastructure. Amid the rise of AI and machine learning, SMART M3S will contribute to Singapore’s AI ecosystem by focusing on the human-machine relationship, enhancing existing AI initiatives in the city-state.\nInspired by MIT’s motto of “mens et manus,” Latin for “mind and hand,” the name M3S reflects the research group’s ideals to promote AI and machine use for practical application — technologies that are extensions of humans and augment their lives. M3S integrates research on robotics and AI with human capital development, economic growth, and public acceptability — an intersectional approach to the ongoing transformation of how we work and live.\nThis interdisciplinary approach encompasses tackling key issues such as physical and digital interfaces between humans and machines, machine learning fundamentals, and understanding the implications of AI for human and social capital development. Other areas of focus include work on structuring human-machine teams within organizations and the developing dynamics between humans and machines in resource allocation and human labor (as well as machine power) management.\nResearch conducted could significantly advance aspects of soft robotics, brain interfaces, learning algorithms, task allocation, team formation, model compression, sustainable technology, technology acceptability in the workplace, social acceptability of robotics and AI, and more. The impact of AI on human welfare and productivity and how AI technology can advance both areas are central considerations for the work at SMART M3S, as society navigates the transition toward an AI- and machine-enhanced future.\n“As a species, humans have spent eons learning how to work effectively with each other, but at the scale of human history, we are still neophytes to computation and automation,” says Zhao, an MIT professor of urban studies and planning who is also founding director of the MIT Mobility Initiative. “We focus on two questions at M3S: How will we design AI and robotics technologies and train humans to build the skills and habits necessary for success in a robotics-heavy work environment? How will we adapt our social and business institutions to create the incentives and protections necessary to drive innovation and social welfare?”\n“The M3S collaboration between researchers at MIT and in Singapore, through SMART, will break new ground in our understanding of AI’s impact on the future of work,” adds Rus, the Andrew (1956) and Erna Viterbi Professor of Electrical Engineering and Computer Science at MIT and director of the MIT Computer Science and Artificial Intelligence Laboratory. “By harnessing our collective expertise and innovative spirit, we aim to advance the state of the art in AI and turn this technological advancement into an engine for human potential and societal progress.”\n“M3S is distinguished by its ambition to address the key challenges of human-AI synergy holistically, from both a scientific and societal perspective,” notes Misra, vice provost for research and the Lee Kong Chian Professor of Computer Science at SMU who is also co-director of the A*STAR-SMU Joint Lab in Social and Human-Centered Computing. “It will focus not just on the technical breakthroughs that will allow human workers and AI-enabled machines and software to work interactively, but also on the training and governance mechanisms that ensure that individuals and organizations adapt to and thrive in this new future of work. I’m especially excited to collaborate with MIT researchers on this important national priority for Singapore, which aligns perfectly with SMU’s strategic multidisciplinary research priority area of digital transformation.”\nThrough interdisciplinary research, knowledge sharing, and impactful collaborations, SMART M3S will explore the intricate interplay between human capabilities, emerging technologies, and societal structures, paving the way for designing inclusive, resilient, and innovative solutions that empower individuals, institutions, and cities in Singapore. By engaging with Singaporean collaborators, SMART M3S hopes to enhance Singapore’s ability to create forward-looking AI policies, invigorate Singapore’s economic standing within AI, and support local workforce training and mentorship on AI topics. \n“With our latest interdisciplinary research group, SMART M3S, we further our commitment to bringing scientific, social, and commercial impact to Singapore and beyond,” says Eugene A. Fitzgerald, CEO and director of SMART. “The focus on a human-centric approach to AI advancement should contribute towards Singapore being at the forefront of the future of work.”\nSince its inception in Singapore in 2007, SMART has developed innovations that have transformed and are transforming a multitude of fields such as autonomous driving, agriculture, microelectronics, cell therapy, mechanics and microfluidics platforms for biology and medical diagnostics, and antimicrobial resistance.\nSMART was established by MIT in coordination with the National Research Foundation of Singapore in 2007 to undertake cutting-edge research in areas of interest to both Singapore and MIT. SMART currently comprises an Innovation Center and four interdisciplinary research groups: Antimicrobial Resistance, Critical Analytics for Manufacturing Personalized-Medicine, Disruptive and Sustainable Technologies for Agricultural Precision, and M3S.\n",
    "published": "2023-08-23",
    "timestamp": "2023-08-30T13:26:06.911779"
  },
  {
    "unique_id": "17f18461-ca8c-5a13-89c9-063a5997d79f",
    "title": "Making sense of all things data",
    "description": "Abel Sanchez helps industries and executives shift their operations in order to make sense of their data and use it to help their bottom lines.",
    "link": "https://news.mit.edu/2023/making-sense-all-things-data-0713",
    "blog_text": "Data, and more specifically using data, is not a new concept, but it remains an elusive one. It comes with terms like “the internet of things” (IoT) and “the cloud,” and no matter how often those are explained, smart people can still be confused. And then there’s the amount of information available and the speed with which it comes in. Software is omnipresent. It’s in coffeemakers and watches, gathering data every second. The question becomes how to take all the new technology and take advantage of the potential insights and analytics. It’s not a small ask.\n“Putting our arms around what digital transformation is can be difficult to do,” says Abel Sanchez. But as the executive director and research director of MIT’s Geospatial Data Center, that’s exactly what he does with his work in helping industries and executives shift their operations in order to make sense of their data and be able to use it to help their bottom lines.\nHandling the pace\nData can lead to making better business decisions. That’s not a new or surprising insight, but as Sanchez says, people still tend to work off of intuition. Part of the problem is that they don’t know what to do with their available data, and there’s usually plenty of available data. Part of that problem is that there’s so much information being produced from so many sources. As soon as a person wakes up and turns on their phone or starts their car, software is running. It’s coming in fast, but because it’s also complex, “it outperforms people,” he says.\nAs an example with Uber, once a person clicks on the app for a ride, predictive models start firing at the rate of 1 million per second. It’s all in order to optimize the trip, taking into account factors such as school schedules, roadway conditions, traffic, and a driver’s availability. It’s helpful for the task, but it’s something that “no human would be able to do,” he says. \nThe solution requires a few components. One is a new way to store data. In the past, the classic was creating the “perfect library,” which was too structured. The response to that was to create a “data lake,” where all the information would go in and somehow people would make sense of it. “This also failed,” Sanchez says.\nData storage needs to be re-imaged, in which a key element is greater accessibility. In most corporations, only 10-20 percent of employees have the access and technical skill to work with the data. The rest have to go through a centralized resource and get into a queue, an inefficient system. The goal, Sanchez says, is to democratize the information by going to a modern stack, which would convert what he calls “dormant data” into “active data.” The result? Better decisions could be made.\nThe first, big step companies need to take is the will to make the change. Part of it is an investment of money, but it’s also an attitude shift. Corporations can have an embedded culture where things have always been done a certain way and deviating from that is resisted because it’s different. But when it comes to data, a new approach is needed. Managing and curating the information can no longer rest in the hands of one person with the institutional memory. It’s not possible. It’s also not practical because companies are losing out on efficiency and productivity, because with technology, “What use to take years to do, now you can do in days,” Sanchez says.\nThe new player\nThe above exemplifies what’s been involved with coordinating data along four intertwined components: IoT, AI, the cloud, and security. The first two create the information, which then gets stored in the cloud, but it’s all for naught without robust security. But one relative newcomer has come into the picture. It’s blockchain technology, a term that is often said but still not fully understood, adding further to the confusion.\nSanchez says that information has been handled and organized a certain way with the World Wide Web. Blockchain is an opportunity to be more nimble and productive by offering the chance to have an accepted identity, currency, and logic that works on a global scale. The holdup has always been that there’s never been any agreement on those three components on a global scale. It leads to people being shut out, inefficiency, and lost business.\nOne example, Sanchez says, of blockchain’s potential is with hospitals. In the United States, they’re private and information has to be constantly integrated from doctors, insurance companies, labs, government regulators, and pharmaceutical companies. It leads to repeated steps to do something as simple as recognizing a patient’s identity, which often can’t be agreed upon. With blockchain, these various entities can create a consortium using open source code with no barriers of access, and it could quickly and easily identify a patient because it set up an agreement, and with it “remove that level of effort.” It’s an incremental step, but one which can be built upon that reduces cost and risk.\nAnother example — “one of the best examples,” Sanchez says — is what was done in Indonesia. Most of the rice, corn, and wheat that comes from this area is produced from smallholder farms. For the people making loans, it’s expensive to understand the risk of cultivating these plots of land. Compounding that is that these farmers don’t have state-issued identities or credit records, so, “They don’t exist in the modern economic sense,” he says. They don’t have access to loans, and banks are losing out on potential good customers.\nWith this project, blockchain allowed local people to gather information about the farms on their smartphones. Banks could acquire the information and compensate the people with tokens, thereby incentivizing the work. The bank would see the creditworthiness of the farms, and farmers could end up getting fair loans.\nIn the end, it creates a beneficial circle for the banks, farmers, and community, but it also represents what can be done with digital transformation by allowing businesses to optimize their processes, make better decisions, and ultimately profit.\n“It’s a tremendous new platform,” Sanchez says. “This is the promise.”\n",
    "published": "2023-07-13",
    "timestamp": "2023-08-30T13:26:06.952426"
  },
  {
    "unique_id": "24f1d796-5c56-5e49-97ec-5d8d60da6dae",
    "title": "Novo Nordisk to support MIT postdocs working at the intersection of AI and life sciences",
    "description": "MIT-Novo Nordisk Artificial Intelligence Postdoctoral Fellows Program will support up to 10 postdocs annually over five years.",
    "link": "https://news.mit.edu/2023/novo-nordisk-support-mit-postdocs-intersection-ai-life-sciences-0615",
    "blog_text": "MIT’s School of Engineering and global health care company Novo Nordisk has announced the launch of a multi-year program to support postdoctoral fellows conducting research at the intersection of artificial intelligence and data science with life sciences. The MIT-Novo Nordisk Artificial Intelligence Postdoctoral Fellows Program will welcome its first cohort of up to 10 postdocs for a two-year term this fall. The program will provide up to $10 million for an annual cohort of up to 10 postdoc for two-year terms.\n“The research being conducted at the intersection of AI and life sciences has the potential to transform health care as we know it,” says Anantha Chandrakasan, dean of the School of Engineering and Vannevar Bush Professor of Electrical Engineering and Computer Science. “I am thrilled that the MIT-Novo Nordisk Program will support early-career researchers who work in this space.”\nThe launch of the MIT-Novo Nordisk Program coincides with the 100th anniversary celebration of Novo Nordisk. The company was founded in 1923 and treated its first patients with insulin, which had recently been discovered in March of that year.\n“The use of AI in the health care industry presents a massive opportunity to improve the lives of people living with chronic diseases,” says Thomas Senderovitz, senior vice president for data science at Novo Nordisk. “Novo Nordisk is committed to the development of new, innovative solutions, and MIT hosts some of the most outstanding researchers in the field. We are therefore excited to support postdocs working on the cutting edge of AI and life sciences.”\nThe MIT-Novo Nordisk Program will support postdocs advancing the use of AI in life science and health. Postdocs will join an annual cohort that participates in frequent events and gatherings. The cohort will meet regularly to exchange ideas about their work and discuss ways to amplify their impact.\n“We are excited to welcome postdocs working on AI, data science, health, and life sciences — research areas of strategic importance across MIT,” adds Chandrakasan.\nA central focus of the program will be offering postdocs professional development and mentorship opportunities. Fellows will be invited to entrepreneurship-focused workshops that enable them to learn from company founders, venture capitalists, and other entrepreneurial leaders. Fellows will also have the opportunity to receive mentorship from experts in life sciences and data science.\n“MIT is always exploring opportunities to innovate and enhance the postdoctoral experience,” adds MIT Provost Cynthia Barnhart. “The MIT-Novo Nordisk Program has been thoughtfully designed to introduce fellows to a wealth of experiences, skill sets, and perspectives that support their professional growth while prioritizing a sense of community with their cohort.”\nAngela Belcher, head of the Department of Biological Engineering, the James Mason Crafts Professor of Biological Engineering and Materials Science, and member of the Koch Institute for Integrative Cancer Research, and Asu Ozdaglar, deputy dean of academics for the MIT Schwarzman College of Computing and head of the Department of Electrical Engineering and Computer Science, will serve as co-faculty leads for the program.\nThe new program complements a separate postdoctoral fellowship program at MIT supported by the Novo Nordisk Foundation that focuses on enabling interdisciplinary research.\n",
    "published": "2023-06-15",
    "timestamp": "2023-08-30T13:26:06.990461"
  },
  {
    "unique_id": "26609bc3-35e4-53aa-aa63-e6f9e2456422",
    "title": "Bringing the social and ethical responsibilities of computing to the forefront",
    "description": "The inaugural SERC Symposium convened experts from multiple disciplines to explore the challenges and opportunities that arise with the broad applicability of computing in many aspects of society.",
    "link": "https://news.mit.edu/2023/bringing-social-ethical-responsibilities-computing-forefront-0608",
    "blog_text": "There has been a remarkable surge in the use of algorithms and artificial intelligence to address a wide range of problems and challenges. While their adoption, particularly with the rise of AI, is reshaping nearly every industry sector, discipline, and area of research, such innovations often expose unexpected consequences that involve new norms, new expectations, and new rules and laws.\nTo facilitate deeper understanding, the Social and Ethical Responsibilities of Computing (SERC), a cross-cutting initiative in the MIT Schwarzman College of Computing, recently brought together social scientists and humanists with computer scientists, engineers, and other computing faculty for an exploration of the ways in which the broad applicability of algorithms and AI has presented both opportunities and challenges in many aspects of society.\n“The very nature of our reality is changing. AI has the ability to do things that until recently were solely the realm of human intelligence — things that can challenge our understanding of what it means to be human,” remarked Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing, in his opening address at the inaugural SERC Symposium. “This poses philosophical, conceptual, and practical questions on a scale not experienced since the start of the Enlightenment. In the face of such profound change, we need new conceptual maps for navigating the change.”\nThe symposium offered a glimpse into the vision and activities of SERC in both research and education. “We believe our responsibility with SERC is to educate and equip our students and enable our faculty to contribute to responsible technology development and deployment,” said Georgia Perakis, the William F. Pounds Professor of Management in the MIT Sloan School of Management, co-associate dean of SERC, and the lead organizer of the symposium. “We’re drawing from the many strengths and diversity of disciplines across MIT and beyond and bringing them together to gain multiple viewpoints.”\nThrough a succession of panels and sessions, the symposium delved into a variety of topics related to the societal and ethical dimensions of computing. In addition, 37 undergraduate and graduate students from a range of majors, including urban studies and planning, political science, mathematics, biology, electrical engineering and computer science, and brain and cognitive sciences, participated in a poster session to exhibit their research in this space, covering such topics as quantum ethics, AI collusion in storage markets, computing waste, and empowering users on social platforms for better content credibility.\nShowcasing a diversity of work\nIn three sessions devoted to themes of beneficent and fair computing, equitable and personalized health, and algorithms and humans, the SERC Symposium showcased work by 12 faculty members across these domains.\nOne such project from a multidisciplinary team of archaeologists, architects, digital artists, and computational social scientists aimed to preserve endangered heritage sites in Afghanistan with digital twins. The project team produced highly detailed interrogable 3D models of the heritage sites, in addition to extended reality and virtual reality experiences, as learning resources for audiences that cannot access these sites.\nIn a project for the United Network for Organ Sharing, researchers showed how they used applied analytics to optimize various facets of an organ allocation system in the United States that is currently undergoing a major overhaul in order to make it more efficient, equitable, and inclusive for different racial, age, and gender groups, among others.\nAnother talk discussed an area that has not yet received adequate public attention: the broader implications for equity that biased sensor data holds for the next generation of models in computing and health care.\nA talk on bias in algorithms considered both human bias and algorithmic bias, and the potential for improving results by taking into account differences in the nature of the two kinds of bias.\nOther highlighted research included the interaction between online platforms and human psychology; a study on whether decision-makers make systemic prediction mistakes on the available information; and an illustration of how advanced analytics and computation can be leveraged to inform supply chain management, operations, and regulatory work in the food and pharmaceutical industries.\nImproving the algorithms of tomorrow\n“Algorithms are, without question, impacting every aspect of our lives,” said Asu Ozdaglar, deputy dean of academics for the MIT Schwarzman College of Computing and head of the Department of Electrical Engineering and Computer Science, in kicking off a panel she moderated on the implications of data and algorithms.\n“Whether it’s in the context of social media, online commerce, automated tasks, and now a much wider range of creative interactions with the advent of generative AI tools and large language models, there’s little doubt that much more is to come,” Ozdaglar said. “While the promise is evident to all of us, there’s a lot to be concerned as well. This is very much time for imaginative thinking and careful deliberation to improve the algorithms of tomorrow.”\nTurning to the panel, Ozdaglar asked experts from computing, social science, and data science for insights on how to understand what is to come and shape it to enrich outcomes for the majority of humanity.\nSarah Williams, associate professor of technology and urban planning at MIT, emphasized the critical importance of comprehending the process of how datasets are assembled, as data are the foundation for all models. She also stressed the need for research to address the potential implication of biases in algorithms that often find their way in through their creators and the data used in their development. “It’s up to us to think about our own ethical solutions to these problems,” she said. “Just as it’s important to progress with the technology, we need to start the field of looking at these questions of what biases are in the algorithms? What biases are in the data, or in that data’s journey?”\nShifting focus to generative models and whether the development and use of these technologies should be regulated, the panelists — which also included MIT’s Srini Devadas, professor of electrical engineering and computer science, John Horton, professor of information technology, and Simon Johnson, professor of entrepreneurship — all concurred that regulating open-source algorithms, which are publicly accessible, would be difficult given that regulators are still catching up and struggling to even set guardrails for technology that is now 20 years old.\nReturning to the question of how to effectively regulate the use of these technologies, Johnson proposed a progressive corporate tax system as a potential solution. He recommends basing companies' tax payments on their profits, especially for large corporations whose massive earnings go largely untaxed due to offshore banking. By doing so, Johnson said that this approach can serve as a regulatory mechanism that discourages companies from trying to “own the entire world” by imposing disincentives.\nThe role of ethics in computing education\nAs computing continues to advance with no signs of slowing down, it is critical to educate students to be intentional in the social impact of the technologies they will be developing and deploying into the world. But can one actually be taught such things? If so, how?\nCaspar Hare, professor of philosophy at MIT and co-associate dean of SERC, posed this looming question to faculty on a panel he moderated on the role of ethics in computing education. All experienced in teaching ethics and thinking about the social implications of computing, each panelist shared their perspective and approach.\nA strong advocate for the importance of learning from history, Eden Medina, associate professor of science, technology, and society at MIT, said that “often the way we frame computing is that everything is new. One of the things that I do in my teaching is look at how people have confronted these issues in the past and try to draw from them as a way to think about possible ways forward.” Medina regularly uses case studies in her classes and referred to a paper written by Yale University science historian Joanna Radin on the Pima Indian Diabetes Dataset that raised ethical issues on the history of that particular collection of data that many don’t consider as an example of how decisions around technology and data can grow out of very specific contexts.\nMilo Phillips-Brown, associate professor of philosophy at Oxford University, talked about the Ethical Computing Protocol that he co-created while he was a SERC postdoc at MIT. The protocol, a four-step approach to building technology responsibly, is designed to train computer science students to think in a better and more accurate way about the social implications of technology by breaking the process down into more manageable steps. “The basic approach that we take very much draws on the fields of value-sensitive design, responsible research and innovation, participatory design as guiding insights, and then is also fundamentally interdisciplinary,” he said.\nFields such as biomedicine and law have an ethics ecosystem that distributes the function of ethical reasoning in these areas. Oversight and regulation are provided to guide front-line stakeholders and decision-makers when issues arise, as are training programs and access to interdisciplinary expertise that they can draw from. “In this space, we have none of that,” said John Basl, associate professor of philosophy at Northeastern University. “For current generations of computer scientists and other decision-makers, we’re actually making them do the ethical reasoning on their own.” Basl commented further that teaching core ethical reasoning skills across the curriculum, not just in philosophy classes, is essential, and that the goal shouldn’t be for every computer scientist be a professional ethicist, but for them to know enough of the landscape to be able to ask the right questions and seek out the relevant expertise and resources that exists.\nAfter the final session, interdisciplinary groups of faculty, students, and researchers engaged in animated discussions related to the issues covered throughout the day during a reception that marked the conclusion of the symposium.\n",
    "published": "2023-06-08",
    "timestamp": "2023-08-30T13:26:07.006376"
  },
  {
    "unique_id": "2a69bb8c-57cc-5ddf-99f9-1e63bf1e5c4e",
    "title": "Envisioning the future of computing",
    "description": "MIT students share ideas, aspirations, and vision for how advances in computing stand to transform society in a competition hosted by the Social and Ethical Responsibilities of Computing.",
    "link": "https://news.mit.edu/2023/envisioning-future-computing-0616",
    "blog_text": "How will advances in computing transform human society?\nMIT students contemplated this impending question as part of the Envisioning the Future of Computing Prize — an essay contest in which they were challenged to imagine ways that computing technologies could improve our lives, as well as the pitfalls and dangers associated with them.\nOffered for the first time this year, the Institute-wide competition invited MIT undergraduate and graduate students to share their ideas, aspirations, and vision for what they think a future propelled by advancements in computing holds. Nearly 60 students put pen to paper, including those majoring in mathematics, philosophy, electrical engineering and computer science, brain and cognitive sciences, chemical engineering, urban studies and planning, and management, and entered their submissions.\nStudents dreamed up highly inventive scenarios for how the technologies of today and tomorrow could impact society, for better or worse. Some recurring themes emerged, such as tackling issues in climate change and health care. Others proposed ideas for particular technologies that ranged from digital twins as a tool for navigating the deluge of information online to a cutting-edge platform powered by artificial intelligence, machine learning, and biosensors to create personalized storytelling films that help individuals understand themselves and others.\nConceived of by the Social and Ethical Responsibilities of Computing (SERC), a cross-cutting initiative of the MIT Schwarzman College of Computing in collaboration with the School of Humanities, Arts, and Social Sciences (SHASS), the intent of the competition was “to create a space for students to think in a creative, informed, and rigorous way about the societal benefits and costs of the technologies they are or will be developing,” says Caspar Hare, professor of philosophy, co-associate dean of SERC, and the lead organizer of the Envisioning the Future of Computing Prize. “We also wanted to convey that MIT values such thinking.”\nPrize winners\nThe contest implemented a two-stage evaluation process wherein all essays were reviewed anonymously by a panel of MIT faculty members from the college and SHASS for the initial round. Three qualifiers were then invited to present their entries at an awards ceremony on May 8, followed by a Q&A with a judging panel and live in-person audience for the final round.\nThe winning entry was awarded to Robert Cunningham '23, a recent graduate in math and physics, for his paper on the implications of a personalized language model that is fine-tuned to predict an individual’s writing based on their past texts and emails. Told from the perspective of three fictional characters: Laura, founder of the tech startup ScribeAI, and Margaret and Vincent, a couple in college who are frequent users of the platform, readers gained insights into the societal shifts that take place and the unforeseen repercussions of the technology.\nCunningham, who took home the grand prize of $10,000, says he came up with the concept for his essay in late January while thinking about the upcoming release of GPT-4 and how it might be applied. Created by the developers of ChatGPT — an AI chatbot that has managed to capture popular imagination for its capacity to imitate human-like text, images, audio, and code — GPT-4, which was unveiled in March, is the newest version of OpenAI’s language model systems.\n“GPT-4 is wild in reality, but some rumors before it launched were even wilder, and I had a few long plane rides to think about them! I enjoyed this opportunity to solidify a vague notion into a piece of writing, and since some of my favorite works of science fiction are short stories, I figured I'd take the chance to write one,” Cunningham says.\nThe other two finalists, awarded $5,000 each, included Gabrielle Kaili-May Liu '23, a recent graduate in mathematics with computer science, and brain and cognitive sciences, for her entry on using the reinforcement learning with human feedback technique as a tool for transforming human interactions with AI; and Abigail Thwaites and Eliot Matthew Watkins, graduate students in the Department of Philosophy and Linguistics, for their joint submission on automatic fact checkers, an AI-driven software that they argue could potentially help mitigate the spread of misinformation and be a profound social good.\n“We were so excited to see the amazing response to this contest. It made clear how much students at MIT, contrary to stereotype, really care about the wider implications of technology, says Daniel Jackson, professor of computer science and one of the final-round judges. “So many of the essays were incredibly thoughtful and creative. Robert’s story was a chilling, but entirely plausible take on our AI future; Abigail and Eliot’s analysis brought new clarity to what harms misinformation actually causes; and Gabrielle’s piece gave a lucid overview of a prominent new technology. I hope we’ll be able to run this contest every year, and that it will encourage all our students to broaden their perspectives even further.”\nFellow judge Graham Jones, professor of anthropology, adds: “The winning entries reflected the incredible breadth of our students’ engagement with socially responsible computing. They challenge us to think differently about how to design computational technologies, conceptualize social impacts, and imagine future scenarios. Working with a cross-disciplinary panel of judges catalyzed lots of new conversations. As a sci-fi fan, I was thrilled that the top prize went to a such a stunning piece of speculative fiction!”\nOther judges on the panel for the final round included:\n\nDan Huttenlocher, dean of the MIT Schwarzman College of Computing;\nAleksander Madry, Cadence Design Systems Professor of Computer Science;\nAsu Ozdaglar, deputy dean of academics for the MIT Schwarzman College of Computing and head of the Department of Electrical Engineering and Computer Science;\nGeorgia Perakis, co-associate dean of SERC and the William F. Pounds Professor of Management; and\nAgustin Rayo, dean of the MIT School of Humanities, Arts, and Social Sciences.\n\nHonorable mentions\nIn addition to the grand prize winner and runners up, 12 students were recognized with honorable mentions for their entries, with each receiving $500.\nThe honorees and the title of their essays include:\n\nAlexa Reese Canaan, Technology and Policy Program, “A New Way Forward: The Internet & Data Economy”;\nFernanda De La Torre Romo, Department of Brain and Cognitive Sciences, “The Empathic Revolution Using AI to Foster Greater Understanding and Connection”;\nSamuel Florin, Department of Mathematics, \"Modeling International Solutions for the Climate Crisis\";\nClaire Gorman, Department of Urban Studies and Planning (DUSP), “Grounding AI — Envisioning Inclusive Computing for Soil Carbon Applications”;\nKevin Hansom, MIT Sloan School of Management, “Quantum Powered Personalized Pharmacogenetic Development and Distribution Model”;\nSharon Jiang, Department of Electrical Engineering and Computer Science (EECS), “Machine Learning Driven Transformation of Electronic Health Records”;\nCassandra Lee, Media Lab, “Considering an Anti-convenience Funding Body”;\nMartin Nisser, EECS, \"Towards Personalized On-Demand Manufacturing\";\nAndi Qu, EECS, \"Revolutionizing Online Learning with Digital Twins\";\nDavid Bradford Ramsay, Media Lab, “The Perils and Promises of Closed Loop Engagement”;\nShuvom Sadhuka, EECS, “Overcoming the False Trade-off in Genomics: Privacy and Collaboration”; and\nLeonard Schrage, DUSP, “Embodied-Carbon-Computing.”\n\nThe Envisioning the Future of Computing Prize was supported by MAC3 Impact Philanthropies.\n",
    "published": "2023-06-16",
    "timestamp": "2023-08-30T13:26:06.988575"
  },
  {
    "unique_id": "3067c637-f776-5f93-9e37-0b212a5ec37f",
    "title": "When computer vision works more like a brain, it sees more like people do",
    "description": "Training artificial neural networks with data from real brains can make computer vision more robust.",
    "link": "https://news.mit.edu/2023/when-computer-vision-works-like-human-brain-0630",
    "blog_text": "From cameras to self-driving cars, many of today’s technologies depend on artificial intelligence to extract meaning from visual information. Today’s AI technology has artificial neural networks at its core, and most of the time we can trust these AI computer vision systems to see things the way we do — but sometimes they falter. According to MIT and IBM research scientists, one way to improve computer vision is to instruct the artificial neural networks that they rely on to deliberately mimic the way the brain’s biological neural network processes visual images.\nResearchers led by MIT Professor James DiCarlo, the director of MIT’s Quest for Intelligence and member of the MIT-IBM Watson AI Lab, have made a computer vision model more robust by training it to work like a part of the brain that humans and other primates rely on for object recognition. This May, at the International Conference on Learning Representations, the team reported that when they trained an artificial neural network using neural activity patterns in the brain’s inferior temporal (IT) cortex, the artificial neural network was more robustly able to identify objects in images than a model that lacked that neural training. And the model’s interpretations of images more closely matched what humans saw, even when images included minor distortions that made the task more difficult.\nComparing neural circuits\nMany of the artificial neural networks used for computer vision already resemble the multilayered brain circuits that process visual information in humans and other primates. Like the brain, they use neuron-like units that work together to process information. As they are trained for a particular task, these layered components collectively and progressively process the visual information to complete the task — determining, for example, that an image depicts a bear or a car or a tree.\nDiCarlo and others previously found that when such deep-learning computer vision systems establish efficient ways to solve visual problems, they end up with artificial circuits that work similarly to the neural circuits that process visual information in our own brains. That is, they turn out to be surprisingly good scientific models of the neural mechanisms underlying primate and human vision.\nThat resemblance is helping neuroscientists deepen their understanding of the brain. By demonstrating ways visual information can be processed to make sense of images, computational models suggest hypotheses about how the brain might accomplish the same task. As developers continue to refine computer vision models, neuroscientists have found new ideas to explore in their own work.\n“As vision systems get better at performing in the real world, some of them turn out to be more human-like in their internal processing. That’s useful from an understanding-biology point of view,” says DiCarlo, who is also a professor of brain and cognitive sciences and an investigator at the McGovern Institute for Brain Research.\nEngineering a more brain-like AI \nWhile their potential is promising, computer vision systems are not yet perfect models of human vision. DiCarlo suspected one way to improve computer vision may be to incorporate specific brain-like features into these models.\nTo test this idea, he and his collaborators built a computer vision model using neural data previously collected from vision-processing neurons in the monkey IT cortex — a key part of the primate ventral visual pathway involved in the recognition of objects — while the animals viewed various images. More specifically, Joel Dapello, a Harvard University graduate student and former MIT-IBM Watson AI Lab intern; and Kohitij Kar, assistant professor and Canada Research Chair (Visual Neuroscience) at York University and visiting scientist at MIT; in collaboration with David Cox, IBM Research’s vice president for AI models and IBM director of the MIT-IBM Watson AI Lab; and other researchers at IBM Research and MIT asked an artificial neural network to emulate the behavior of these primate vision-processing neurons while the network learned to identify objects in a standard computer vision task.\n“In effect, we said to the network, ‘please solve this standard computer vision task, but please also make the function of one of your inside simulated “neural” layers be as similar as possible to the function of the corresponding biological neural layer,’” DiCarlo explains. “We asked it to do both of those things as best it could.” This forced the artificial neural circuits to find a different way to process visual information than the standard, computer vision approach, he says.\nAfter training the artificial model with biological data, DiCarlo’s team compared its activity to a similarly-sized neural network model trained without neural data, using the standard approach for computer vision. They found that the new, biologically informed model IT layer was — as instructed — a better match for IT neural data.  That is, for every image tested, the population of artificial IT neurons in the model responded more similarly to the corresponding population of biological IT neurons.\nThe researchers also found that the model IT was also a better match to IT neural data collected from another monkey, even though the model had never seen data from that animal, and even when that comparison was evaluated on that monkey’s IT responses to new images. This indicated that the team’s new, “neurally aligned” computer model may be an improved model of the neurobiological function of the primate IT cortex — an interesting finding, given that it was previously unknown whether the amount of neural data that can be currently collected from the primate visual system is capable of directly guiding model development.\nWith their new computer model in hand, the team asked whether the “IT neural alignment” procedure also leads to any changes in the overall behavioral performance of the model. Indeed, they found that the neurally-aligned model was more human-like in its behavior — it tended to succeed in correctly categorizing objects in images for which humans also succeed, and it tended to fail when humans also fail.\nAdversarial attacks\nThe team also found that the neurally aligned model was more resistant to “adversarial attacks” that developers use to test computer vision and AI systems. In computer vision, adversarial attacks introduce small distortions into images that are meant to mislead an artificial neural network.\n“Say that you have an image that the model identifies as a cat. Because you have the knowledge of the internal workings of the model, you can then design very small changes in the image so that the model suddenly thinks it’s no longer a cat,” DiCarlo explains.\nThese minor distortions don’t typically fool humans, but computer vision models struggle with these alterations. A person who looks at the subtly distorted cat still reliably and robustly reports that it’s a cat. But standard computer vision models are more likely to mistake the cat for a dog, or even a tree.\n“There must be some internal differences in the way our brains process images that lead to our vision being more resistant to those kinds of attacks,” DiCarlo says. And indeed, the team found that when they made their model more neurally aligned, it became more robust, correctly identifying more images in the face of adversarial attacks. The model could still be fooled by stronger “attacks,” but so can people, DiCarlo says. His team is now exploring the limits of adversarial robustness in humans.\nA few years ago, DiCarlo’s team found they could also improve a model’s resistance to adversarial attacks by designing the first layer of the artificial network to emulate the early visual processing layer in the brain. One key next step is to combine such approaches — making new models that are simultaneously neurally aligned at multiple visual processing layers.\nThe new work is further evidence that an exchange of ideas between neuroscience and computer science can drive progress in both fields. “Everybody gets something out of the exciting virtuous cycle between natural/biological intelligence and artificial intelligence,” DiCarlo says. “In this case, computer vision and AI researchers get new ways to achieve robustness, and neuroscientists and cognitive scientists get more accurate mechanistic models of human vision.”\nThis work was supported by the MIT-IBM Watson AI Lab, Semiconductor Research Corporation, the U.S. Defense Research Projects Agency, the MIT Shoemaker Fellowship, U.S. Office of Naval Research, the Simons Foundation, and Canada Research Chair Program.\n",
    "published": "2023-06-30",
    "timestamp": "2023-08-30T13:26:06.969377"
  },
  {
    "unique_id": "32627b31-f634-55bf-88a9-f7dd8dcf9d21",
    "title": "Artificial intelligence for augmentation and productivity",
    "description": "The MIT Schwarzman College of Computing awards seed grants to seven interdisciplinary projects exploring AI-augmented management.",
    "link": "https://news.mit.edu/2023/artificial-intelligence-augmentation-and-productivity-0818",
    "blog_text": "The MIT Stephen A. Schwarzman College of Computing has awarded seed grants to seven projects that are exploring how artificial intelligence and human-computer interaction can be leveraged to enhance modern work spaces to achieve better management and higher productivity.\nFunded by Andrew W. Houston ’05 and Dropbox Inc., the projects are intended to be interdisciplinary and bring together researchers from computing, social sciences, and management.\nThe seed grants can enable the project teams to conduct research that leads to bigger endeavors in this rapidly evolving area, as well as build community around questions related to AI-augmented management.\nThe seven selected projects and research leads include:\n“LLMex: Implementing Vannevar Bush’s Vision of the Memex Using Large Language Models,” led by Pattie Maes of the Media Lab and David Karger of the Department of Electrical Engineering and Computer Science (EECS) and the Computer Science and Artificial Intelligence Laboratory (CSAIL). Inspired by Vannevar Bush’s Memex, this project proposes to design, implement, and test the concept of memory prosthetics using large language models (LLMs). The AI-based system will intelligently help an individual keep track of vast amounts of information, accelerate productivity, and reduce errors by automatically recording their work actions and meetings, supporting retrieval based on metadata and vague descriptions, and suggesting relevant, personalized information proactively based on the user’s current focus and context.\n“Using AI Agents to Simulate Social Scenarios,” led by John Horton of the MIT Sloan School of Management and Jacob Andreas of EECS and CSAIL. This project imagines the ability to easily simulate policies, organizational arrangements, and communication tools with AI agents before implementation. Tapping into the capabilities of modern LLMs to serve as a computational model of humans makes this vision of social simulation more realistic, and potentially more predictive.\n“Human Expertise in the Age of AI: Can We Have Our Cake and Eat it Too?” led by Manish Raghavan of MIT Sloan and EECS, and Devavrat Shah of EECS and the Laboratory for Information and Decision Systems. Progress in machine learning, AI, and in algorithmic decision aids has raised the prospect that algorithms may complement human decision-making in a wide variety of settings. Rather than replacing human professionals, this project sees a future where AI and algorithmic decision aids play a role that is complementary to human expertise.\n“Implementing Generative AI in U.S. Hospitals,” led by Julie Shah of the Department of Aeronautics and Astronautics and CSAIL, Retsef Levi of MIT Sloan and the Operations Research Center, Kate Kellog of MIT Sloan, and Ben Armstrong of the Industrial Performance Center. In recent years, studies have linked a rise in burnout from doctors and nurses in the United States with increased administrative burdens associated with electronic health records and other technologies. This project aims to develop a holistic framework to study how generative AI technologies can both increase productivity for organizations and improve job quality for workers in health care settings.\n“Generative AI Augmented Software Tools to Democratize Programming,” led by Harold Abelson of EECS and CSAIL, Cynthia Breazeal of the Media Lab, and Eric Klopfer of the Comparative Media Studies/Writing. Progress in generative AI over the past year is fomenting an upheaval in assumptions about future careers in software and deprecating the role of coding. This project will stimulate a similar transformation in computing education for those who have no prior technical training by creating a software tool that could eliminate much of the need for learners to deal with code when creating applications.\n“Acquiring Expertise and Societal Productivity in a World of Artificial Intelligence,” led by David Atkin and Martin Beraja of the Department of Economics, and Danielle Li of MIT Sloan. Generative AI is thought to augment the capabilities of workers performing cognitive tasks. This project seeks to better understand how the arrival of AI technologies may impact skill acquisition and productivity, and to explore complementary policy interventions that will allow society to maximize the gains from such technologies.\n“AI Augmented Onboarding and Support,” led by Tim Kraska of EECS and CSAIL, and Christoph Paus of the Department of Physics and the Laboratory for Nuclear Science. While LLMs have made enormous leaps forward in recent years and are poised to fundamentally change the way students and professionals learn about new tools and systems, there is often a steep learning curve which people have to climb in order to make full use of the resource. To help mitigate the issue, this project proposes the development of new LLM-powered onboarding and support systems that will positively impact the way support teams operate and improve the user experience.\n",
    "published": "2023-08-18",
    "timestamp": "2023-08-30T13:26:06.917427"
  },
  {
    "unique_id": "32ad1cd0-f288-5c83-b057-7bc699fa4b3e",
    "title": "AI helps robots manipulate objects with their whole bodies",
    "description": "With a new technique, a robot can reason efficiently about moving objects using more than just its fingertips.",
    "link": "https://news.mit.edu/2023/ai-technique-robots-manipulate-objects-whole-bodies-0824",
    "blog_text": "Imagine you want to carry a large, heavy box up a flight of stairs. You might spread your fingers out and lift that box with both hands, then hold it on top of your forearms and balance it against your chest, using your whole body to manipulate the box. \nHumans are generally good at whole-body manipulation, but robots struggle with such tasks. To the robot, each spot where the box could touch any point on the carrier’s fingers, arms, and torso represents a contact event that it must reason about. With billions of potential contact events, planning for this task quickly becomes intractable.\nNow MIT researchers found a way to simplify this process, known as contact-rich manipulation planning. They use an AI technique called smoothing, which summarizes many contact events into a smaller number of decisions, to enable even a simple algorithm to quickly identify an effective manipulation plan for the robot.\nWhile still in its early days, this method could potentially enable factories to use smaller, mobile robots that can manipulate objects with their entire arms or bodies, rather than large robotic arms that can only grasp using fingertips. This may help reduce energy consumption and drive down costs. In addition, this technique could be useful in robots sent on exploration missions to Mars or other solar system bodies, since they could adapt to the environment quickly using only an onboard computer.      \n“Rather than thinking about this as a black-box system, if we can leverage the structure of these kinds of robotic systems using models, there is an opportunity to accelerate the whole procedure of trying to make these decisions and come up with contact-rich plans,” says H.J. Terry Suh, an electrical engineering and computer science (EECS) graduate student and co-lead author of a paper on this technique.\nJoining Suh on the paper are co-lead author Tao Pang PhD ’23, a roboticist at Boston Dynamics AI Institute; Lujie Yang, an EECS graduate student; and senior author Russ Tedrake, the Toyota Professor of EECS, Aeronautics and Astronautics, and Mechanical Engineering, and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL). The research appears this week in IEEE Transactions on Robotics.\nLearning about learning\nReinforcement learning is a machine-learning technique where an agent, like a robot, learns to complete a task through trial and error with a reward for getting closer to a goal. Researchers say this type of learning takes a black-box approach because the system must learn everything about the world through trial and error.\nIt has been used effectively for contact-rich manipulation planning, where the robot seeks to learn the best way to move an object in a specified manner.\nBut because there may be billions of potential contact points that a robot must reason about when determining how to use its fingers, hands, arms, and body to interact with an object, this trial-and-error approach requires a great deal of computation.\n“Reinforcement learning may need to go through millions of years in simulation time to actually be able to learn a policy,” Suh adds.\nOn the other hand, if researchers specifically design a physics-based model using their knowledge of the system and the task they want the robot to accomplish, that model incorporates structure about this world that makes it more efficient.\nYet physics-based approaches aren’t as effective as reinforcement learning when it comes to contact-rich manipulation planning — Suh and Pang wondered why.\nThey conducted a detailed analysis and found that a technique known as smoothing enables reinforcement learning to perform so well.\nMany of the decisions a robot could make when determining how to manipulate an object aren’t important in the grand scheme of things. For instance, each infinitesimal adjustment of one finger, whether or not it results in contact with the object, doesn’t matter very much.  Smoothing averages away many of those unimportant, intermediate decisions, leaving a few important ones.\nReinforcement learning performs smoothing implicitly by trying many contact points and then computing a weighted average of the results. Drawing on this insight, the MIT researchers designed a simple model that performs a similar type of smoothing, enabling it to focus on core robot-object interactions and predict long-term behavior. They showed that this approach could be just as effective as reinforcement learning at generating complex plans.\n“If you know a bit more about your problem, you can design more efficient algorithms,” Pang says.\nA winning combination\nEven though smoothing greatly simplifies the decisions, searching through the remaining decisions can still be a difficult problem. So, the researchers combined their model with an algorithm that can rapidly and efficiently search through all possible decisions the robot could make.\nWith this combination, the computation time was cut down to about a minute on a standard laptop.\nThey first tested their approach in simulations where robotic hands were given tasks like moving a pen to a desired configuration, opening a door, or picking up a plate. In each instance, their model-based approach achieved the same performance as reinforcement learning, but in a fraction of the time. They saw similar results when they tested their model in hardware on real robotic arms.\n“The same ideas that enable whole-body manipulation also work for planning with dexterous, human-like hands. Previously, most researchers said that reinforcement learning was the only approach that scaled to dexterous hands, but Terry and Tao showed that by taking this key idea of (randomized) smoothing from reinforcement learning, they can make more traditional planning methods work extremely well, too,” Tedrake says.\nHowever, the model they developed relies on a simpler approximation of the real world, so it cannot handle very dynamic motions, such as objects falling. While effective for slower manipulation tasks, their approach cannot create a plan that would enable a robot to toss a can into a trash bin, for instance. In the future, the researchers plan to enhance their technique so it could tackle these highly dynamic motions.\n“If you study your models carefully and really understand the problem you are trying to solve, there are definitely some gains you can achieve. There are benefits to doing things that are beyond the black box,” Suh says.\nThis work is funded, in part, by Amazon, MIT Lincoln Laboratory, the National Science Foundation, and the Ocado Group.\n",
    "published": "2023-08-24",
    "timestamp": "2023-08-30T13:26:06.909173"
  },
  {
    "unique_id": "34c91399-8efe-5e1a-a283-8c9a366165f0",
    "title": "Defining the public interest in new technologies",
    "description": "New online journal seeks to bring together the MIT community to discuss the social responsibilities of individuals who design, implement, and evaluate technologies.",
    "link": "https://news.mit.edu/2023/defining-public-interest-new-technologies-0613",
    "blog_text": "How are waves of disruptive technologies, such as more advanced versions of artificial intelligence systems, changing the way we work, live, and play? Are there pathways that academics, practitioners, innovators, and entrepreneurs ought to be pursuing to ensure that the largest share of the benefits associated with new technologies uplift the most marginalized populations? What professional training is needed to ensure that this happens? What responsibility do creators of new or repurposed technologies have when they, and their organizations, create products or systems that might have adverse societal consequences? We are in an era in need of clear professional guidelines and norms, to say nothing of laws and regulations regarding the social impacts of new technologies.\nPublic interest technology, as an emerging field, aims to help shift the scholarly focus from the technologies to the technologists. To support this nascent field, students, faculty, and staff at MIT are launching a conversation to encourage technologists from different fields to confront the ethical and moral dilemmas that require them to redefine best practices in the face of ever-changing societal needs and norms. \nThe Public Interest Technologist (TPIT) is a new online publication that seeks to bring together the MIT community to define and discuss the social responsibilities of individuals who design, implement, and evaluate technologies, especially in new fields. The editorial team for this publication has identified public interest technology as a new multidisciplinary field that emphasizes the benefits that could flow from both old and new technologies as they are developed in the most responsible fashion.\n“As the pace of technology innovation quickens, the impacts, often unexpected, of new technologies generate gains and losses. Past experience with technological innovation has demonstrated that those diverse gains and losses are distributed unequally,” says Lawrence Susskind, the Ford Professor of Urban and Environmental Planning at the MIT Department of Urban Studies and Planning and a member of TPIT’s editorial team. “I think that those of us who care, and those of us with leadership roles in this field, have a responsibility to take concerted action to minimize the most harmful effects while ensuring that benefits reach those most in need. We see this publication as a means to move in this direction.” \nFraming the public interest in tech design and development\nAs one example of technology’s recent impact on society, the Covid-19 pandemic dramatically changed how we work and commute. Among other shifts, public transit agencies have been forced to contend with a new normal.\nIn an interview with Emilie Flamme, an MIT graduate student in city planning and a TPIT editor, Jim Aloisi and Jinhua Zhao of MIT's Transit Lab propose a way: “to modernize and optimize transit for a labor workforce currently experiencing shortages. To implement this process, they underscore that defining the public interest involves co-defining questions that public agencies and staff must answer with the public. Aloisi and Zhao note that their Transit Lab emphasizes the question of what is in the public interest. Public technology is at the heart of the work they do, and Zhao wonders whether students receiving technological training get enough exposure and education regarding the public interest.\nFostering conversations, both at MIT and beyond\nAt MIT, TPIT’s editorial team seeks to provoke a campus-wide conversation: How do public interest technologists define their social responsibilities? Is it reasonable to assume that those who invent or implement new technologies will take some responsibility for the impacts or effects these technologies have? Who should decide what these responsibilities should be? Do norms need to be enforced?\nMembers of 63 universities, including MIT, have formed a coalition with the support of New America Foundation to share ideas about public interest technology (PIT). Should it be the focus of new degree programs? What research questions regarding PIT deserve the highest priority? The PIT-UN coalition provides grants and organizes an annual convening, including the 2023 PIT-UN Convening at Boston University in October. The Public Interest Technologist is an extension of MIT’s involvement with the PIT-UN network.\nThe editorial team at TPIT hopes to involve all MIT community members in shaping its current and future content. The team invites nominations for prospective interviewees from across the MIT community, article suggestions, and already published materials that will support a broader discussion of public interest technology at MIT. Community members are also invited to attend the PIT-UN annual meeting at Boston University this fall.\n",
    "published": "2023-06-13",
    "timestamp": "2023-08-30T13:26:06.998307"
  },
  {
    "unique_id": "3e2f6660-2f65-59a6-b1c7-507f4f730621",
    "title": "An AI challenge only humans can solve",
    "description": "In their new book, “Power and Progress,” Daron Acemoglu and Simon Johnson ask whether the benefits of AI will be shared widely or feed inequality.",
    "link": "https://news.mit.edu/2023/power-and-progress-book-ai-inequality-0517",
    "blog_text": "The Dark Ages were not entirely dark. Advances in agriculture and building technology increased Medieval wealth and led to a wave of cathedral construction in Europe. However, it was a time of profound inequality. Elites captured virtually all economic gains. In Britain, as Canterbury Cathedral soared upward, peasants had no net increase in wealth between 1100 and 1300. Life expectancy hovered around 25 years. Chronic malnutrition was rampant.\n\n“We’ve been struggling to share prosperity for a long time,” says MIT Professor Simon Johnson. “Every cathedral that your parents dragged you to see in Europe is a symbol of despair and expropriation, made possible by higher productivity.”\n\nAt a glance, this might not seem relevant to life in 2023. But Johnson and his MIT colleague Daron Acemoglu, both economists, think it is. Technology drives economic progress. As innovations take hold, one perpetual question is: Who benefits?\n\nThis applies, the scholars believe, to automation and artificial intelligence, which is the focus of a new book by Acemoglu and Johnson, “Power and Progress: Our 1000-Year Struggle Over Technology and Prosperity,” published this week by PublicAffairs. In it, they examine who reaped the rewards from past innovations and who may gain from AI today, economically and politically.\n\n“The book is about the choices we make with technology,” Johnson says. “That’s a very MIT type of theme. But a lot of people feel technology just descends on you, and you have to live with it.”\n\nAI could develop as a beneficial force, Johnson says. However, he adds, “Many algorithms are being designed to try to replace humans as much as possible. We think that’s entirely wrong. The way we make progress with technology is by making machines useful to people, not displacing them. In the past we have had automation, but with new tasks for people to do and sufficient countervailing power in society.”\n\nToday, AI is a tool of social control for some governments that also creates riches for a small number of people, according to Acemoglu and Johnson. “The current path of AI is neither good for the economy nor for democracy, and these two problems, unfortunately, reinforce each other,” they write.\n\nA return to shared prosperity?\n\nAcemoglu and Johnson have collaborated before; in the early 2000s, with political scientist James Robinson, they produced influential papers about politics and economic progress. Acemoglu, an Institute Professor at MIT, also co-authored with Robinson the books “Why Nations Fail” (2012), about political institutions and growth, and “The Narrow Corridor” (2019), which casts liberty as the never-assured outcome of social struggle.\n\nJohnson, the Ronald A. Kurtz Professor of Entrepreneurship at the MIT Sloan School of Management, wrote “13 Bankers” (2010), about finance reform, and, with MIT economist Jonathan Gruber, “Jump-Starting America” (2019), a call for more investment in scientific research.\n\nIn “Power and Progress,” the authors emphasize that technology has created remarkable long-term benefits. As they write, “we are greatly better off than our ancestors,” and “scientific and technological progress is a vital part of that story.”\n\nStill, a lot of suffering and oppression has occurred while the long term is unfolding, and not just during Medieval times.  \n\n“It was a 100-year struggle during the Industrial Revolution for workers to get any cut of these massive productivity gains in textiles and railways,” Johnson observes. Broader progress has come through increased labor power and electoral government; when the U.S. economy grew spectacularly for three decades after World War II, gains were widely distributed, though that has not been the case recently.\n\n“We’re suggesting we can get back onto that path of shared prosperity, reharness technology for everybody, and get productivity gains,” Johnson says. “We had all that in the postwar period. We can get it back, but not with the current form of our machine intelligence obsession. That, we think, is undermining prosperity in the U.S. and around the world.”\n\nA call for “machine usefulness,” not “so-so automation” \n\nWhat do Acemoglu and Johnson think is deficient about AI? For one thing, they believe the development of AI is too focused on mimicking human intelligence. The scholars are skeptical of the notion that AI mirrors human thinking all told — even things like the chess program AlphaZero, which they regard more as a specialized set of instructions.\n\nOr, for instance, image recognition programs — Is that a husky or a wolf? — use large data sets of past human decisions to build predictive models. But these are often correlation-dependent (a husky is more likely to be in front of your house), and can’t replicate the same cues humans rely on. Researchers know this, of course, and keep refining their tools. But Acemoglu and Robinson contend that many AI programs are less agile than the human mind, and suboptimal replacements for it, even as AI is designed to replace human work.\n\nAcemoglu, who has published many papers on automation and robots, calls these replacement tools “so-so technologies.” A supermarket self-checkout machine does not add meaningful economic productivity; it just transfers work to customers and wealth to shareholders. Or, among more sophisticated AI tools, for instance, a customer service line using AI that doesn’t address a given problem can frustrate people, leading them to vent once they do reach a human and making the whole process less efficient.\n\nAll told, Acemoglu and Johnson write, “neither traditional digital technologies nor AI can perform essential tasks that involve social interaction, adaptation, flexibility, and communication.”\n\nInstead, growth-minded economists prefer technologies creating “marginal productivity” gains, which compel firms to hire more workers. Instead of aiming to eliminate medical specialists like radiologists, a much-forecast AI development that has not occurred, Acemoglu and Johnson suggest AI tools might expand what home health care workers can do, and make their services more valuable, without reducing workers in the sector.\n\n“We think there is a fork in the road, and it’s not too late — AI is a very good opportunity to reassert machine usefulness as a philosophy of design,” Johnson says. “And to look for ways to put tools in the hands of workers, including lower-wage workers.”\n\nDefining the discussion\n\nAnother set of AI issues Acemoglu and Johnson are concerned about extend directly into politics: Surveillance technologies, facial-recognition tools, intensive data collection, and AI-spread misinformation.\n\nChina deploys AI to create “social credit” scores for citizens, along with heavy surveillance, while tightly restricting freedom of expression. Elsewhere, social media platforms use algorithms to influence what users see; by emphasizing “engagement” above other priorities, they can spread harmful misinformation.\n\nIndeed, throughout “Power and Progress,” Acemoglu and Johnson emphasize that the use of AI can set up self-reinforcing dynamics in which those who benefit economically can gain political influence and power at the expense of wider democratic participation.\n\nTo alter this trajectory, Acemoglu and Johnson advocate for an extensive menu of policy responses, including data ownership for internet users (an idea of technologist Jaron Lanier); tax reform that rewards employment more than automation; government support for a diversity of high-tech research directions; repealing Section 230 of the 1996 Communications Decency Act, which protects online platforms from regulation or legal action based on the content they host; and a digital advertising tax (aimed to limit the profitability of algorithm-driven misinformation).\n\nJohnson believes people of all ideologies have incentives to support such measures: “The point we’re making is not a partisan point,” he says.\n\nOther scholars have praised “Power and Progress.” Michael Sandel, the Anne T. and Robert M. Bass Professor of Government at Harvard University, has called it a “humane and hopeful book” that “shows how we can steer technology to promote the public good,” and is “required reading for everyone who cares about the fate of democracy in a digital age.”\n\nFor their part, Acemoglu and Johnson want to broaden the public discussion of AI beyond industry leaders, discard notions about the AI inevitability, and think again about human agency, social priorities, and economic possibilities.\n\n“Debates on new technology ought to center not just on the brilliance of new products and algorithms but on whether they are working for the people or against the people,” they write.\n\n“We need these discussions,” Johnson says. “There’s nothing inherent in technology. It’s within our control. Even if you think we can’t say no to new technology, you can channel it, and get better outcomes from it, if you talk about it.”\n",
    "published": "2023-05-17",
    "timestamp": "2023-08-30T13:26:07.041422"
  },
  {
    "unique_id": "3e9f5fde-9dce-5c16-ac97-18d0d8b3eddf",
    "title": "Educating national security leaders on artificial intelligence",
    "description": "Experts from MIT’s School of Engineering, Schwarzman College of Computing, and Sloan Executive Education educate national security leaders in AI fundamentals.",
    "link": "https://news.mit.edu/2023/educating-national-security-leaders-ai-0630",
    "blog_text": "Understanding artificial intelligence and how it relates to matters of national security has become a top priority for military and government leaders in recent years. A new three-day custom program entitled “Artificial Intelligence for National Security Leaders” — AI4NSL for short — aims to educate leaders who may not have a technical background on the basics of AI, machine learning, and data science, and how these topics intersect with national security.\n“National security fundamentally is about two things: getting information out of sensors and processing that information. These are two things that AI excels at. The AI4NSL class engages national security leaders in understanding how to navigate the benefits and opportunities that AI affords, while also understanding its potential negative consequences,” says Aleksander Madry, the Cadence Design Systems Professor at MIT and one of the course’s faculty directors.\nOrganized jointly by MIT’s School of Engineering, MIT Stephen A. Schwarzman College of Computing, and MIT Sloan Executive Education, AI4NSL wrapped up its fifth cohort in April. The course brings leaders from every branch of the U.S. military, as well as some foreign military leaders from NATO, to MIT’s campus, where they learn from faculty experts on a variety of technical topics in AI, as well as how to navigate organizational challenges that arise in this context.\n“We set out to put together a real executive education class on AI for senior national security leaders,” says Madry. “For three days, we are teaching these leaders not only an understanding of what this technology is about, but also how to best adopt these technologies organizationally.”\nThe original idea sprang from discussions with senior U.S. Air Force (USAF) leaders and members of the Department of the Air Force (DAF)-MIT AI Accelerator in 2019.\nAccording to Major John Radovan, former deputy director of the DAF-MIT AI Accelerator, in recent years it has become clear that national security leaders needed a deeper understanding of AI technologies and its implications on security, warfare, and military operations. In February 2020, Radovan and his team at the DAF-MIT AI Accelerator started building a custom course to help guide senior leaders in their discussions about AI.\n“This is the only course out there that is focused on AI specifically for national security,” says Radovan. “We didn’t want to make this course just for members of the Air Force — it had to be for all branches of the military. If we are going to operate as a joint force, we need to have the same vocabulary and the same mental models about how to use this technology.”\nAfter a pilot program in collaboration with MIT Open Learning and the MIT Computer Science and Artificial Intelligence Laboratory, Radovan connected with faculty at the School of Engineering and MIT Schwarzman College of Computing, including Madry, to refine the course’s curriculum. They enlisted the help of colleagues and faculty at MIT Sloan Executive Education to refine the class’s curriculum and cater the content to its audience. The result of this cross-school collaboration was a new iteration of AI4NSL, which was launched last summer.\nIn addition to providing participants with a basic overview of AI technologies, the course places a heavy emphasis on organizational planning and implementation.\n“What we wanted to do was to create smart consumers at the command level. The idea was to present this content at a higher level so that people could understand the key frameworks, which will guide their thinking around the use and adoption of this material,” says Roberto Fernandez, the William F. Pounds Professor of Management and one of the AI4NSL instructors, as well as the other course’s faculty director.\nDuring the three-day course, instructors from MIT’s Department of Electrical Engineering and Computer Science, Department of Aeronautics and Astronautics, and MIT Sloan School of Management cover a wide range of topics.\nThe first half of the course starts with a basic overview of concepts including AI, machine learning, deep learning, and the role of data. Instructors also present the problems and pitfalls of using AI technologies, including the potential for adversarial manipulation of machine learning systems, privacy challenges, and ethical considerations.\nIn the middle of day two, the course shifts to examine the organizational perspective, encouraging participants to consider how to effectively implement these technologies in their own units.\n“What’s exciting about this course is the way it is formatted first in terms of understanding AI, machine learning, what data is, and how data feeds AI, and then giving participants a framework to go back to their units and build a strategy to make this work,” says Colonel Michelle Goyette, director of the Army Strategic Education Program at the Army War College and an AI4NSL participant.\nThroughout the course, breakout sessions provide participants with an opportunity to collaborate and problem-solve on an exercise together. These breakout sessions build upon one another as the participants are exposed to new concepts related to AI.\n“The breakout sessions have been distinctive because they force you to establish relationships with people you don’t know, so the networking aspect is key. Any time you can do more than receive information and actually get into the application of what you were taught, that really enhances the learning environment,” says Lieutenant General Brian Robinson, the commander of Air Education and Training Command for the USAF and an AI4NSL participant.\nThis spirit of teamwork, collaboration, and bringing together individuals from different backgrounds permeates the three-day program. The AI4NSL classroom not only brings together national security leaders from all branches of the military, it also brings together faculty from three schools across MIT.\n“One of the things that's most exciting about this program is the kind of overarching theme of collaboration,” says Rob Dietel, director of executive programs at Sloan School of Management. “We're not drawing just from the MIT Sloan faculty, we're bringing in top faculty from the Schwarzman College of Computing and the School of Engineering. It's wonderful to be able to tap into those resources that are here on MIT’s campus to really make it the most impactful program that we can.”\nAs new developments in generative AI, such as ChatGPT, and machine learning alter the national security landscape, the organizers at AI4NSL will continue to update the curriculum to ensure it is preparing leaders to understand the implications for their respective units.\n“The rate of change for AI and national security is so fast right now that it's challenging to keep up, and that's part of the reason we've designed this program. We've brought in some of our world-class faculty from different parts of MIT to really address the changing dynamic of AI,” adds Dietel.\n",
    "published": "2023-06-30",
    "timestamp": "2023-08-30T13:26:06.971559"
  },
  {
    "unique_id": "4be49c33-e9cb-598d-95f6-e7deec75c806",
    "title": "New tool helps people choose the right method for evaluating AI models",
    "description": "Selecting the right method gives users a more accurate picture of how their model is behaving, so they are better equipped to correctly interpret its predictions.",
    "link": "https://news.mit.edu/2023/new-tool-helps-people-choose-right-method-evaluating-ai-models-0531",
    "blog_text": "When machine-learning models are deployed in real-world situations, perhaps to flag potential disease in X-rays for a radiologist to review, human users need to know when to trust the model’s predictions.\n\nBut machine-learning models are so large and complex that even the scientists who design them don’t understand exactly how the models make predictions. So, they create techniques known as saliency methods that seek to explain model behavior.\n\nWith new methods being released all the time, researchers from MIT and IBM Research created a tool to help users choose the best saliency method for their particular task. They developed saliency cards, which provide standardized documentation of how a method operates, including its strengths and weaknesses and explanations to help users interpret it correctly.\n\nThey hope that, armed with this information, users can deliberately select an appropriate saliency method for both the type of machine-learning model they are using and the task that model is performing, explains co-lead author Angie Boggust, a graduate student in electrical engineering and computer science at MIT and member of the Visualization Group of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL).\n\nInterviews with AI researchers and experts from other fields revealed that the cards help people quickly conduct a side-by-side comparison of different methods and pick a task-appropriate technique. Choosing the right method gives users a more accurate picture of how their model is behaving, so they are better equipped to correctly interpret its predictions.\n\n“Saliency cards are designed to give a quick, glanceable summary of a saliency method and also break it down into the most critical, human-centric attributes. They are really designed for everyone, from machine-learning researchers to lay users who are trying to understand which method to use and choose one for the first time,” says Boggust.\n\nJoining Boggust on the paper are co-lead author Harini Suresh, an MIT postdoc; Hendrik Strobelt, a senior research scientist at IBM Research; John Guttag, the Dugald C. Jackson Professor of Computer Science and Electrical Engineering at MIT; and senior author Arvind Satyanarayan, associate professor of computer science at MIT who leads the Visualization Group in CSAIL. The research will be presented at the ACM Conference on Fairness, Accountability, and Transparency.\n\nPicking the right method\n\nThe researchers have previously evaluated saliency methods using the notion of faithfulness. In this context, faithfulness captures how accurately a method reflects a model’s decision-making process.\n\nBut faithfulness is not black-and-white, Boggust explains. A method might perform well under one test of faithfulness, but fail another. With so many saliency methods, and so many possible evaluations, users often settle on a method because it is popular or a colleague has used it.\n\nHowever, picking the “wrong” method can have serious consequences. For instance, one saliency method, known as integrated gradients, compares the importance of features in an image to a meaningless baseline. The features with the largest importance over the baseline are most meaningful to the model’s prediction. This method typically uses all 0s as the baseline, but if applied to images, all 0s equates to the color black.\n\n“It will tell you that any black pixels in your image aren’t important, even if they are, because they are identical to that meaningless baseline. This could be a big deal if you are looking at X-rays since black could be meaningful to clinicians,” says Boggust. \n\nSaliency cards can help users avoid these types of problems by summarizing how a saliency method works in terms of 10 user-focused attributes. The attributes capture the way saliency is calculated, the relationship between the saliency method and the model, and how a user perceives its outputs.\n\nFor example, one attribute is hyperparameter dependence, which measures how sensitive that saliency method is to user-specified parameters. A saliency card for integrated gradients would describe its parameters and how they affect its performance. With the card, a user could quickly see that the default parameters — a baseline of all 0s — might generate misleading results when evaluating X-rays.\n\nThe cards could also be useful for scientists by exposing gaps in the research space. For instance, the MIT researchers were unable to identify a saliency method that was computationally efficient, but could also be applied to any machine-learning model.\n\n“Can we fill that gap? Is there a saliency method that can do both things? Or maybe these two ideas are theoretically in conflict with one another,” Boggust says.\n\nShowing their cards\n\nOnce they had created several cards, the team conducted a user study with eight domain experts, from computer scientists to a radiologist who was unfamiliar with machine learning. During interviews, all participants said the concise descriptions helped them prioritize attributes and compare methods. And even though he was unfamiliar with machine learning, the radiologist was able to understand the cards and use them to take part in the process of choosing a saliency method, Boggust says.\n\nThe interviews also revealed a few surprises. Researchers often expect that clinicians want a method that is sharp, meaning it focuses on a particular object in a medical image. But the clinician in this study actually preferred some noise in medical images to help them attenuate uncertainty.\n\n“As we broke it down into these different attributes and asked people, not a single person had the same priorities as anyone else in the study, even when they were in the same role,” she says.\n\nMoving forward, the researchers want to explore some of the more under-evaluated attributes and perhaps design task-specific saliency methods. They also want to develop a better understanding of how people perceive saliency method outputs, which could lead to better visualizations. In addition, they are hosting their work on a public repository so others can provide feedback that will drive future work, Boggust says.\n\n“We are really hopeful that these will be living documents that grow as new saliency methods and evaluations are developed. In the end, this is really just the start of a larger conversation around what the attributes of a saliency method are and how those play into different tasks,” she says.\n\nThe research was supported, in part, by the MIT-IBM Watson AI Lab, the U.S. Air Force Research Laboratory, and the U.S. Air Force Artificial Intelligence Accelerator.\n\n",
    "published": "2023-05-31",
    "timestamp": "2023-08-30T13:26:07.023010"
  },
  {
    "unique_id": "4ea34ed3-2db6-58f5-94a2-47951d344609",
    "title": "Using AI, scientists find a drug that could combat drug-resistant infections",
    "description": "The machine-learning algorithm identified a compound that kills Acinetobacter baumannii, a bacterium that lurks in many hospital settings.",
    "link": "https://news.mit.edu/2023/using-ai-scientists-combat-drug-resistant-infections-0525",
    "blog_text": "Using an artificial intelligence algorithm, researchers at MIT and McMaster University have identified a new antibiotic that can kill a type of bacteria that is responsible for many drug-resistant infections.\n\nIf developed for use in patients, the drug could help to combat Acinetobacter baumannii, a species of bacteria that is often found in hospitals and can lead to pneumonia, meningitis, and other serious infections. The microbe is also a leading cause of infections in wounded soldiers in Iraq and Afghanistan.\n“Acinetobacter can survive on hospital doorknobs and equipment for long periods of time, and it can take up antibiotic resistance genes from its environment. It’s really common now to find A. baumannii isolates that are resistant to nearly every antibiotic,” says Jonathan Stokes, a former MIT postdoc who is now an assistant professor of biochemistry and biomedical sciences at McMaster University.\nThe researchers identified the new drug from a library of nearly 7,000 potential drug compounds using a machine-learning model that they trained to evaluate whether a chemical compound will inhibit the growth of A. baumannii.\n“This finding further supports the premise that AI can significantly accelerate and expand our search for novel antibiotics,” says James Collins, the Termeer Professor of Medical Engineering and Science in MIT’s Institute for Medical Engineering and Science (IMES) and Department of Biological Engineering. “I’m excited that this work shows that we can use AI to help combat problematic pathogens such as A. baumannii.”\nCollins and Stokes are the senior authors of the new study, which appears today in Nature Chemical Biology. The paper’s lead authors are McMaster University graduate students Gary Liu and Denise Catacutan and recent McMaster graduate Khushi Rathod.\n\nDrug discovery\n\nOver the past several decades, many pathogenic bacteria have become increasingly resistant to existing antibiotics, while very few new antibiotics have been developed.\n\nSeveral years ago, Collins, Stokes, and MIT Professor Regina Barzilay (who is also an author on the new study), set out to combat this growing problem by using machine learning, a type of artificial intelligence that can learn to recognize patterns in vast amounts of data. Collins and Barzilay, who co-direct MIT’s Abdul Latif Jameel Clinic for Machine Learning in Health, hoped this approach could be used to identify new antibiotics whose chemical structures are different from any existing drugs.\n\nIn their initial demonstration, the researchers trained a machine-learning algorithm to identify chemical structures that could inhibit growth of E. coli. In a screen of more than 100 million compounds, that algorithm yielded a molecule that the researchers called halicin, after the fictional artificial intelligence system from “2001: A Space Odyssey.” This molecule, they showed, could kill not only E. coli but several other bacterial species that are resistant to treatment.\n\n“After that paper, when we showed that these machine-learning approaches can work well for complex antibiotic discovery tasks, we turned our attention to what I perceive to be public enemy No. 1 for multidrug-resistant bacterial infections, which is Acinetobacter,” Stokes says.\n\nTo obtain training data for their computational model, the researchers first exposed A. baumannii grown in a lab dish to about 7,500 different chemical compounds to see which ones could inhibit growth of the microbe. Then they fed the structure of each molecule into the model. They also told the model whether each structure could inhibit bacterial growth or not. This allowed the algorithm to learn chemical features associated with growth inhibition.\n\nOnce the model was trained, the researchers used it to analyze a set of 6,680 compounds it had not seen before, which came from the Drug Repurposing Hub at the Broad Institute. This analysis, which took less than two hours, yielded a few hundred top hits. Of these, the researchers chose 240 to test experimentally in the lab, focusing on compounds with structures that were different from those of existing antibiotics or molecules from the training data.\n\nThose tests yielded nine antibiotics, including one that was very potent. This compound, which was originally explored as a potential diabetes drug, turned out to be extremely effective at killing A. baumannii but had no effect on other species of bacteria including Pseudomonas aeruginosa, Staphylococcus aureus, and carbapenem-resistant Enterobacteriaceae.\n\nThis “narrow spectrum” killing ability is a desirable feature for antibiotics because it minimizes the risk of bacteria rapidly spreading resistance against the drug. Another advantage is that the drug would likely spare the beneficial bacteria that live in the human gut and help to suppress opportunistic infections such as Clostridium difficile.\n\n“Antibiotics often have to be administered systemically, and the last thing you want to do is cause significant dysbiosis and open up these already sick patients to secondary infections,” Stokes says.\n\nA novel mechanism\n\nIn studies in mice, the researchers showed that the drug, which they named abaucin, could treat wound infections caused by A. baumannii. They also showed, in lab tests, that it works against a variety of drug-resistant A. baumannii strains isolated from human patients.\n\nFurther experiments revealed that the drug kills cells by interfering with a process known as lipoprotein trafficking, which cells use to transport proteins from the interior of the cell to the cell envelope. Specifically, the drug appears to inhibit LolE, a protein involved in this process.\n\nAll Gram-negative bacteria express this enzyme, so the researchers were surprised to find that abaucin is so selective in targeting A. baumannii. They hypothesize that slight differences in how A. baumannii performs this task might account for the drug’s selectivity.\n\n“We haven’t finalized the experimental data acquisition yet, but we think it’s because A. baumannii does lipoprotein trafficking a little bit differently than other Gram-negative species. We believe that’s why we’re getting this narrow spectrum activity,” Stokes says.\n\nStokes’ lab is now working with other researchers at McMaster to optimize the medicinal properties of the compound, in hopes of developing it for eventual use in patients.\n\nThe researchers also plan to use their modeling approach to identify potential antibiotics for other types of drug-resistant infections, including those caused by Staphylococcus aureus and Pseudomonas aeruginosa.\n\nThe research was funded by the David Braley Center for Antibiotic Discovery, the Weston Family Foundation, the Audacious Project, the C3.ai Digital Transformation Institute, the Abdul Latif Jameel Clinic for Machine Learning in Health, the DTRA Discovery of Medical Countermeasures Against New and Emerging Threats program, the DARPA Accelerated Molecular Discovery program, the Canadian Institutes of Health Research, Genome Canada, the Faculty of Health Sciences of McMaster University, the Boris Family, a Marshall Scholarship, and the Department of Energy Biological and Environmental Research program.\n",
    "published": "2023-05-25",
    "timestamp": "2023-08-30T13:26:07.030431"
  },
  {
    "unique_id": "4ee1c21a-d66e-590e-ad57-2056b73b765e",
    "title": "MIT scientists build a system that can generate AI models for biology research",
    "description": "BioAutoMATED, an open-source, automated machine-learning platform, aims to help democratize artificial intelligence for research labs. ",
    "link": "https://news.mit.edu/2023/bioautomated-open-source-machine-learning-platform-for-research-labs-0706",
    "blog_text": "Is it possible to build machine-learning models without machine-learning expertise?\nJim Collins, the Termeer Professor of Medical Engineering and Science in the Department of Biological Engineering at MIT and the life sciences faculty lead at the Abdul Latif Jameel Clinic for Machine Learning in Health (Jameel Clinic), along with a number of colleagues decided to tackle this problem when facing a similar conundrum. An open-access paper on their proposed solution, called BioAutoMATED, was published on June 21 in Cell Systems.\nRecruiting machine-learning researchers can be a time-consuming and financially costly process for science and engineering labs. Even with a machine-learning expert, selecting the appropriate model, formatting the dataset for the model, then fine-tuning it can dramatically change how the model performs, and takes a lot of work. \n“In your machine-learning project, how much time will you typically spend on data preparation and transformation?” asks a 2022 Google course on the Foundations of Machine Learning (ML). The two choices offered are either “Less than half the project time” or “More than half the project time.” If you guessed the latter, you would be correct; Google states that it takes over 80 percent of project time to format the data, and that’s not even taking into account the time needed to frame the problem in machine-learning terms.\n“It would take many weeks of effort to figure out the appropriate model for our dataset, and this is a really prohibitive step for a lot of folks that want to use machine learning or biology,” says Jacqueline Valeri, a fifth-year PhD student of biological engineering in Collins’s lab who is first co-author of the paper. \nBioAutoMATED is an automated machine-learning system that can select and build an appropriate model for a given dataset and even take care of the laborious task of data preprocessing, whittling down a months-long process to just a few hours. Automated machine-learning (AutoML) systems are still in a relatively nascent stage of development, with current usage primarily focused on image and text recognition, but largely unused in subfields of biology, points out first co-author and Jameel Clinic postdoc Luis Soenksen PhD '20.\n“The fundamental language of biology is based on sequences,” explains Soenksen, who earned his doctorate in the MIT Department of Mechanical Engineering. “Biological sequences such as DNA, RNA, proteins, and glycans have the amazing informational property of being intrinsically standardized, like an alphabet. A lot of AutoML tools are developed for text, so it made sense to extend it to [biological] sequences.”\nMoreover, most AutoML tools can only explore and build reduced types of models. “But you can’t really know from the start of a project which model will be best for your dataset,” Valeri says. “By incorporating multiple tools under one umbrella tool, we really allow a much larger search space than any individual AutoML tool could achieve on its own.”\nBioAutoMATED’s repertoire of supervised ML models includes three types: binary classification models (dividing data into two classes), multi-class classification models (dividing data into multiple classes), and regression models (fitting continuous numerical values or measuring the strength of key relationships between variables). BioAutoMATED is even able to help determine how much data is required to appropriately train the chosen model.\n\"Our tool explores models that are better-suited for smaller, sparser biological datasets as well as more complex neural networks,” Valeri says. This is an advantage for research groups with new data that may or may not be suited for a machine learning problem.\n\"Conducting novel and successful experiments at the intersection of biology and machine learning can cost a lot of money,” Soenksen explains. \"Currently, biology-centric labs need to invest in significant digital infrastructure and AI-ML trained human resources before they can even see if their ideas are poised to pan out. We want to lower these barriers for domain experts in biology.” With BioAutoMATED, researchers have the freedom to run initial experiments to assess if it’s worthwhile to hire a machine-learning expert to build a different model for further experimentation. \nThe open-source code is publicly available and, researchers emphasize, it is easy to run. “What we would love to see is for people to take our code, improve it, and collaborate with larger communities to make it a tool for all,” Soenksen says. “We want to prime the biological research community and generate awareness related to AutoML techniques, as a seriously useful pathway that could merge rigorous biological practice with fast-paced AI-ML practice better than it is achieved today.”\nCollins, the senior author on the paper, is also affiliated with the MIT Institute for Medical Engineering and Science, the Harvard-MIT Program in Health Sciences and Technology, the Broad Institute of MIT and Harvard, and the Wyss Institute. Additional MIT contributors to the paper include Katherine M. Collins '21; Nicolaas M. Angenent-Mari PhD '21; Felix Wong, a former postdoc in the Department of Biological Engineering, IMES, and the Broad Institute; and Timothy K. Lu, a professor of biological engineering and of electrical engineering and computer science.\nThis work was supported, in part, by a Defense Threat Reduction Agency grant, the Defense Advance Research Projects Agency SD2 program, the Paul G. Allen Frontiers Group, the Wyss Institute for Biologically Inspired Engineering of Harvard University; an MIT-Takeda Fellowship, a Siebel Foundation Scholarship, a CONACyT grant, an MIT-TATA Center fellowship, a Johnson & Johnson Undergraduate Research Scholarship, a Barry Goldwater Scholarship, a Marshall Scholarship, Cambridge Trust, and the National Institute of Allergy and Infectious Diseases of the National Institutes of Health. This work is part of the Antibiotics-AI Project, which is supported by the Audacious Project, Flu Lab, LLC, the Sea Grape Foundation, Rosamund Zander and Hansjorg Wyss for the Wyss Foundation, and an anonymous donor.\n",
    "published": "2023-07-06",
    "timestamp": "2023-08-30T13:26:06.966949"
  },
  {
    "unique_id": "54e3beb5-1cfb-57d9-bec0-d455ebd1d07a",
    "title": "A better way to study ocean currents",
    "description": "A new machine-learning model makes more accurate predictions about ocean currents, which could help with tracking plastic pollution and oil spills, and aid in search and rescue.",
    "link": "https://news.mit.edu/2023/new-machine-learning-model-ocean-currents-0517",
    "blog_text": "To study ocean currents, scientists release GPS-tagged buoys in the ocean and record their velocities to reconstruct the currents that transport them. These buoy data are also used to identify “divergences,” which are areas where water rises up from below the surface or sinks beneath it.\n\nBy accurately predicting currents and pinpointing divergences, scientists can more precisely forecast the weather, approximate how oil will spread after a spill, or measure energy transfer in the ocean. A new model that incorporates machine learning makes more accurate predictions than conventional models do, a new study reports.\n\nA multidisciplinary research team including computer scientists at MIT and oceanographers has found that a standard statistical model typically used on buoy data can struggle to accurately reconstruct currents or identify divergences because it makes unrealistic assumptions about the behavior of water.\n\nThe researchers developed a new model that incorporates knowledge from fluid dynamics to better reflect the physics at work in ocean currents. They show that their method, which only requires a small amount of additional computational expense, is more accurate at predicting currents and identifying divergences than the traditional model.\n\nThis new model could help oceanographers make more accurate estimates from buoy data, which would enable them to more effectively monitor the transportation of biomass (such as Sargassum seaweed), carbon, plastics, oil, and nutrients in the ocean. This information is also important for understanding and tracking climate change.\n\n“Our method captures the physical assumptions more appropriately and more accurately. In this case, we know a lot of the physics already. We are giving the model a little bit of that information so it can focus on learning the things that are important to us, like what are the currents away from the buoys, or what is this divergence and where is it happening?” says senior author Tamara Broderick, an associate professor in MIT’s Department of Electrical Engineering and Computer Science (EECS) and a member of the Laboratory for Information and Decision Systems and the Institute for Data, Systems, and Society.\n\nBroderick’s co-authors include lead author Renato Berlinghieri, an electrical engineering and computer science graduate student; Brian L. Trippe, a postdoc at Columbia University; David R. Burt and Ryan Giordano, MIT postdocs; Kaushik Srinivasan, an assistant researcher in atmospheric and ocean sciences at the University of California at Los Angeles; Tamay Özgökmen, professor in the Department of Ocean Sciences at the University of Miami; and Junfei Xia, a graduate student at the University of Miami. The research will be presented at the International Conference on Machine Learning.\n\nDiving into the data\n\nOceanographers use data on buoy velocity to predict ocean currents and identify “divergences” where water rises to the surface or sinks deeper.\n\nTo estimate currents and find divergences, oceanographers have used a machine-learning technique known as a Gaussian process, which can make predictions even when data are sparse. To work well in this case, the Gaussian process must make assumptions about the data to generate a prediction.\n\nA standard way of applying a Gaussian process to oceans data assumes the latitude and longitude components of the current are unrelated. But this assumption isn’t physically accurate. For instance, this existing model implies that a current’s divergence and its vorticity (a whirling motion of fluid) operate on the same magnitude and length scales. Ocean scientists know this is not true, Broderick says. The previous model also assumes the frame of reference matters, which means fluid would behave differently in the latitude versus the longitude direction.\n\n“We were thinking we could address these problems with a model that incorporates the physics,” she says.\n\nThey built a new model that uses what is known as a Helmholtz decomposition to accurately represent the principles of fluid dynamics. This method models an ocean current by breaking it down into a vorticity component (which captures the whirling motion) and a divergence component (which captures water rising or sinking).\n\nIn this way, they give the model some basic physics knowledge that it uses to make more accurate predictions.\n\nThis new model utilizes the same data as the old model. And while their method can be more computationally intensive, the researchers show that the additional cost is relatively small.\n\nBuoyant performance\n\nThey evaluated the new model using synthetic and real ocean buoy data. Because the synthetic data were fabricated by the researchers, they could compare the model’s predictions to ground-truth currents and divergences. But simulation involves assumptions that may not reflect real life, so the researchers also tested their model using data captured by real buoys released in the Gulf of Mexico.\n\n\n\n\nIn each case, their method demonstrated superior performance for both tasks, predicting currents and identifying divergences, when compared to the standard Gaussian process and another machine-learning approach that used a neural network. For example, in one simulation that included a vortex adjacent to an ocean current, the new method correctly predicted no divergence while the previous Gaussian process method and the neural network method both predicted a divergence with very high confidence.\n\nThe technique is also good at identifying vortices from a small set of buoys, Broderick adds.\n\nNow that they have demonstrated the effectiveness of using a Helmholtz decomposition, the researchers want to incorporate a time element into their model, since currents can vary over time as well as space. In addition, they want to better capture how noise impacts the data, such as winds that sometimes affect buoy velocity. Separating that noise from the data could make their approach more accurate.\n\n“Our hope is to take this noisily observed field of velocities from the buoys, and then say what is the actual divergence and actual vorticity, and predict away from those buoys, and we think that our new technique will be helpful for this,” she says.\n\n“The authors cleverly integrate known behaviors from fluid dynamics to model ocean currents in a flexible model,” says Massimiliano Russo, an associate biostatistician at Brigham and Women’s Hospital and instructor at Harvard Medical School, who was not involved with this work. “The resulting approach retains the flexibility to model the nonlinearity in the currents but can also characterize phenomena such as vortices and connected currents that would only be noticed if the fluid dynamic structure is integrated into the model. This is an excellent example of where a flexible model can be substantially improved with a well thought and scientifically sound specification.”\n\nThis research is supported by the Office of Naval Research through a Multi University Research Initiative (MURI) program titled \"Machine Learning for Submesoscale Characterization, Ocean Prediction, and Exploration (ML-SCOPE).\" It is also supported in part by a National Science Foundation (NSF) CAREER Award and the Rosenstiel School of Marine, Atmospheric, and Earth Science at the University of Miami.\n",
    "published": "2023-05-17",
    "timestamp": "2023-08-30T13:26:07.037459"
  },
  {
    "unique_id": "5abbbf73-92e9-508c-b69f-57e59dc72206",
    "title": "Q&A: Gabriela Sá Pessoa on Brazilian politics, human rights in the Amazon, and AI",
    "description": "The Brazilian social justice reporter is a fellow at the MIT Center for International Studies.",
    "link": "https://news.mit.edu/2023/gabriela-sa-pessoa-brazilian-politics-human-rights-amazon-ai-0606",
    "blog_text": "Gabriela Sá Pessoa is a journalist passionate about the intersection of human rights and climate change. She came to MIT from The Washington Post, where she worked from her home country of Brazil as a news researcher reporting on the Amazon, human rights violations, and environmental crimes. Before that, she held roles at two of the most influential media outlets in Brazil: Folha de S.Paulo, covering local and national politics, and UOL, where she was assigned to coronavirus coverage and later joined the investigative desk.\nSá Pessoa was awarded the 2023 Elizabeth Neuffer Fellowship by the International Women’s Media Foundation, which supports its recipient with research opportunities at MIT and further training at The Boston Globe and The New York Times. She is currently based at the MIT Center for International Studies. Recently, she sat down to talk about her work on the Amazon, recent changes in Brazilian politics, and her experience at MIT.\nQ: One focus of your reporting is human rights and environmental issues in the Amazon. As part of your fellowship, you contributed to a recent editorial in The Boston Globe on fighting deforestation in the region. Why is reporting on this topic important?\nA: For many Brazilians, the Amazon is a remote and distant territory, and people living in other parts of the country aren't fully aware of all of its problems and all of its potential. This is similar to the United States — like many people here, they don't see how they could be related to the human rights violations and the destruction of the rainforest that are happening.\nBut, we are all complicit in the destruction in some ways because the economic forces driving the deforestation of the rainforest all have a market, and these markets are everywhere, in Brazil and here in the U.S. I think it is part of journalism to show people in the U.S., Brazil, and elsewhere that we are part of the problem, and as part of the problem, we should be part of the solution by being aware of it, caring about it, and taking actions that are within our power.\nIn the U.S., for example, voters can influence policy like the current negotiations for financial support for fighting deforestation in the Amazon. And as consumers, we can be more aware — is the beef we are consuming related to deforestation? Is the timber on our construction sites coming from the Amazon?\nTruth is, in Brazil, we have turned our backs to the Amazon for so long. It’s our duty to protect it for the sake of climate change. If we don't take care of it, there will be serious consequences to our local climate, our local communities, and for the whole world. It's a huge matter of human rights because our living depends on that, both locally and globally.\nQ: Before coming to MIT, you were at The Washington Post in São Paulo, where you contributed to reporting on the recent presidential election. What changes do you expect to see with the new Lula administration?\nA: To climate and environment, the first signs were positive. But the optimism did not last a semester, as politics is imposing itself. Lula is facing increasing difficulty building a majority in a conservative Congress, over which agribusiness holds tremendous power and influence. As we speak, environmental policy is under Congress's attack. A committee in the House has just passed a ruling drowning power from the environmental minister, Marina Silva, and from the recently created National Indigenous People Ministry, led by Sonia Guajajara. Both Marina and Sonia are global ecological and human rights champions, and I wonder what the impact would be if Congress ratifies these changes. It is still unclear how it would impact the efforts to fight deforestation.\nIn addition, there is an internal dispute in the government between environmentalists and those in favor of mining and big infrastructure projects. Petrobras, the state-run oil company, is trying to get authorization to research and drill offshore oil reserves in the mouth of the Amazon River. The federal environmental protection agency did a conclusive report suspending the operation, saying it is critical and threatens the region's sensitive environment and indigenous communities. And, of course, it would be another source of greenhouse gas emissions. ​\nThat said, it's not a denialist government. I should mention the quick response from the administration to the Yanomami genocide earlier this year. In January, an independent media organization named Sumaúma reported on the deaths of over five hundred indigenous children from the Yanomami community in the Amazon over the past four years. This was a huge shock in Brazil, and the administration responded immediately. They sent task forces to the region and are now expelling the illegal miners that were bringing diseases and were ultimately responsible for these humanitarian tragedies. To be clear: It is still a problem. It's not solved. But this is already a good example of positive action.\nFighting deforestation in the Amazon and the Cerrado, another biome critical to climate regulation in Brazil, will not be easy. Rebuilding the environmental policy will take time, and the agencies responsible for enforcement are understaffed. In addition, environmental crime has become more sophisticated, connecting with other major criminal organizations in the country. In April, for the first time, there was a reduction in deforestation in the Amazon after two consecutive months of higher numbers. These are still preliminary data, and it is still too early to confirm whether they signal a turning point and may indicate a tendency for deforestation to decrease. On the other hand, the Cerrado registered record deforestation in April.\nThere are problems everywhere in the economy and politics that Lula will have to face. In the first week of the new term, on Jan. 8, we saw an insurrection in Brasília, the country’s capital, from Bolsonaro voters who wouldn’t accept the election results. The events resembled what Americans saw in the Capitol attacks in 2021. We also seem to have imported problems from the United States, like mass killings in schools. We never used to have them in Brazil, but we are seeing them now. I'm curious to see how the country will address those problems and if the U.S. can also inspire solutions to that. That’s something I’m thinking about, being here: Are there solutions here? What are they?\nQ: What have you learned so far from MIT and your fellowship? \nA: It's hard to put everything into words! I'm mostly taking courses and attending lectures on pressing issues to humanity, like existential threats such as climate change, artificial intelligence, biosecurity, and more.\nI’m learning about all these issues, but also, as a journalist, I think that I’m learning more about how I can incorporate the scientific approach into my work; for example, being more pro-positive. I am already a rigorous journalist, but I am thinking about how I can be more rigorous and more transparent about my methods. Being in the academic and scientific environment is inspiring that way.\nI am also learning a lot about how to cover scientific topics and thinking about how technology can offer us solutions (and problems). I’m learning so much that I think I will need some time to digest and fully understand what this period means for me!\nQ: You mentioned artificial intelligence. Would you like to weigh in on this subject and what you have been learning? \nA: It has been a particularly good semester to be at MIT. Generative artificial intelligence, which became more popular after ChatGPT, has been a topic of intense discussion this semester, and I was able to attend many classes, seminars, and events about AI here, especially from a policy perspective.\nAlgorithms have influenced the economy, society, and public health for many years. It has had great outcomes, but also injustice. Popular systems like ChatGPT have made this technology incredibly popular and accessible, even for those with no computer knowledge. This is scary and, at the same time, very exciting. Here, I learned that we need guardrails for artificial intelligence, just like other technologies. Think of the pharmaceutical or automobile industries, which have to meet safety criteria before putting a new product on the market. But with artificial intelligence, it's going to be different; supply chains are very complex and sometimes not very transparent, and the speed at which new resources develop is so fast that it challenges the policymaker’s ability to respond.\nArtificial intelligence is changing the world radically. It's exciting to have the privilege of being here and seeing these discussions take place. After all, I have a future to report on. At least, I hope so!\nQ: What are you working on going forward?\nA: After MIT, I am going to New York, where I'll be working with The New York Times in their internship program. I'm really excited about that because it will be a different pace from MIT. I am also doing research on carbon credit markets and hope to continue that project, either in a reporting or academic environment. \nHonestly, I feel inspired to keep studying. I would love to spend more time here at MIT. I would love to do a master's or join any program here. I’m going to work on coming back to academia because I think that I need to learn more from the academic environment. I hope that it's at MIT because honestly, it's the most exciting environment that I’ve ever been in, with all the people here from different fields and different backgrounds. I'm not a scientist, but it's inspiring to be with them, and if there's a way that I could contribute to their work in a way that they're contributing to my work, I'll be thrilled to spend more time here.\n",
    "published": "2023-06-06",
    "timestamp": "2023-08-30T13:26:07.015248"
  },
  {
    "unique_id": "5b47d4a3-3ecc-5f34-913c-7b02cc334e2a",
    "title": "MIT-Pillar AI Collective announces first seed grant recipients",
    "description": "Six teams conducting research in AI, data science, and machine learning receive funding for projects that have potential commercial applications.  ",
    "link": "https://news.mit.edu/2023/mit-pillar-ai-collective-first-seed-grant-recipients-0622",
    "blog_text": "The MIT-Pillar AI Collective has announced its first six grant recipients. Students, alumni, and postdocs working on a broad range of topics in artificial intelligence, machine learning, and data science will receive funding and support for research projects that could translate into commercially viable products or companies. These grants are intended to help students explore commercial applications for their research, and eventually drive that commercialization through the creation of a startup.\n“These tremendous students and postdocs are working on projects that have the potential to be truly transformative across a diverse range of industries. It’s thrilling to think that the novel research these teams are conducting could lead to the founding of startups that revolutionize everything from drug delivery to video conferencing,” says Anantha Chandrakasan, dean of the School of Engineering and the Vannevar Bush Professor of Electrical Engineering and Computer Science.\nLaunched in September 2022, the MIT-Pillar AI Collective is a pilot program funded by a $1 million gift from Pillar VC that aims to cultivate prospective entrepreneurs and drive innovation in areas related to AI. Administered by the MIT Deshpande Center for Technological Innovation, the AI Collective centers on the market discovery process, advancing projects through market research, customer discovery, and prototyping. Graduate students and postdocs supported by the program work toward the development of minimum viable products.\n“In addition to funding, the MIT-Pillar AI Collective provides grant recipients with mentorship and guidance. With the rapid advancement of AI technologies, this type of support is critical to ensure students and postdocs are able to access the resources required to move quickly in this fast-pace environment,” says Jinane Abounadi, managing director of the MIT-Pillar AI Collective.\nThe six inaugural recipients will receive support in identifying key milestones and advice from experienced entrepreneurs. The AI Collective assists seed grant recipients in gathering feedback from potential end-users, as well as getting insights from early-stage investors. The program also organizes community events, including a “Founder Talks” speaker series, and other team-building activities.   \n“Each one of these grant recipients exhibits an entrepreneurial spirit. It is exciting to provide support and guidance as they start a journey that could one day see them as founders and leaders of successful companies,” adds Jamie Goldstein ’89, founder of Pillar VC.\nThe first cohort of grant recipients include the following projects:\nPredictive query interface\nAbdullah Alomar SM '21, a PhD candidate studying electrical engineering and computer science, is building a predictive query interface for time series databases to better forecast demand and financial data. This user-friendly interface can help alleviate some of the bottlenecks and issues related to unwieldy data engineering processes while providing state-of-the-art statistical accuracy. Alomar is advised by Devavrat Shah, the Andrew (1956) and Erna Viterbi Professor at MIT.\nDesign of light-activated drugs\nSimon Axelrod, a PhD candidate studying chemical physics at Harvard University, is combining AI with physics simulations to design light-activated drugs that could reduce side effects and improve effectiveness. Patients would receive an inactive form of a drug, which is then activated by light in a specific area of the body containing diseased tissue. This localized use of photoactive drugs would minimize the side effects from drugs targeting healthy cells. Axelrod is developing novel computational models that predict properties of photoactive drugs with high speed and accuracy, allowing researchers to focus on only the highest-quality drug candidates. He is advised by Rafael Gomez-Bombarelli, the Jeffrey Cheah Career Development Chair in Engineering in the MIT Department of Materials Science and Engineering. \nLow-cost 3D perception\nArjun Balasingam, a PhD student in electrical engineering and computer science and a member of the Computer Science and Artificial Intelligence Laboratory’s (CSAIL) Networks and Mobile Systems group, is developing a technology, called MobiSee, that enables real-time 3D reconstruction in challenging dynamic environments. MobiSee uses self-supervised AI methods along with video and lidar to provide low-cost, state-of-the-art 3D perception on consumer mobile devices like smartphones. This technology could have far-reaching applications across mixed reality, navigation, safety, and sports streaming, in addition to unlocking opportunities for new real-time and immersive experiences. He is advised by Hari Balakrishnan, the Fujitsu Professor of Computer Science and Artificial Intelligence at MIT and member of CSAIL.\nSleep therapeutics\nGuillermo Bernal SM ’14, PhD ’23, a recent PhD graduate in media arts and sciences, is developing a sleep therapeutic platform that would enable sleep specialists and researchers to conduct robust sleep studies and develop therapy plans remotely, while the patient is comfortable in their home. Called Fascia, the three-part system consists of a polysomnogram with a sleep mask form factor that collects data, a hub that enables researchers to provide stimulation and feedback via olfactory, auditory, and visual stimuli, and a web portal that enables researchers to read a patient’s signals in real time with machine learning analysis. Bernal was advised by Pattie Maes, professor of media arts and sciences at the MIT Media Lab.\nAutonomous manufacturing assembly with human-like tactile perception\nMichael Foshey, a mechanical engineer and project manager with MIT CSAIL’s Computational Design and Fabrication Group, is developing an AI-enabled tactile perception system that can be used to give robots human-like dexterity. With this new technology platform, Foshey and his team hope to enable industry-changing applications in manufacturing. Currently, assembly tasks in manufacturing are largely done by hand and are typically repetitive and tedious. As a result, these jobs are being largely left unfilled. These labor shortages can cause supply chain shortages and increases in the cost of production. Foshey’s new technology platform aims to address this by automating assembly tasks to reduce reliance on manual labor. Foshey is supervised by Wojciech Matusik, MIT professor of electrical engineering and computer science and member of CSAIL.  \nGenerative AI for video conferencing\nVibhaalakshmi Sivaraman SM ’19, a PhD candidate in electrical engineering and computer science who is a member of CSAIL’s Networking and Mobile Systems Group, is developing a generative technology, Gemino, to facilitate video conferencing in high-latency and low-bandwidth network environments. Gemino is a neural compression system for video conferencing that overcomes the robustness concerns and compute complexity challenges that limit current face-image-synthesis models. This technology could enable sustained video conferencing calls in regions and scenarios that cannot reliably support video calls today. Sivaraman is advised by Mohammad Alizadeh, MIT associate professor of electrical engineering and computer science and member of CSAIL. \n",
    "published": "2023-06-22",
    "timestamp": "2023-08-30T13:26:06.985464"
  },
  {
    "unique_id": "6ba137c7-d373-55b5-b7d7-23baf155024e",
    "title": "New model offers a way to speed up drug discovery",
    "description": "By applying a language model to protein-drug interactions, researchers can quickly screen large libraries of potential drug compounds.",
    "link": "https://news.mit.edu/2023/new-model-offers-speedy-drug-discovery-0608",
    "blog_text": "Huge libraries of drug compounds may hold potential treatments for a variety of diseases, such as cancer or heart disease. Ideally, scientists would like to experimentally test each of these compounds against all possible targets, but doing that kind of screen is prohibitively time-consuming.\n\nIn recent years, researchers have begun using computational methods to screen those libraries in hopes of speeding up drug discovery. However, many of those methods also take a long time, as most of them calculate each target protein’s three-dimensional structure from its amino-acid sequence, then use those structures to predict which drug molecules it will interact with.\n\nResearchers at MIT and Tufts University have now devised an alternative computational approach based on a type of artificial intelligence algorithm known as a large language model. These models — one well-known example is ChatGPT — can analyze huge amounts of text and figure out which words (or, in this case, amino acids) are most likely to appear together. The new model, known as ConPLex, can match target proteins with potential drug molecules without having to perform the computationally intensive step of calculating the molecules’ structures.\n\nUsing this method, the researchers can screen more than 100 million compounds in a single day — much more than any existing model.\n\n“This work addresses the need for efficient and accurate in silico screening of potential drug candidates, and the scalability of the model enables large-scale screens for assessing off-target effects, drug repurposing, and determining the impact of mutations on drug binding,” says Bonnie Berger, the Simons Professor of Mathematics, head of the Computation and Biology group in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), and one of the senior authors of the new study.\nLenore Cowen, a professor of computer science at Tufts University, is also a senior author of the paper, which appears this week in the Proceedings of the National Academy of Sciences. Rohit Singh, a CSAIL research scientist, and Samuel Sledzieski, an MIT graduate student, are the lead authors of the paper, and Bryan Bryson, an associate professor of biological engineering at MIT and a member of the Ragon Institute of MGH, MIT, and Harvard, is also an author. In addition to the paper, the researchers have made their model available online for other scientists to use.\nMaking predictions\n\nIn recent years, computational scientists have made great advances in developing models that can predict the structures of proteins based on their amino-acid sequences. However, using these models to predict how a large library of potential drugs might interact with a cancerous protein, for example, has proven challenging, mainly because calculating the three-dimensional structures of the proteins requires a great deal of time and computing power.\n\nAn additional obstacle is that these kinds of models don’t have a good track record for eliminating compounds known as decoys, which are very similar to a successful drug but don’t actually interact well with the target.\n“One of the longstanding challenges in the field has been that these methods are fragile, in the sense that if I gave the model a drug or a small molecule that looked almost like the true thing, but it was slightly different in some subtle way, the model might still predict that they will interact, even though it should not,” Singh says.\nResearchers have designed models that can overcome this kind of fragility, but they are usually tailored to just one class of drug molecules, and they aren’t well-suited to large-scale screens because the computations take too long. \nThe MIT team decided to take an alternative approach, based on a protein model they first developed in 2019. Working with a database of more than 20,000 proteins, the language model encodes this information into meaningful numerical representations of each amino-acid sequence that capture associations between sequence and structure.\n“With these language models, even proteins that have very different sequences but potentially have similar structures or similar functions can be represented in a similar way in this language space, and we're able to take advantage of that to make our predictions,” Sledzieski says.\nIn their new study, the researchers applied the protein model to the task of figuring out which protein sequences will interact with specific drug molecules, both of which have numerical representations that are transformed into a common, shared space by a neural network. They trained the network on known protein-drug interactions, which allowed it to learn to associate specific features of the proteins with drug-binding ability, without having to calculate the 3D structure of any of the molecules.\n“With this high-quality numerical representation, the model can short-circuit the atomic representation entirely, and from these numbers predict whether or not this drug will bind,” Singh says. “The advantage of this is that you avoid the need to go through an atomic representation, but the numbers still have all of the information that you need.”\nAnother advantage of this approach is that it takes into account the flexibility of protein structures, which can be “wiggly” and take on slightly different shapes when interacting with a drug molecule.\nHigh affinity\nTo make their model less likely to be fooled by decoy drug molecules, the researchers also incorporated a training stage based on the concept of contrastive learning. Under this approach, the researchers give the model examples of “real” drugs and imposters and teach it to distinguish between them.\nThe researchers then tested their model by screening a library of about 4,700 candidate drug molecules for their ability to bind to a set of 51 enzymes known as protein kinases.\nFrom the top hits, the researchers chose 19 drug-protein pairs to test experimentally. The experiments revealed that of the 19 hits, 12 had strong binding affinity (in the nanomolar range), whereas nearly all of the many other possible drug-protein pairs would have no affinity. Four of these pairs bound with extremely high, sub-nanomolar affinity (so strong that a tiny drug concentration, on the order of parts per billion, will inhibit the protein).\nWhile the researchers focused mainly on screening small-molecule drugs in this study, they are now working on applying this approach to other types of drugs, such as therapeutic antibodies. This kind of modeling could also prove useful for running toxicity screens of potential drug compounds, to make sure they don’t have any unwanted side effects before testing them in animal models.\n“Part of the reason why drug discovery is so expensive is because it has high failure rates. If we can reduce those failure rates by saying upfront that this drug is not likely to work out, that could go a long way in lowering the cost of drug discovery,” Singh says.\nThis new approach “represents a significant breakthrough in drug-target interaction prediction and opens up additional opportunities for future research to further enhance its capabilities,” says Eytan Ruppin, chief of the Cancer Data Science Laboratory at the National Cancer Institute, who was not involved in the study. “For example, incorporating structural information into the latent space or exploring molecular generation methods for generating decoys could further improve predictions.”\nThe research was funded by the National Institutes of Health, the National Science Foundation, and the Phillip and Susan Ragon Foundation.\n",
    "published": "2023-06-08",
    "timestamp": "2023-08-30T13:26:07.009130"
  },
  {
    "unique_id": "6c034b53-983f-5318-becf-bc49d907b178",
    "title": "A new dataset of Arctic images will spur artificial intelligence research",
    "description": "The dataset, being collected as part of a US Coast Guard science mission, will be released open source to help advance naval mission planning and climate change studies.",
    "link": "https://news.mit.edu/2023/new-dataset-arctic-images-will-spur-artificial-intelligence-research-0724",
    "blog_text": "As the U.S. Coast Guard (USCG) icebreaker Healy takes part in a voyage across the North Pole this summer, it is capturing images of the Arctic to further the study of this rapidly changing region. Lincoln Laboratory researchers installed a camera system aboard the Healy while at port in Seattle before it embarked on a three-month science mission on July 11. The resulting dataset, which will be one of the first of its kind, will be used to develop artificial intelligence tools that can analyze Arctic imagery.\n\"This dataset not only can help mariners navigate more safely and operate more efficiently, but also help protect our nation by providing critical maritime domain awareness and an improved understanding of how AI analysis can be brought to bear in this challenging and unique environment,\" says Jo Kurucar, a researcher in Lincoln Laboratory's AI Software Architectures and Algorithms Group, which led this project.\nAs the planet warms and sea ice melts, Arctic passages are opening up to more traffic, both to military vessels and ships conducting illegal fishing. These movements may pose national security challenges to the United States. The opening Arctic also leaves questions about how its climate, wildlife, and geography are changing.\nToday, very few imagery datasets of the Arctic exist to study these changes. Overhead images from satellites or aircraft can only provide limited information about the environment. An outward-looking camera attached to a ship can capture more details of the setting and different angles of objects, such as other ships, in the scene. These types of images can then be used to train AI computer-vision tools, which can help the USCG plan naval missions and automate analysis. According to Kurucar, USCG assets in the Arctic are spread thin and can benefit greatly from AI tools, which can act as a force multiplier.\nThe Healy is the USCG's largest and most technologically advanced icebreaker. Given its current mission, it was a fitting candidate to be equipped with a new sensor to gather this dataset. The laboratory research team collaborated with the USCG Research and Development Center to determine the sensor requirements. Together, they developed the Cold Region Imaging and Surveillance Platform (CRISP).\n\"Lincoln Laboratory has an excellent relationship with the Coast Guard, especially with the Research and Development Center. Over a decade, we’ve established ties that enabled the deployment of the CRISP system,\" says Amna Greaves, the CRISP project lead and an assistant leader in the AI Software Architectures and Algorithms Group. \"We have strong ties not only because of the USCG veterans working at the laboratory and in our group, but also because our technology missions are complementary. Today it was deploying infrared sensing in the Arctic; tomorrow it could be operating quadruped robot dogs on a fast-response cutter.\"\nThe CRISP system comprises a long-wave infrared camera, manufactured by Teledyne FLIR (for forward-looking infrared), that is designed for harsh maritime environments. The camera can stabilize itself during rough seas and image in complete darkness, fog, and glare. It is paired with a GPS-enabled time-synchronized clock and a network video recorder to record both video and still imagery along with GPS-positional data.  \nThe camera is mounted at the front of the ship's fly bridge, and the electronics are housed in a ruggedized rack on the bridge. The system can be operated manually from the bridge or be placed into an autonomous surveillance mode, in which it slowly pans back and forth, recording 15 minutes of video every three hours and a still image once every 15 seconds.\n\"The installation of the equipment was a unique and fun experience. As with any good project, our expectations going into the install did not meet reality,\" says Michael Emily, the project's IT systems administrator who traveled to Seattle for the install. Working with the ship's crew, the laboratory team had to quickly adjust their route for running cables from the camera to the observation station after they discovered that the expected access points weren't in fact accessible. \"We had 100-foot cables made for this project just in case of this type of scenario, which was a good thing because we only had a few inches to spare,\" Emily says.\nThe CRISP project team plans to publicly release the dataset, anticipated to be about 4 terabytes in size, once the USCG science mission concludes in the fall.\nThe goal in releasing the dataset is to enable the wider research community to develop better tools for those operating in the Arctic, especially as this region becomes more navigable. \"Collecting and publishing the data allows for faster and greater progress than what we could accomplish on our own,\" Kurucar adds. \"It also enables the laboratory to engage in more advanced AI applications while others make more incremental advances using the dataset.\"\nOn top of providing the dataset, the laboratory team plans to provide a baseline object-detection model, from which others can make progress on their own models. More advanced AI applications planned for development are classifiers for specific objects in the scene and the ability to identify and track objects across images.\nBeyond assisting with USCG missions, this project could create an influential dataset for researchers looking to apply AI to data from the Arctic to help combat climate change, says Paul Metzger, who leads the AI Software Architectures and Algorithms Group.\nMetzger adds that the group was honored to be a part of this project and is excited to see the advances that come from applying AI to novel challenges facing the United States: “I’m extremely proud of how our group applies AI to the highest-priority challenges in our nation, from predicting outbreaks of Covid-19 and assisting the U.S. European Command in their support of Ukraine to now employing AI in the Arctic for maritime awareness.\"\nOnce the dataset is available, it will be free to download on the Lincoln Laboratory dataset website.\n",
    "published": "2023-07-24",
    "timestamp": "2023-08-30T13:26:06.936104"
  },
  {
    "unique_id": "78aa693a-33f8-5329-841d-6862accc0db7",
    "title": "Gamifying medical data labeling to advance AI",
    "description": "MIT alumnus’ platform taps the wisdom of crowds to label medical data for AI companies.",
    "link": "https://news.mit.edu/2023/gamifying-medical-data-labeling-ai-0628",
    "blog_text": "When Erik Duhaime PhD ’19 was working on his thesis in MIT’s Center for Collective Intelligence, he noticed his wife, then a medical student, spending hours studying on apps that offered flash cards and quizzes. His research had shown that, as a group, medical students could classify skin lesions more accurately than professional dermatologists; the trick was to continually measure each student’s performance on cases with known answers, throw out the opinions of people who were bad at the task, and intelligently pool the opinions of people that were good.\nCombining his wife’s studying habits with his research, Duhaime founded Centaur Labs, a company that created a mobile app called DiagnosUs to gather the opinions of medical experts on real-world scientific and biomedical data. Through the app, users review anything from images of potentially cancerous skin lesions or audio clips of heart and lung sounds that could indicate a problem. If the users are accurate, Centaur uses their opinions and awards them small cash prizes. Those opinions, in turn, help medical AI companies train and improve their algorithms.\nThe approach combines the desire of medical experts to hone their skills with the desperate need for well-labeled medical data by companies using AI for biotech, developing pharmaceuticals, or commercializing medical devices.\n“I realized my wife’s studying could be productive work for AI developers,” Duhaime recalls. “Today we have tens of thousands of people using our app, and about half are medical students who are blown away that they win money in the process of studying. So, we have this gamified platform where people are competing with each other to train data and winning money if they’re good and improving their skills at the same time — and by doing that they are labeling data for teams building life saving AI.”\nGamifying medical labeling\nDuhaime completed his PhD under Thomas Malone, the Patrick J. McGovern Professor of Management and founding director of the Center for Collective Intelligence.\n“What interested me was the wisdom of crowds phenomenon,” Duhaime says. “Ask a bunch of people how many jelly beans are in a jar, and the average of everybody’s answer is pretty close. I was interested in how you navigate that problem in a task that requires skill or expertise. Obviously you don’t just want to ask a bunch of random people if you have cancer, but at the same time, we know that second opinions in health care can be extremely valuable. You can think of our platform as a supercharged way of getting a second opinion.”\nDuhaime began exploring ways to leverage collective intelligence to improve medical diagnoses. In one experiment, he trained groups of lay people and medical school students that he describes as “semiexperts” to classify skin conditions, finding that by combining the opinions of the highest performers he could outperform professional dermatologists. He also found that by combining algorithms trained to detect skin cancer with the opinions of experts, he could outperform either method on its own.\n“The core insight was you do two things,” Duhaime explains. “The first thing is to measure people’s performance — which sounds obvious, but even in the medical domain it isn’t done much. If you ask a dermatologist if they’re good, they say, ‘Yeah of course, I’m a dermatologist.’ They don’t necessarily know how good they are at specific tasks. The second thing is that when you get multiple opinions, you need to identify complementarities between the different people. You need to recognize that expertise is multidimensional, so it’s a little more like putting together the optimal trivia team than it is getting the five people who are all the best at the same thing. For example, one dermatologist might be better at identifying melanoma, whereas another might be better at classifying the severity of psoriasis.”\nWhile still pursuing his PhD, Duhaime founded Centaur and began using MIT’s entrepreneurial ecosystem to further develop the idea. He received funding from MIT’s Sandbox Innovation Fund in 2017 and participated in the delta v startup accelerator run by the Martin Trust Center for MIT Entrepreneurship over the summer of 2018. The experience helped him get into the prestigious Y Combinator accelerator later that year.\nThe DiagnosUs app, which Duhaime developed with Centaur co-founders Zach Rausnitz and Tom Gellatly, is designed to help users test and improve their skills. Duhaime says about half of users are medical school students and the other half are mostly doctors, nurses, and other medical professionals.\n“It’s better than studying for exams, where you might have multiple choice questions,” Duhaime says. “They get to see actual cases and practice.”\nCentaur gathers millions of opinions every week from tens of thousands of people around the world. Duhaime says most people earn coffee money, although the person who’s earned the most from the platform is a doctor in eastern Europe who’s made around $10,000.\n“People can do it on the couch, they can do it on the T,” Duhaime says. “It doesn’t feel like work — it’s fun.”\nThe approach stands in sharp contrast to traditional data labeling and AI content moderation, which are typically outsourced to low-resource countries.\nCentaur’s approach produces accurate results, too. In a paper with researchers from Brigham and Women’s Hospital, Massachusetts General Hospital (MGH), and Eindhoven University of Technology, Centaur showed its crowdsourced opinions labeled lung ultrasounds as reliably as experts did. Another study with researchers at Memorial Sloan Kettering showed crowdsourced labeling of dermoscopic images was more accurate than that of highly experienced dermatologists. Beyond images, Centaur’s platform also works with video, audio, text from sources like research papers or anonymized conversations between doctors and patients, and waves from electroencephalograms (EEGs) and electrocardiographys (ECGs).\nFinding the experts\nCentaur has found that the best performers come from surprising places. In 2021, to collect expert opinions on EEG patterns, researchers held a contest through the DiagnosUs app at a conference featuring about 50 epileptologists, each with more than 10 years of experience. The organizers made a custom shirt to give to the contest’s winner, who they assumed would be in attendance at the conference.\nBut when the results came in, a pair of medical students in Ghana, Jeffery Danquah and Andrews Gyabaah, had beaten everyone in attendance. The highest-ranked conference attendee had come in ninth.\n“I started by doing it for the money, but I realized it actually started helping me a lot,” Gyabaah told Centaur’s team later. “There were times in the clinic where I realized that I was doing better than others because of what I learned on the DiagnosUs app.”\nAs AI continues to change the nature of work, Duhaime believes Centaur Labs will be used as an ongoing check on AI models.\n“Right now, we’re helping people train algorithms primarily, but increasingly I think we’ll be used for monitoring algorithms and in conjunction with algorithms, basically serving as the humans in the loop for a range of tasks,” Duhaime says. “You might think of us less as a way to train AI and more as a part of the full life cycle, where we’re providing feedback on models’ outputs or monitoring the model.”\nDuhaime sees the work of humans and AI algorithms becoming increasingly integrated and believes Centaur Labs has an important role to play in that future.\n“It’s not just train algorithm, deploy algorithm,” Duhaime says. “Instead, there will be these digital assembly lines all throughout the economy, and you need on-demand expert human judgment infused in different places along the value chain.”\n",
    "published": "2023-06-28",
    "timestamp": "2023-08-30T13:26:06.979667"
  },
  {
    "unique_id": "7baccb63-d021-5ac3-a71a-e270332c0b66",
    "title": "Researchers use AI to identify similar materials in images",
    "description": "This machine-learning method could assist with robotic scene understanding, image editing, or online recommendation systems.",
    "link": "https://news.mit.edu/2023/researchers-identify-similar-materials-images-0523",
    "blog_text": "A robot manipulating objects while, say, working in a kitchen, will benefit from understanding which items are composed of the same materials. With this knowledge, the robot would know to exert a similar amount of force whether it picks up a small pat of butter from a shadowy corner of the counter or an entire stick from inside the brightly lit fridge.\n\nIdentifying objects in a scene that are composed of the same material, known as material selection, is an especially challenging problem for machines because a material’s appearance can vary drastically based on the shape of the object or lighting conditions.\n\nScientists at MIT and Adobe Research have taken a step toward solving this challenge. They developed a technique that can identify all pixels in an image representing a given material, which is shown in a pixel selected by the user.\n\nThe method is accurate even when objects have varying shapes and sizes, and the machine-learning model they developed isn’t tricked by shadows or lighting conditions that can make the same material appear different.\n\nAlthough they trained their model using only “synthetic” data, which are created by a computer that modifies 3D scenes to produce many varying images, the system works effectively on real indoor and outdoor scenes it has never seen before. The approach can also be used for videos; once the user identifies a pixel in the first frame, the model can identify objects made from the same material throughout the rest of the video.\n\n\n\nIn addition to applications in scene understanding for robotics, this method could be used for image editing or incorporated into computational systems that deduce the parameters of materials in images. It could also be utilized for material-based web recommendation systems. (Perhaps a shopper is searching for clothing made from a particular type of fabric, for example.)\n\n“Knowing what material you are interacting with is often quite important. Although two objects may look similar, they can have different material properties. Our method can facilitate the selection of all the other pixels in an image that are made from the same material,” says Prafull Sharma, an electrical engineering and computer science graduate student and lead author of a paper on this technique.\n\nSharma’s co-authors include Julien Philip and Michael Gharbi, research scientists at Adobe Research; and senior authors William T. Freeman, the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL); Frédo Durand, a professor of electrical engineering and computer science and a member of CSAIL; and Valentin Deschaintre, a research scientist at Adobe Research. The research will be presented at the SIGGRAPH 2023 conference.\n\nA new approach\n\nExisting methods for material selection struggle to accurately identify all pixels representing the same material. For instance, some methods focus on entire objects, but one object can be composed of multiple materials, like a chair with wooden arms and a leather seat. Other methods may utilize a predetermined set of materials, but these often have broad labels like “wood,” despite the fact that there are thousands of varieties of wood.\n\nInstead, Sharma and his collaborators developed a machine-learning approach that dynamically evaluates all pixels in an image to determine the material similarities between a pixel the user selects and all other regions of the image. If an image contains a table and two chairs, and the chair legs and tabletop are made of the same type of wood, their model could accurately identify those similar regions.\n\nBefore the researchers could develop an AI method to learn how to select similar materials, they had to overcome a few hurdles. First, no existing dataset contained materials that were labeled finely enough to train their machine-learning model. The researchers rendered their own synthetic dataset of indoor scenes, which included 50,000 images and more than 16,000 materials randomly applied to each object.\n\n“We wanted a dataset where each individual type of material is marked independently,” Sharma says.\n\nSynthetic dataset in hand, they trained a machine-learning model for the task of identifying similar materials in real images — but it failed. The researchers realized distribution shift was to blame. This occurs when a model is trained on synthetic data, but it fails when tested on real-world data that can be very different from the training set.\n\nTo solve this problem, they built their model on top of a pretrained computer vision model, which has seen millions of real images. They utilized the prior knowledge of that model by leveraging the visual features it had already learned.\n\n“In machine learning, when you are using a neural network, usually it is learning the representation and the process of solving the task together. We have disentangled this. The pretrained model gives us the representation, then our neural network just focuses on solving the task,” he says.\n\nSolving for similarity\n\nThe researchers’ model transforms the generic, pretrained visual features into material-specific features, and it does this in a way that is robust to object shapes or varied lighting conditions.\n\n\n\nThe model can then compute a material similarity score for every pixel in the image. When a user clicks a pixel, the model figures out how close in appearance every other pixel is to the query. It produces a map where each pixel is ranked on a scale from 0 to 1 for similarity.\n\n“The user just clicks one pixel and then the model will automatically select all regions that have the same material,” he says.\n\nSince the model is outputting a similarity score for each pixel, the user can fine-tune the results by setting a threshold, such as 90 percent similarity, and receive a map of the image with those regions highlighted. The method also works for cross-image selection — the user can select a pixel in one image and find the same material in a separate image.\n\nDuring experiments, the researchers found that their model could predict regions of an image that contained the same material more accurately than other methods. When they measured how well the prediction compared to ground truth, meaning the actual areas of the image that are comprised of the same material, their model matched up with about 92 percent accuracy.\n\nIn the future, they want to enhance the model so it can better capture fine details of the objects in an image, which would boost the accuracy of their approach.\n\n“Rich materials contribute to the functionality and beauty of the world we live in. But computer vision algorithms typically overlook materials, focusing heavily on objects instead. This paper makes an important contribution in recognizing materials in images and video across a broad range of challenging conditions,” says Kavita Bala, Dean of the Cornell Bowers College of Computing and Information Science and Professor of Computer Science, who was not involved with this work. “This technology can be very useful to end consumers and designers alike. For example, a home owner can envision how expensive choices like reupholstering a couch, or changing the carpeting in a room, might appear, and can be more confident in their design choices based on these visualizations.”\n\n",
    "published": "2023-05-23",
    "timestamp": "2023-08-30T13:26:07.034026"
  },
  {
    "unique_id": "7cc284e3-faae-56dd-8ceb-24a31a42d363",
    "title": "Generative AI imagines new protein structures ",
    "description": "“FrameDiff” is a computational tool that uses generative AI to craft new protein structures, with the aim of accelerating drug development and improving gene therapy. ",
    "link": "https://news.mit.edu/2023/generative-ai-imagines-new-protein-structures-0712",
    "blog_text": "Biology is a wondrous yet delicate tapestry. At the heart is DNA, the master weaver that encodes proteins, responsible for orchestrating the many biological functions that sustain life within the human body. However, our body is akin to a finely tuned instrument, susceptible to losing its harmony. After all, we’re faced with an ever-changing and relentless natural world: pathogens, viruses, diseases, and cancer. \nImagine if we could expedite the process of creating vaccines or drugs for newly emerged pathogens. What if we had gene editing technology capable of automatically producing proteins to rectify DNA errors that cause cancer? The quest to identify proteins that can strongly bind to targets or speed up chemical reactions is vital for drug development, diagnostics, and numerous industrial applications, yet it is often a protracted and costly endeavor.\nTo advance our capabilities in protein engineering, MIT CSAIL researchers came up with “FrameDiff,” a computational tool for creating new protein structures beyond what nature has produced. The machine learning approach generates “frames” that align with the inherent properties of protein structures, enabling it to construct novel proteins independently of preexisting designs, facilitating unprecedented protein structures.\n\"In nature, protein design is a slow-burning process that takes millions of years. Our technique aims to provide an answer to tackling human-made problems that evolve much faster than nature's pace,” says MIT CSAIL PhD student Jason Yim, a lead author on a new paper about the work. “The aim, with respect to this new capacity of generating synthetic protein structures, opens up a myriad of enhanced capabilities, such as better binders. This means engineering proteins that can attach to other molecules more efficiently and selectively, with widespread implications related to targeted drug delivery and biotechnology, where it could result in the development of better biosensors. It could also have implications for the field of biomedicine and beyond, offering possibilities such as developing more efficient photosynthesis proteins, creating more effective antibodies, and engineering nanoparticles for gene therapy.” \nFraming FrameDiff\nProteins have complex structures, made up of many atoms connected by chemical bonds. The most important atoms that determine the protein’s 3D shape are called the “backbone,” kind of like the spine of the protein. Every triplet of atoms along the backbone shares the same pattern of bonds and atom types. Researchers noticed this pattern can be exploited to build machine learning algorithms using ideas from differential geometry and probability. This is where the frames come in: Mathematically, these triplets can be modeled as rigid bodies called “frames” (common in physics) that have a position and rotation in 3D. \nThese frames equip each triplet with enough information to know about its spatial surroundings. The task is then for a machine learning algorithm to learn how to move each frame to construct a protein backbone. By learning to construct existing proteins, the algorithm hopefully will generalize and be able to create new proteins never seen before in nature.\nTraining a model to construct proteins via “diffusion” involves injecting noise that randomly moves all the frames and blurs what the original protein looked like. The algorithm’s job is to move and rotate each frame until it looks like the original protein. Though simple, the development of diffusion on frames requires techniques in stochastic calculus on Riemannian manifolds. On the theory side, the researchers developed “SE(3) diffusion” for learning probability distributions that nontrivially connects the translations and rotations components of each frame.\nThe subtle art of diffusion\nIn 2021, DeepMind introduced AlphaFold2, a deep learning algorithm for predicting 3D protein structures from their sequences. When creating synthetic proteins, there are two essential steps: generation and prediction. Generation means the creation of new protein structures and sequences, while \"prediction\" means figuring out what the 3D structure of a sequence is. It’s no coincidence that AlphaFold2 also used frames to model proteins. SE(3) diffusion and FrameDiff were inspired to take the idea of frames further by incorporating frames into diffusion models, a generative AI technique that has become immensely popular in image generation, like Midjourney, for example. \nThe shared frames and principles between protein structure generation and prediction meant the best models from both ends were compatible. In collaboration with the Institute for Protein Design at the University of Washington, SE(3) diffusion is already being used to create and experimentally validate novel proteins. Specifically, they combined SE(3) diffusion with RosettaFold2, a protein structure prediction tool much like AlphaFold2, which led to “RFdiffusion.” This new tool brought protein designers closer to solving crucial problems in biotechnology, including the development of highly specific protein binders for accelerated vaccine design, engineering of symmetric proteins for gene delivery, and robust motif scaffolding for precise enzyme design. \nFuture endeavors for FrameDiff involve improving generality to problems that combine multiple requirements for biologics such as drugs. Another extension is to generalize the models to all biological modalities including DNA and small molecules. The team posits that by expanding FrameDiff's training on more substantial data and enhancing its optimization process, it could generate foundational structures boasting design capabilities on par with RFdiffusion, all while preserving the inherent simplicity of FrameDiff. \n“Discarding a pretrained structure prediction model [in FrameDiff] opens up possibilities for rapidly generating structures extending to large lengths,” says Harvard University computational biologist Sergey Ovchinnikov. The researchers' innovative approach offers a promising step toward overcoming the limitations of current structure prediction models. Even though it's still preliminary work, it's an encouraging stride in the right direction. As such, the vision of protein design, playing a pivotal role in addressing humanity's most pressing challenges, seems increasingly within reach, thanks to the pioneering work of this MIT research team.” \nYim wrote the paper alongside Columbia University postdoc Brian Trippe, French National Center for Scientific Research in Paris' Center for Science of Data researcher Valentin De Bortoli, Cambridge University postdoc Emile Mathieu, and Oxford University professor of statistics and senior research scientist at DeepMind Arnaud Doucet. MIT professors Regina Barzilay and Tommi Jaakkola advised the research. \nThe team's work was supported, in part, by the MIT Abdul Latif Jameel Clinic for Machine Learning in Health, EPSRC grants and a Prosperity Partnership between Microsoft Research and Cambridge University, the National Science Foundation Graduate Research Fellowship Program, NSF Expeditions grant, Machine Learning for Pharmaceutical Discovery and Synthesis consortium, the DTRA Discovery of Medical Countermeasures Against New and Emerging threats program, the DARPA Accelerated Molecular Discovery program, and the Sanofi Computational Antibody Design grant. This research will be presented at the International Conference on Machine Learning in July.\n",
    "published": "2023-07-12",
    "timestamp": "2023-08-30T13:26:06.957939"
  },
  {
    "unique_id": "7cfb3431-b155-5476-a272-2a701f69102a",
    "title": "Understanding viral justice",
    "description": "Author and African American studies scholar Ruha Benjamin urges MIT Libraries staff to “re-imagine the default settings” of technology for a more just future.",
    "link": "https://news.mit.edu/2023/understanding-viral-justice-ruha-benjamin-0717",
    "blog_text": "In the wake of the Covid-19 pandemic, the word “viral” has a new resonance, and it’s not necessarily positive. Ruha Benjamin, a scholar who investigates the social dimensions of science, medicine, and technology, advocates a shift in perspective. She thinks justice can also be contagious. That’s the premise of Benjamin’s award-winning book “Viral Justice: How We Grow the World We Want,” as she shared with MIT Libraries staff on a June 14 visit. \n“If this pandemic has taught us anything, it's that something almost undetectable can be deadly, and that we can transmit it without even knowing,” said Benjamin, professor of African American studies at Princeton University. “Doesn't this imply that small things, seemingly minor actions, decisions, or habits, could have exponential effects in the other direction, tipping the scales towards justice?” \nTo seek a more just world, Benjamin exhorted library staff to notice the ways exclusion is built into our daily lives, showing examples of park benches with armrests at regular intervals. On the surface they appear welcoming, but they also make lying down — or sleeping — impossible. This idea is taken to the extreme with “Pay and Sit,” an art installation by Fabian Brunsing in the form of a bench that deploys sharp spikes on the seat if the user doesn’t pay a meter. It serves as a powerful metaphor for discriminatory design. \n“Dr. Benjamin’s keynote was seriously mind-blowing,” said Cherry Ibrahim, human resources generalist in the MIT Libraries. “One part that really grabbed my attention was when she talked about benches purposely designed to prevent unhoused people from sleeping on them. There are these hidden spikes in our community that we might not even realize because they don't directly impact us.” \nBenjamin urged the audience to look for those “spikes,” which new technologies can make even more insidious — gender and racial bias in facial recognition, the use of racial data in software used to predict student success, algorithmic bias in health care — often in the guise of progress. She coined the term “the New Jim Code” to describe the combination of coded bias and the imagined objectivity we ascribe to technology. \n“At the MIT Libraries, we’re deeply concerned with combating inequities through our work, whether it’s democratizing access to data or investigating ways disparate communities can participate in scholarship with minimal bias or barriers,” says Director of Libraries Chris Bourg. “It’s our mission to remove the ‘spikes’ in the systems through which we create, use, and share knowledge.”\nCalling out the harms encoded into our digital world is critical, argues Benjamin, but we must also create alternatives. This is where the collective power of individuals can be transformative. Benjamin shared examples of those who are “re-imagining the default settings of technology and society,” citing initiatives like Data for Black Lives movement and the Detroit Community Technology Project. “I'm interested in the way that everyday people are changing the digital ecosystem and demanding different kinds of rights and responsibilities and protections,” she said.\nIn 2020, Benjamin founded the Ida B. Wells Just Data Lab with a goal of bringing together students, educators, activists, and artists to develop a critical and creative approach to data conception, production, and circulation. Its projects have examined different aspects of data and racial inequality: assessing the impact of Covid-19 on student learning; providing resources that confront the experience of Black mourning, grief, and mental health; or developing a playbook for Black maternal mental health. Through the lab’s student-led projects Benjamin sees the next generation re-imagining technology in ways that respond to the needs of marginalized people.\n“If inequity is woven into the very fabric of our society — we see it from policing to education to health care to work — then each twist, coil, and code is a chance for us to weave new patterns, practices, and politics,” she said. “The vastness of the problems that we're up against will be their undoing.”\n",
    "published": "2023-07-17",
    "timestamp": "2023-08-30T13:26:06.941225"
  },
  {
    "unique_id": "826ea793-aa03-5c6f-97eb-2a42c856119c",
    "title": "A step toward safe and reliable autopilots for flying",
    "description": "A new AI-based approach for controlling autonomous robots satisfies the often-conflicting goals of safety and stability.",
    "link": "https://news.mit.edu/2023/safe-and-reliable-autopilots-flying-0612",
    "blog_text": "In the film “Top Gun: Maverick,” Maverick, played by Tom Cruise, is charged with training young pilots to complete a seemingly impossible mission — to fly their jets deep into a rocky canyon, staying so low to the ground they cannot be detected by radar, then rapidly climb out of the canyon at an extreme angle, avoiding the rock walls. Spoiler alert: With Maverick’s help, these human pilots accomplish their mission.\nA machine, on the other hand, would struggle to complete the same pulse-pounding task. To an autonomous aircraft, for instance, the most straightforward path toward the target is in conflict with what the machine needs to do to avoid colliding with the canyon walls or staying undetected. Many existing AI methods aren’t able to overcome this conflict, known as the stabilize-avoid problem, and would be unable to reach their goal safely.\nMIT researchers have developed a new technique that can solve complex stabilize-avoid problems better than other methods. Their machine-learning approach matches or exceeds the safety of existing methods while providing a tenfold increase in stability, meaning the agent reaches and remains stable within its goal region.\nIn an experiment that would make Maverick proud, their technique effectively piloted a simulated jet aircraft through a narrow corridor without crashing into the ground. \n“This has been a longstanding, challenging problem. A lot of people have looked at it but didn’t know how to handle such high-dimensional and complex dynamics,” says Chuchu Fan, the Wilson Assistant Professor of Aeronautics and Astronautics, a member of the Laboratory for Information and Decision Systems (LIDS), and senior author of a new paper on this technique.\nFan is joined by lead author Oswin So, a graduate student. The paper will be presented at the Robotics: Science and Systems conference.\nThe stabilize-avoid challenge\nMany approaches tackle complex stabilize-avoid problems by simplifying the system so they can solve it with straightforward math, but the simplified results often don’t hold up to real-world dynamics.\nMore effective techniques use reinforcement learning, a machine-learning method where an agent learns by trial-and-error with a reward for behavior that gets it closer to a goal. But there are really two goals here — remain stable and avoid obstacles — and finding the right balance is tedious.\nThe MIT researchers broke the problem down into two steps. First, they reframe the stabilize-avoid problem as a constrained optimization problem. In this setup, solving the optimization enables the agent to reach and stabilize to its goal, meaning it stays within a certain region. By applying constraints, they ensure the agent avoids obstacles, So explains. \nThen for the second step, they reformulate that constrained optimization problem into a mathematical representation known as the epigraph form and solve it using a deep reinforcement learning algorithm. The epigraph form lets them bypass the difficulties other methods face when using reinforcement learning. \n“But deep reinforcement learning isn’t designed to solve the epigraph form of an optimization problem, so we couldn’t just plug it into our problem. We had to derive the mathematical expressions that work for our system. Once we had those new derivations, we combined them with some existing engineering tricks used by other methods,” So says.\nNo points for second place\nTo test their approach, they designed a number of control experiments with different initial conditions. For instance, in some simulations, the autonomous agent needs to reach and stay inside a goal region while making drastic maneuvers to avoid obstacles that are on a collision course with it.\n\nWhen compared with several baselines, their approach was the only one that could stabilize all trajectories while maintaining safety. To push their method even further, they used it to fly a simulated jet aircraft in a scenario one might see in a “Top Gun” movie. The jet had to stabilize to a target near the ground while maintaining a very low altitude and staying within a narrow flight corridor.\nThis simulated jet model was open-sourced in 2018 and had been designed by flight control experts as a testing challenge. Could researchers create a scenario that their controller could not fly? But the model was so complicated it was difficult to work with, and it still couldn’t handle complex scenarios, Fan says.\nThe MIT researchers’ controller was able to prevent the jet from crashing or stalling while stabilizing to the goal far better than any of the baselines.\nIn the future, this technique could be a starting point for designing controllers for highly dynamic robots that must meet safety and stability requirements, like autonomous delivery drones. Or it could be implemented as part of larger system. Perhaps the algorithm is only activated when a car skids on a snowy road to help the driver safely navigate back to a stable trajectory.\nNavigating extreme scenarios that a human wouldn’t be able to handle is where their approach really shines, So adds.\n“We believe that a goal we should strive for as a field is to give reinforcement learning the safety and stability guarantees that we will need to provide us with assurance when we deploy these controllers on mission-critical systems. We think this is a promising first step toward achieving that goal,” he says.\nMoving forward, the researchers want to enhance their technique so it is better able to take uncertainty into account when solving the optimization. They also want to investigate how well the algorithm works when deployed on hardware, since there will be mismatches between the dynamics of the model and those in the real world.\n“Professor Fan’s team has improved reinforcement learning performance for dynamical systems where safety matters. Instead of just hitting a goal, they create controllers that ensure the system can reach its target safely and stay there indefinitely,” says Stanley Bak, an assistant professor in the Department of Computer Science at Stony Brook University, who was not involved with this research. “Their improved formulation allows the successful generation of safe controllers for complex scenarios, including a 17-state nonlinear jet aircraft model designed in part by researchers from the Air Force Research Lab (AFRL), which incorporates nonlinear differential equations with lift and drag tables.”\nThe work is funded, in part, by MIT Lincoln Laboratory under the Safety in Aerobatic Flight Regimes program.\n",
    "published": "2023-06-12",
    "timestamp": "2023-08-30T13:26:07.001161"
  },
  {
    "unique_id": "867552d3-cc65-5eaa-ae3a-7b4511923017",
    "title": "Armando Solar-Lezama named inaugural Distinguished Professor of Computing",
    "description": "EECS professor appointed to new professorship in the MIT Schwarzman College of Computing.",
    "link": "https://news.mit.edu/2023/armando-solar-lezama-named-inaugural-distinguished-college-computing-professor-0717",
    "blog_text": "The MIT Stephen A. Schwarzman College of Computing named Armando Solar-Lezama as the inaugural Distinguished Professor of Computing, effective July 1. \nSolar-Lezama is the first person appointed to this position generously endowed by Professor Jae S. Lim of the Department of Electrical Engineering and Computer Science (EECS). Established in the MIT Schwarzman College of Computing, the chair is being awarded to Solar-Lezama for being an outstanding faculty member who is recognized as a leader and innovator.\n“I’m pleased to make this appointment and recognize Armando for his remarkable contributions to MIT and the scientific community,” says Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing and the Henry Ellis Warren Professor of Electrical Engineering and Computer Science. “I’m greatly appreciative of Professor Lim for his thoughtful gesture in creating this new chair in the college, providing us with the opportunity to acknowledge the accomplishments of our faculty.”\nSolar-Lezama, a professor of electrical engineering and computer science, leads the Computer-Aided Programming Group in the Computer Science and Artificial Intelligence Laboratory (CSAIL) that focuses on program synthesis, an area of research that lies at the intersection of programming systems and artificial intelligence. The group’s research ranges from designing new analysis techniques and automated reasoning mechanisms to developing new programming models that automate challenging aspects of programming.\nA member of the EECS faculty since 2008, Solar-Lezama, who also serves as the associate director and chief operating officer for CSAIL, is most interested in software synthesis and its applications to particular program domains such as high-performance computing. He first found this niche area of program synthesis as a graduate student at the University of California at Berkeley, for which his thesis project, a language called Sketch, treats program synthesis as a search problem in which the algorithms pare down the search space to make the search faster and more efficient. Since then, program synthesis research has greatly expanded into the active field it is today.\n",
    "published": "2023-07-17",
    "timestamp": "2023-08-30T13:26:06.942823"
  },
  {
    "unique_id": "89657f51-3d92-5736-98cc-4fa403ff9cb2",
    "title": "3 Questions: Honing robot perception and mapping",
    "description": "Luca Carlone and Jonathan How of MIT LIDS discuss how future robots might perceive and interact with their environment.",
    "link": "https://news.mit.edu/2023/honing-robot-perception-mapping-0710",
    "blog_text": "Walking to a friend’s house or browsing the aisles of a grocery store might feel like simple tasks, but they in fact require sophisticated capabilities. That's because humans are able to effortlessly understand their surroundings and detect complex information about patterns, objects, and their own location in the environment.\nWhat if robots could perceive their environment in a similar way? That question is on the minds of MIT Laboratory for Information and Decision Systems (LIDS) researchers Luca Carlone and Jonathan How. In 2020, a team led by Carlone released the first iteration of Kimera, an open-source library that enables a single robot to construct a three-dimensional map of its environment in real time, while labeling different objects in view. Last year, Carlone’s and How’s research groups (SPARK Lab and Aerospace Controls Lab) introduced Kimera-Multi, an updated system in which multiple robots communicate among themselves in order to create a unified map. A 2022 paper associated with the project recently received this year’s IEEE Transactions on Robotics King-Sun Fu Memorial Best Paper Award, given to the best paper published in the journal in 2022.\nCarlone, who is the Leonardo Career Development Associate Professor of Aeronautics and Astronautics, and How, the Richard Cockburn Maclaurin Professor in Aeronautics and Astronautics, spoke to LIDS about Kimera-Multi and the future of how robots might perceive and interact with their environment.\nQ: Currently your labs are focused on increasing the number of robots that can work together in order to generate 3D maps of the environment. What are some potential advantages to scaling this system?\nHow: The key benefit hinges on consistency, in the sense that a robot can create an independent map, and that map is self-consistent but not globally consistent. We’re aiming for the team to have a consistent map of the world; that’s the key difference in trying to form a consensus between robots as opposed to mapping independently.\nCarlone: In many scenarios it’s also good to have a bit of redundancy. For example, if we deploy a single robot in a search-and-rescue mission, and something happens to that robot, it would fail to find the survivors. If multiple robots are doing the exploring, there’s a much better chance of success. Scaling up the team of robots also means that any given task may be completed in a shorter amount of time.\nQ: What are some of the lessons you’ve learned from recent experiments, and challenges you’ve had to overcome while designing these systems?\nCarlone: Recently we did a big mapping experiment on the MIT campus, in which eight robots traversed up to 8 kilometers in total. The robots have no prior knowledge of the campus, and no GPS. Their main tasks are to estimate their own trajectory and build a map around it. You want the robots to understand the environment as humans do; humans not only understand the shape of obstacles, to get around them without hitting them, but also understand that an object is a chair, a desk, and so on. There’s the semantics part.\nThe interesting thing is that when the robots meet each other, they exchange information to improve their map of the environment. For instance, if robots connect, they can leverage information to correct their own trajectory. The challenge is that if you want to reach a consensus between robots, you don’t have the bandwidth to exchange too much data. One of the key contributions of our 2022 paper is to deploy a distributed protocol, in which robots exchange limited information but can still agree on how the map looks. They don’t send camera images back and forth but only exchange specific 3D coordinates and clues extracted from the sensor data. As they continue to exchange such data, they can form a consensus.\nRight now we are building color-coded 3D meshes or maps, in which the color contains some semantic information, like “green” corresponds to grass, and “magenta” to a building. But as humans, we have a much more sophisticated understanding of reality, and we have a lot of prior knowledge about relationships between objects. For instance, if I was looking for a bed, I would go to the bedroom instead of exploring the entire house. If you start to understand the complex relationships between things, you can be much smarter about what the robot can do in the environment. We’re trying to move from capturing just one layer of semantics, to a more hierarchical representation in which the robots understand rooms, buildings, and other concepts.\nQ: What kinds of applications might Kimera and similar technologies lead to in the future?\nHow: Autonomous vehicle companies are doing a lot of mapping of the world and learning from the environments they’re in. The holy grail would be if these vehicles could communicate with each other and share information, then they could improve models and maps that much quicker. The current solutions out there are individualized. If a truck pulls up next to you, you can’t see in a certain direction. Could another vehicle provide a field of view that your vehicle otherwise doesn’t have? This is a futuristic idea because it requires vehicles to communicate in new ways, and there are privacy issues to overcome. But if we could resolve those issues, you could imagine a significantly improved safety situation, where you have access to data from multiple perspectives, not only your field of view.\nCarlone: These technologies will have a lot of applications. Earlier I mentioned search and rescue. Imagine that you want to explore a forest and look for survivors, or map buildings after an earthquake in a way that can help first responders access people who are trapped. Another setting where these technologies could be applied is in factories. Currently, robots that are deployed in factories are very rigid. They follow patterns on the floor, and are not really able to understand their surroundings. But if you’re thinking about much more flexible factories in the future, robots will have to cooperate with humans and exist in a much less structured environment.\n",
    "published": "2023-07-10",
    "timestamp": "2023-08-30T13:26:06.961300"
  },
  {
    "unique_id": "8e1f910a-a0a4-5798-93c3-5f3abf339c2a",
    "title": "A faster way to teach a robot",
    "description": "A new technique helps a nontechnical user understand why a robot failed, and then fine-tune it with minimal effort to perform a task effectively.",
    "link": "https://news.mit.edu/2023/faster-way-teach-robot-technique-0718",
    "blog_text": "Imagine purchasing a robot to perform household tasks. This robot was built and trained in a factory on a certain set of tasks and has never seen the items in your home. When you ask it to pick up a mug from your kitchen table, it might not recognize your mug (perhaps because this mug is painted with an unusual image, say, of MIT’s mascot, Tim the Beaver). So, the robot fails.\n\n“Right now, the way we train these robots, when they fail, we don’t really know why. So you would just throw up your hands and say, ‘OK, I guess we have to start over.’ A critical component that is missing from this system is enabling the robot to demonstrate why it is failing so the user can give it feedback,” says Andi Peng, an electrical engineering and computer science (EECS) graduate student at MIT.\n\nPeng and her collaborators at MIT, New York University, and the University of California at Berkeley created a framework that enables humans to quickly teach a robot what they want it to do, with a minimal amount of effort.\n\nWhen a robot fails, the system uses an algorithm to generate counterfactual explanations that describe what needed to change for the robot to succeed. For instance, maybe the robot would have been able to pick up the mug if the mug were a certain color. It shows these counterfactuals to the human and asks for feedback on why the robot failed. Then the system utilizes this feedback and the counterfactual explanations to generate new data it uses to fine-tune the robot.\nFine-tuning involves tweaking a machine-learning model that has already been trained to perform one task, so it can perform a second, similar task.\n\nThe researchers tested this technique in simulations and found that it could teach a robot more efficiently than other methods. The robots trained with this framework performed better, while the training process consumed less of a human’s time.\n\nThis framework could help robots learn faster in new environments without requiring a user to have technical knowledge. In the long run, this could be a step toward enabling general-purpose robots to efficiently perform daily tasks for the elderly or individuals with disabilities in a variety of settings.\n\nPeng, the lead author, is joined by co-authors Aviv Netanyahu, an EECS graduate student; Mark Ho, an assistant professor at the Stevens Institute of Technology; Tianmin Shu, an MIT postdoc; Andreea Bobu, a graduate student at UC Berkeley; and senior authors Julie Shah, an MIT professor of aeronautics and astronautics and the director of the Interactive Robotics Group in the Computer Science and Artificial Intelligence Laboratory (CSAIL), and Pulkit Agrawal, an EECS professor and CSAIL affiliate. The research will be presented at the International Conference on Machine Learning.\n\nOn-the-job training\n\nRobots often fail due to distribution shift — the robot is presented with objects and spaces it did not see during training, and it doesn’t understand what to do in this new environment.\n\nOne way to retrain a robot for a specific task is imitation learning. The user could demonstrate the correct task to teach the robot what to do. If a user tries to teach a robot to pick up a mug, but demonstrates with a white mug, the robot could learn that all mugs are white. It may then fail to pick up a red, blue, or “Tim-the-Beaver-brown” mug.\n\nTraining a robot to recognize that a mug is a mug, regardless of its color, could take thousands of demonstrations.\n\n“I don’t want to have to demonstrate with 30,000 mugs. I want to demonstrate with just one mug. But then I need to teach the robot so it recognizes that it can pick up a mug of any color,” Peng says.\n\nTo accomplish this, the researchers’ system determines what specific object the user cares about (a mug) and what elements aren’t important for the task (perhaps the color of the mug doesn’t matter). It uses this information to generate new, synthetic data by changing these “unimportant” visual concepts. This process is known as data augmentation.\n\nThe framework has three steps. First, it shows the task that caused the robot to fail. Then it collects a demonstration from the user of the desired actions and generates counterfactuals by searching over all features in the space that show what needed to change for the robot to succeed.\n\nThe system shows these counterfactuals to the user and asks for feedback to determine which visual concepts do not impact the desired action. Then it uses this human feedback to generate many new augmented demonstrations.\n\nIn this way, the user could demonstrate picking up one mug, but the system would produce demonstrations showing the desired action with thousands of different mugs by altering the color. It uses these data to fine-tune the robot.\n\nCreating counterfactual explanations and soliciting feedback from the user are critical for the technique to succeed, Peng says.\n\nFrom human reasoning to robot reasoning\n\nBecause their work seeks to put the human in the training loop, the researchers tested their technique with human users. They first conducted a study in which they asked people if counterfactual explanations helped them identify elements that could be changed without affecting the task.\n\n“It was so clear right off the bat. Humans are so good at this type of counterfactual reasoning. And this counterfactual step is what allows human reasoning to be translated into robot reasoning in a way that makes sense,” she says.\n\nThen they applied their framework to three simulations where robots were tasked with: navigating to a goal object, picking up a key and unlocking a door, and picking up a desired object then placing it on a tabletop. In each instance, their method enabled the robot to learn faster than with other techniques, while requiring fewer demonstrations from users.\n\nMoving forward, the researchers hope to test this framework on real robots. They also want to focus on reducing the time it takes the system to create new data using generative machine-learning models.\n\n“We want robots to do what humans do, and we want them to do it in a semantically meaningful way. Humans tend to operate in this abstract space, where they don’t think about every single property in an image. At the end of the day, this is really about enabling a robot to learn a good, human-like representation at an abstract level,” Peng says.\n\nThis research is supported, in part, by a National Science Foundation Graduate Research Fellowship, Open Philanthropy, an Apple AI/ML Fellowship, Hyundai Motor Corporation, the MIT-IBM Watson AI Lab, and the National Science Foundation Institute for Artificial Intelligence and Fundamental Interactions.\n\n",
    "published": "2023-07-18",
    "timestamp": "2023-08-30T13:26:06.939422"
  },
  {
    "unique_id": "93fd40ea-bd3d-5c52-b89b-76941716ec3d",
    "title": "How an “AI-tocracy” emerges",
    "description": "In China, the use of AI-driven facial recognition helps the regime repress dissent while enhancing the technology, researchers report.",
    "link": "https://news.mit.edu/2023/how-ai-tocracy-emerges-0713",
    "blog_text": "Many scholars, analysts, and other observers have suggested that resistance to innovation is an Achilles’ heel of authoritarian regimes. Such governments can fail to keep up with technological changes that help their opponents; they may also, by stifling rights, inhibit innovative economic activity and weaken the long-term condition of the country.\n\nBut a new study co-led by an MIT professor suggests something quite different. In China, the research finds, the government has increasingly deployed AI-driven facial-recognition technology to surpress dissent; has been successful at limiting protest; and in the process, has spurred the development of better AI-based facial-recognition tools and other forms of software.\n\n“What we found is that in regions of China where there is more unrest, that leads to greater government procurement of facial-recognition AI, subsequently, by local government units such as municipal police departments,” says MIT economist Martin Beraja, who is co-author of a new paper detailing the findings.\n\nWhat follows, as the paper notes, is that “AI innovation entrenches the regime, and the regime’s investment in AI for political control stimulates further frontier innovation.”\n\nThe scholars call this state of affairs an “AI-tocracy,” describing the connected cycle in which increased deployment of the AI-driven technology quells dissent while also boosting the country’s innovation capacity.\n\nThe open-access paper, also called “AI-tocracy,” appears in the August issue of the Quarterly Journal of Economics. The co-authors are Beraja, who is the Pentti Kouri Career Development Associate Professor of Economics at MIT; Andrew Kao, a doctoral candidate in economics at Harvard University; David Yang, a professor of economics at Harvard; and Noam Yuchtman, a professor of management at the London School of Economics.\n\nTo conduct the study, the scholars drew on multiple kinds of evidence spanning much of the last decade. To catalogue instances of political unrest in China, they used data from the Global Database of Events, Language, and Tone (GDELT) Project, which records news feeds globally. The team turned up 9,267 incidents of unrest between 2014 and 2020.\n\nThe researchers then examined records of almost 3 million procurement contracts issued by the Chinese government between 2013 and 2019, from a database maintained by China’s Ministry of Finance. They found that local governments’ procurement of facial-recognition AI services and complementary public security tools — high-resolution video cameras — jumped significantly in the quarter following an episode of public unrest in that area.\n\nGiven that Chinese government officials were clearly responding to public dissent activities by ramping up on facial-recognition technology, the researchers then examined a follow-up question: Did this approach work to suppress dissent?\n\nThe scholars believe that it did, although as they note in the paper, they “cannot directly estimate the effect” of the technology on political unrest. But as one way of getting at that question, they studied the relationship between weather and political unrest in different areas of China. Certain weather conditions are conducive to political unrest. But in prefectures in China that had already invested heavily in facial-recognition technology, such weather conditions are less conducive to unrest compared to prefectures that had not made the same investments.\n\nIn so doing, the researchers also accounted for issues such as whether or not greater relative wealth levels in some areas might have produced larger investments in AI-driven technologies regardless of protest patterns. However, the scholars still reached the same conclusion: Facial-recognition technology was being deployed in response to past protests, and then reducing further protest levels.\n\n“It suggests that the technology is effective in chilling unrest,” Beraja says.\n\nFinally, the research team studied the effects of increased AI demand on China’s technology sector and found the government’s greater use of facial-recognition tools appears to be driving the country’s tech sector forward. For instance, firms that are granted procurement contracts for facial-recognition technologies subsequently produce about 49 percent more software products in the two years after gaining the government contract than they had beforehand.\n\n“We examine if this leads to greater innovation by facial-recognition AI firms, and indeed it does,” Beraja says.\n\nSuch data — from China’s Ministry of Industry and Information Technology — also indicates that AI-driven tools are not necessarily “crowding out” other kinds of high-tech innovation.\n\nAdding it all up, the case of China indicates how autocratic governments can potentially reach a near-equilibrium state in which their political power is enhanced, rather than upended, when they harness technological advances.\n\n“In this age of AI, when the technologies not only generate growth but are also technologies of repression, they can be very useful” to authoritarian regimes, Beraja says.\n\nThe finding also bears on larger questions about forms of government and economic growth. A significant body of scholarly research shows that rights-granting democratic institutions do generate greater economic growth over time, in part by creating better conditions for technological innovation. Beraja notes that the current study does not contradict those earlier findings, but in examining the effects of AI in use, it does identify one avenue through which authoritarian governments can generate more growth than they otherwise would have.\n\n“This may lead to cases where more autocratic institutions develop side by side with growth,” Beraja adds.\n\nOther experts in the societal applications of AI say the paper makes a valuable contribution to the field.\n\n“This is an excellent and important paper that improves our understanding of the interaction between technology, economic success, and political power,” says Avi Goldfarb, the Rotman Chair in Artificial Intelligence and Healthcare and a professor of marketing at the Rotman School of Management at the University of Toronto. “The paper documents a positive feedback loop between the use of AI facial-recognition technology to monitor suppress local unrest in China and the development and training of AI models. This paper is pioneering research in AI and political economy. As AI diffuses, I expect this research area to grow in importance.”\n\nFor their part, the scholars are continuing to work on related aspects of this issue. One forthcoming paper of theirs examines the extent to which China is exporting advanced facial-recognition technologies around the world — highlighting a mechanism through which government repression could grow globally.\n\nSupport for the research was provided in part by the U.S. National Science Foundation Graduate Research Fellowship Program; the Harvard Data Science Initiative; and the British Academy’s Global Professorships program.\n",
    "published": "2023-07-13",
    "timestamp": "2023-08-30T13:26:06.955833"
  },
  {
    "unique_id": "98f0a8ee-00b5-5d7e-be05-d1581c2b5860",
    "title": "How machine-learning models can amplify inequities in medical diagnosis and treatment",
    "description": "MIT researchers investigate the causes of health care disparities among underrepresented groups.<br />",
    "link": "https://news.mit.edu/2023/how-machine-learning-models-can-amplify-inequities-medical-diagnosis-treatment-0817",
    "blog_text": "Prior to receiving a PhD in computer science from MIT in 2017, Marzyeh Ghassemi had already begun to wonder whether the use of AI techniques might enhance the biases that already existed in health care. She was one of the early researchers to take up this issue, and she’s been exploring it ever since. In a new paper, Ghassemi, now an assistant professor in MIT’s Department of Electrical Science and Engineering (EECS), and three collaborators based at the Computer Science and Artificial Intelligence Laboratory, have probed the roots of the disparities that can arise in machine learning, often causing models that perform well overall to falter when it comes to subgroups for which relatively few data have been collected and utilized in the training process. The paper — written by two MIT PhD students, Yuzhe Yang and Haoran Zhang, EECS computer scientist Dina Katabi (the Thuan and Nicole Pham Professor), and Ghassemi — was presented last month at the 40th International Conference on Machine Learning in Honolulu, Hawaii.\nIn their analysis, the researchers focused on \"subpopulation shifts\" — differences in the way machine learning models perform for one subgroup as compared to another. “We want the models to be fair and work equally well for all groups, but instead we consistently observe the presence of shifts among different groups that can lead to inferior medical diagnosis and treatment,” says Yang, who along with Zhang are the two lead authors on the paper. The main point of their inquiry is to determine the kinds of subpopulation shifts that can occur and to uncover the mechanisms behind them so that, ultimately, more equitable models can be developed.\nThe new paper “significantly advances our understanding” of the subpopulation shift phenomenon, claims Stanford University computer scientist Sanmi Koyejo. “This research contributes valuable insights for future advancements in machine learning models' performance on underrepresented subgroups.”\nCamels and cattle\nThe MIT group has identified four principal types of shifts — spurious correlations, attribute imbalance, class imbalance, and attribute generalization — which, according to Yang, “have never been put together into a coherent and unified framework. We’ve come up with a single equation that shows you where biases can come from.”\nBiases can, in fact, stem from what the researchers call the class, or from the attribute, or both. To pick a simple example, suppose the task assigned to the machine learning model is to sort images of objects — animals in this case — into two classes: cows and camels. Attributes are descriptors that don’t specifically relate to the class itself. It might turn out, for instance, that all the images used in the analysis show cows standing on grass and camels on sand — grass and sand serving as the attributes here. Given the data available to it, the machine could reach an erroneous conclusion — namely that cows can only be found on grass, not on sand, with the opposite being true for camels. Such a finding would be incorrect, however, giving rise to a spurious correlation, which, Yang explains, is a “special case” among subpopulation shifts — “one in which you have a bias in both the class and the attribute.”\nIn a medical setting, one could rely on machine learning models to determine whether a person has pneumonia or not based on an examination of X-ray images. There would be two classes in this situation, one consisting of people who have the lung ailment, another for those who are infection-free. A relatively straightforward case would involve just two attributes: the people getting X-rayed are either female or male. If, in this particular dataset, there were 100 males diagnosed with pneumonia for every one female diagnosed with pneumonia, that could lead to an attribute imbalance, and the model would likely do a better job of correctly detecting pneumonia for a man than for a woman. Similarly, having 1,000 times more healthy (pneumonia-free) subjects than sick ones would lead to a class imbalance, with the model biased toward healthy cases. Attribute generalization is the last shift highlighted in the new study. If your sample contained 100 male patients with pneumonia and zero female subjects with the same illness, you still would like the model to be able to generalize and make predictions about female subjects even though there are no samples in the training data for females with pneumonia.\nThe team then took 20 advanced algorithms, designed to carry out classification tasks, and tested them on a dozen datasets to see how they performed across different population groups. They reached some unexpected conclusions: By improving the “classifier,” which is the last layer of the neural network, they were able to reduce the occurrence of spurious correlations and class imbalance, but the other shifts were unaffected. Improvements to the “encoder,” one of the uppermost layers in the neural network, could reduce the problem of attribute imbalance. “However, no matter what we did to the encoder or classifier, we did not see any improvements in terms of attribute generalization,” Yang says, “and we don’t yet know how to address that.”\nPrecisely accurate\nThere is also the question of assessing how well your model actually works in terms of evenhandedness among different population groups. The metric normally used, called worst-group accuracy or WGA, is based on the assumption that if you can improve the accuracy — of, say, medical diagnosis — for the group that has the worst model performance, you would have improved the model as a whole. “The WGA is considered the gold standard in subpopulation evaluation,” the authors contend, but they made a surprising discovery: boosting worst-group accuracy results in a decrease in what they call “worst-case precision.” In medical decision-making of all sorts, one needs both accuracy — which speaks to the validity of the findings — and precision, which relates to the reliability of the methodology. “Precision and accuracy are both very important metrics in classification tasks, and that is especially true in medical diagnostics,” Yang explains. “You should never trade precision for accuracy. You always need to balance the two.”\nThe MIT scientists are putting their theories into practice. In a study they're conducting with a medical center, they’re looking at public datasets for tens of thousands of patients and hundreds of thousands of chest X-rays, trying to see whether it’s possible for machine learning models to work in an unbiased manner for all populations. That’s still far from the case, even though more awareness has been drawn to this problem, Yang says. “We are finding many disparities across different ages, gender, ethnicity, and intersectional groups.”\nHe and his colleagues agree on the eventual goal, which is to achieve fairness in health care among all populations. But before we can reach that point, they maintain, we still need a better understanding of the sources of unfairness and how they permeate our current system. Reforming the system as a whole will not be easy, they acknowledge. In fact, the title of the paper they introduced at the Honolulu conference, “Change is Hard,” gives some indications as to the challenges that they and like-minded researchers face.\nThis research is funded by the MIT-IBM Watson AI Lab.\n",
    "published": "2023-08-17",
    "timestamp": "2023-08-30T13:26:06.921254"
  },
  {
    "unique_id": "9962ffa8-2f97-5cb7-ae79-1f059737b438",
    "title": "Study finds ChatGPT boosts worker productivity for some writing tasks",
    "description": "A new report by MIT researchers highlights the potential of generative AI to help workers with certain writing assignments.",
    "link": "https://news.mit.edu/2023/study-finds-chatgpt-boosts-worker-productivity-writing-0714",
    "blog_text": "Amid a huge amount of hype around generative AI, a new study from researchers at MIT sheds light on the technology’s impact on work, finding that it increased productivity for workers assigned tasks like writing cover letters, delicate emails, and cost-benefit analyses.\nThe tasks in the study weren’t quite replicas of real work: They didn’t require precise factual accuracy or context about things like a company’s goals or a customer’s preferences. Still, a number of the study’s participants said the assignments were similar to things they’d written in their real jobs — and the benefits were substantial. Access to the assistive chatbot ChatGPT decreased the time it took workers to complete the tasks by 40 percent, and output quality, as measured by independent evaluators, rose by 18 percent.\nThe researchers hope the study, which appears today in open-access form in the journal Science, helps people understand the impact that AI tools like ChatGPT can have on the workforce.\n“What we can say for sure is generative AI is going to have a big effect on white collar work,” says Shakked Noy, a PhD student in MIT’s Department of Economics, who co-authored the paper with fellow PhD student Whitney Zhang ’21. “I think what our study shows is that this kind of technology has important applications in white collar work. It’s a useful technology. But it’s still too early to tell if it will be good or bad, or how exactly it’s going to cause society to adjust.”\nSimulating work for chatbots\nFor centuries, people have worried that new technological advancements would lead to mass automation and job loss. But new technologies also create new jobs, and when they increase worker productivity, they can have a net positive effect on the economy.\n“Productivity is front of mind for economists when thinking of new technological developments,” Noy says. “The classical view in economics is that the most important thing that technological advancement does is raise productivity, in the sense of letting us produce economic output more efficiently.”\nTo study generative AI’s effect on worker productivity, the researchers gave 453 college-educated marketers, grant writers, consultants, data analysts, human resource professionals, and managers two writing tasks specific to their occupation. The 20- to 30-minute tasks included writing cover letters for grant applications, emails about organizational restructuring, and plans for analyses helping a company decide which customers to send push notifications to based on given customer data. Experienced professionals in the same occupations as each participant evaluated each submission as if they were encountering it in a work setting. Evaluators did not know which submissions were created with the help of ChatGPT.\nHalf of participants were given access to the chatbot ChatGPT-3.5, developed by the company OpenAI, for the second assignment. Those users finished tasks 11 minutes faster than the control group, while their average quality evaluations increased by 18 percent.\nThe data also showed that performance inequality between workers decreased, meaning workers who received a lower grade in the first task benefitted more from using ChatGPT for the second task.\nThe researchers say the tasks were broadly representative of assignments such professionals see in their real jobs, but they noted a number of limitations. Because they were using anonymous participants, the researchers couldn’t require contextual knowledge about a specific company or customer. They also had to give explicit instructions for each assignment, whereas real-world tasks may be more open-ended. Additionally, the researchers didn’t think it was feasible to hire fact-checkers to evaluate the accuracy of the outputs. Accuracy is a major problem for today’s generative AI technologies.\nThe researchers said those limitations could lessen ChatGPT’s productivity-boosting potential in the real world. Still, they believe the results show the technology’s promise — an idea supported by another of the study’s findings: Workers exposed to ChatGPT during the experiment were twice as likely to report using it in their real job two weeks after the experiment.\n“The experiment demonstrates that it does bring significant speed benefits, even if those speed benefits are lesser in the real world because you need to spend time fact-checking and writing the prompts,” Noy says.\nTaking the macro view\nThe study offered a close-up look at the impact that tools like ChatGPT can have on certain writing tasks. But extrapolating that impact out to understand generative AI’s effect on the economy is more difficult. That’s what the researchers hope to work on next.\n“There are so many other factors that are going to affect wages, employment, and shifts across sectors that would require pieces of evidence that aren’t in our paper,” Zhang says. “But the magnitude of time saved and quality increases are very large in our paper, so it does seem like this is pretty revolutionary, at least for certain types of work.”\nBoth researchers agree that, even if it’s accepted that ChatGPT will increase many workers’ productivity, much work remains to be done to figure out how society should respond to generative AI’s proliferation.\n“The policy needed to adjust to these technologies can be very different depending on what future research finds,” Zhang says. “If we think this will boost wages for lower-paid workers, that’s a very different implication than if it’s going to increase wage inequality by boosting the wages of already high earners. I think there’s a lot of downstream economic and political effects that are important to pin down.”\nThe study was supported by an Emergent Ventures grant, the Mercatus Center, George Mason University, a George and Obie Shultz Fund grant, the MIT Department of Economics, and a National Science Foundation Graduate Research Fellowship Grant.\n",
    "published": "2023-07-14",
    "timestamp": "2023-08-30T13:26:06.948055"
  },
  {
    "unique_id": "9ac18b23-9bd2-52a8-bd3f-8b25698b5d70",
    "title": "Scaling audio-visual learning without labels",
    "description": "A new multimodal technique blends major self-supervised learning methods to learn more similarly to humans.",
    "link": "https://news.mit.edu/2023/scaling-audio-visual-learning-without-labels-0605",
    "blog_text": "Researchers from MIT, the MIT-IBM Watson AI Lab, IBM Research, and elsewhere have developed a new technique for analyzing unlabeled audio and visual data that could improve the performance of machine-learning models used in applications like speech recognition and object detection. The work, for the first time, combines two architectures of self-supervised learning, contrastive learning and masked data modeling, in an effort to scale machine-learning tasks like event classification in single- and multimodal data without the need for annotation, thereby replicating how humans understand and perceive our world.\n“A larger portion of human knowledge is learned in a self-supervised way, because we don't always get supervision signals, and we want to enable the machine-learning model to have the same ability,” says Yuan Gong, an MIT postdoc in the Computer Science and Artificial Intelligence Laboratory (CSAIL).\n“So, another way to put it is that self-supervised learning often forms the foundation of an initial model, because it can learn on vast amounts of unlabeled data. And then you can use classical, supervised learning or reinforcement learning to fine tune the model to something particular if you want to,” says Jim Glass, an MIT senior research scientist and member of the MIT-IBM Watson AI Lab.\nThe technique, called the contrastive audio-visual masked autoencoder (CAV-MAE), is a type of neural network that can learn to extract and map meaningful latent representations into high-dimensional space from acoustic and visual data by training on large YouTube datasets of audio and video 10-second clips. The researchers say the technique is more effective than previous approaches because it explicitly models the relationships between audio and visual data in a way that other methods do not.\nJoining Gong and Glass on the study are graduate students Andrew Rouditchenko and Alexander H. Liu of MIT, David Harwath PhD ’18 of the University of Texas at Austin, and MIT-IBM Watson AI Lab members Leonid Karlinsky and Hilde Kuehne. Kuehne is also affiliated with Goethe University Frankfurt. The method was recently presented at the International Conference on Learning Representations.\nA joint and coordinated approach\nThe CAV-MAE works by “learning by prediction” and “learning by comparison,” says Gong. The masked data modeling, or the prediction method, takes a video along with its coordinated audio waveform, converts the audio to a spectrogram, and masks 75 percent of both. The unmasked data is tokenized, then fed into separate audio and visual encoders before entering a joint encoder/decoder, where the model is asked to recover the missing data. The difference (reconstruction loss) between the resulting reconstructed prediction and the original audio-visual combination is then used to train the model for better performance. An example of this would be covering part of a video of a piano and part of a spectrogram of piano music, and then asking the model to try to determine the masked inputs. Unfortunately, this method may not capture the association between the video and audio pair, whereas contrastive learning leverages this, but may discard some modality-unique information, like the background in a video.\nContrastive learning aims to map representations that are similar close to each other. For example, the model will attempt to place different video and audio data of different parrots close to each other and further away from pairs of video and audio of guitars playing. In a similar fashion to masked autoencoding, audio-visual pairs are passed into separate modality encoders; however, the audio and visual components are kept separately within the joint encoder before the model performs pooling and contrastive loss. In this way, contrastive learning tries to identify the parts of each audio or video that are most relevant to the other. For example, if a video shows someone speaking and the corresponding audio clip contains speech, the autoencoder will learn to associate the mouth movements of the speaker with the words being spoken. It will then adjust the model’s parameters so that those inputs are represented close to each other. Ultimately, the CAV-MAE method combines both techniques with multiple forward data streams with masking as a first step, modality-specific encoders, and layer normalization so that the representation strengths are similar.\n“We [then] wanted to compare the proposed CAV-MAE with a model trained only with a masked autoencoder and a model trained only with contrastive learning, because we want to show that by combining masked autoencoder and contrastive learning, we can get some performance improvement,” says Gong, “and the results support our hypothesis that there’s obvious improvement.”\nThe researchers tested CAV-MAE — as well as their method without contrastive loss or a masked autoencoder — against other state-of-the-art methods on audio-visual retrieval and audio-visual event classification tasks using standard AudioSet (20K and 2M) and VGGSound datasets — labeled, realistic short clips, which could include multiple sounds. Audio-visual retrieval means that the model sees either the audio or visual component of a query pair and searches for the missing one; event classification includes identifying actions or sounds within data, like a person singing or a car driving.\nOverall, they found that contrastive learning and masked data modeling are complementary methods. CAV-MAE was able to outperform previous techniques (with fully self-supervised pre-training) by about 2 percent for event classification performance verses models with comparable computation and, more impressively, kept pace with or outperformed models with industry-level computational resources. The team’s model ranked similarly to models trained with only the contrastive loss. And surprisingly, the team says, the incorporation of multi-modal data into CAV-MAE pre-training greatly improves the fine-tuning of single-modality representation via supervised learning (with some labeled data) and performance on audio-only event classification tasks. This demonstrates that, like humans, multi-modal information provides an additional “soft label” boost even for audio or visual only tasks; for instance, it helps the model to understand if it’s looking for an electric or acoustic guitar — a richer supervision signal.\n“I think people like the elegance of this model for combining information in the different audio and visual streams. It has the contrastive and the reconstruction loss, and compared to models that have been evaluated with similar data, it clearly does very well across a range of these tasks,” says Glass.\nBuilding on this, “one special thing is, our model can do both classification and the retrieval, which is not common,” Gong adds. “Before this work, these methods are used separately, but after this work, I see that most of the audio-visual learning frameworks use contracting loss and the masked autoencoder together, implicitly or explicitly.”\nBringing self-supervised audio-visual learning into our world\nThe researchers see their contribution of the contrastive audio-visual masked autoencoder (CAV-MAE) as an important milestone and a step forward for applications, which are increasingly moving from single modality to multi-modality and which require or leverage audio-visual fusion. They hypothesize that one day it could be used for action recognition in realms like sports, education, entertainment, motor vehicles, and public safety. It could also, one day, extend to other modalities. At this time, the fact that, “this only applies to audio-visual data may be a limitation, but we are targeting multi-modal learning, which is trend of machine learning,” says Gong. “As humans, we have multi-modalities — we have smell, touch — many more things that just audio-visual. So, when we try to build AI, we try to mimic humans somehow, not necessarily from the biological perspective, and this method could [potentially be] generalized to other unexplored modalities.”\nAs machine-learning models continue to play an increasingly important role in our lives, techniques like this one will become increasingly valuable.\nThis research was supported by the MIT-IBM Watson AI Lab.\n",
    "published": "2023-06-05",
    "timestamp": "2023-08-30T13:26:07.017477"
  },
  {
    "unique_id": "9b3fc437-d1ce-5ad5-b8a6-1de70da8010e",
    "title": "A simpler method for learning to control a robot",
    "description": "Researchers develop a machine-learning technique that can efficiently learn to control a robot, leading to better performance with fewer data.",
    "link": "https://news.mit.edu/2023/simpler-method-learning-control-robot-0726",
    "blog_text": "Researchers from MIT and Stanford University have devised a new machine-learning approach that could be used to control a robot, such as a drone or autonomous vehicle, more effectively and efficiently in dynamic environments where conditions can change rapidly.\nThis technique could help an autonomous vehicle learn to compensate for slippery road conditions to avoid going into a skid, allow a robotic free-flyer to tow different objects in space, or enable a drone to closely follow a downhill skier despite being buffeted by strong winds.\nThe researchers’ approach incorporates certain structure from control theory into the process for learning a model in such a way that leads to an effective method of controlling complex dynamics, such as those caused by impacts of wind on the trajectory of a flying vehicle. One way to think about this structure is as a hint that can help guide how to control a system.\n“The focus of our work is to learn intrinsic structure in the dynamics of the system that can be leveraged to design more effective, stabilizing controllers,” says Navid Azizan, the Esther and Harold E. Edgerton Assistant Professor in the MIT Department of Mechanical Engineering and the Institute for Data, Systems, and Society (IDSS), and a member of the Laboratory for Information and Decision Systems (LIDS). “By jointly learning the system’s dynamics and these unique control-oriented structures from data, we’re able to naturally create controllers that function much more effectively in the real world.”\nUsing this structure in a learned model, the researchers’ technique immediately extracts an effective controller from the model, as opposed to other machine-learning methods that require a controller to be derived or learned separately with additional steps. With this structure, their approach is also able to learn an effective controller using fewer data than other approaches. This could help their learning-based control system achieve better performance faster in rapidly changing environments.\n“This work tries to strike a balance between identifying structure in your system and just learning a model from data,” says lead author Spencer M. Richards, a graduate student at Stanford University. “Our approach is inspired by how roboticists use physics to derive simpler models for robots. Physical analysis of these models often yields a useful structure for the purposes of control — one that you might miss if you just tried to naively fit a model to data. Instead, we try to identify similarly useful structure from data that indicates how to implement your control logic.”\nAdditional authors of the paper are Jean-Jacques Slotine, professor of mechanical engineering and of brain and cognitive sciences at MIT, and Marco Pavone, associate professor of aeronautics and astronautics at Stanford. The research will be presented at the International Conference on Machine Learning (ICML).\nLearning a controller\nDetermining the best way to control a robot to accomplish a given task can be a difficult problem, even when researchers know how to model everything about the system.\nA controller is the logic that enables a drone to follow a desired trajectory, for example. This controller would tell the drone how to adjust its rotor forces to compensate for the effect of winds that can knock it off a stable path to reach its goal.\nThis drone is a dynamical system — a physical system that evolves over time. In this case, its position and velocity change as it flies through the environment. If such a system is simple enough, engineers can derive a controller by hand. \nModeling a system by hand intrinsically captures a certain structure based on the physics of the system. For instance, if a robot were modeled manually using differential equations, these would capture the relationship between velocity, acceleration, and force. Acceleration is the rate of change in velocity over time, which is determined by the mass of and forces applied to the robot.\nBut often the system is too complex to be exactly modeled by hand. Aerodynamic effects, like the way swirling wind pushes a flying vehicle, are notoriously difficult to derive manually, Richards explains. Researchers would instead take measurements of the drone’s position, velocity, and rotor speeds over time, and use machine learning to fit a model of this dynamical system to the data. But these approaches typically don’t learn a control-based structure. This structure is useful in determining how to best set the rotor speeds to direct the motion of the drone over time.\nOnce they have modeled the dynamical system, many existing approaches also use data to learn a separate controller for the system.\n“Other approaches that try to learn dynamics and a controller from data as separate entities are a bit detached philosophically from the way we normally do it for simpler systems. Our approach is more reminiscent of deriving models by hand from physics and linking that to control,” Richards says.\nIdentifying structure\nThe team from MIT and Stanford developed a technique that uses machine learning to learn the dynamics model, but in such a way that the model has some prescribed structure that is useful for controlling the system.\nWith this structure, they can extract a controller directly from the dynamics model, rather than using data to learn an entirely separate model for the controller.\n“We found that beyond learning the dynamics, it’s also essential to learn the control-oriented structure that supports effective controller design. Our approach of learning state-dependent coefficient factorizations of the dynamics has outperformed the baselines in terms of data efficiency and tracking capability, proving to be successful in efficiently and effectively controlling the system’s trajectory,” Azizan says. \nWhen they tested this approach, their controller closely followed desired trajectories, outpacing all the baseline methods. The controller extracted from their learned model nearly matched the performance of a ground-truth controller, which is built using the exact dynamics of the system.\n“By making simpler assumptions, we got something that actually worked better than other complicated baseline approaches,” Richards adds.\nThe researchers also found that their method was data-efficient, which means it achieved high performance even with few data. For instance, it could effectively model a highly dynamic rotor-driven vehicle using only 100 data points. Methods that used multiple learned components saw their performance drop much faster with smaller datasets.\nThis efficiency could make their technique especially useful in situations where a drone or robot needs to learn quickly in rapidly changing conditions.\nPlus, their approach is general and could be applied to many types of dynamical systems, from robotic arms to free-flying spacecraft operating in low-gravity environments.\nIn the future, the researchers are interested in developing models that are more physically interpretable, and that would be able to identify very specific information about a dynamical system, Richards says. This could lead to better-performing controllers.\n“Despite its ubiquity and importance, nonlinear feedback control remains an art, making it especially suitable for data-driven and learning-based methods. This paper makes a significant contribution to this area by proposing a method that jointly learns system dynamics, a controller, and control-oriented structure,” says Nikolai Matni, an assistant professor in the Department of Electrical and Systems Engineering at the University of Pennsylvania, who was not involved with this work. “What I found particularly exciting and compelling was the integration of these components into a joint learning algorithm, such that control-oriented structure acts as an inductive bias in the learning process. The result is a data-efficient learning process that outputs dynamic models that enjoy intrinsic structure that enables effective, stable, and robust control. While the technical contributions of the paper are excellent themselves, it is this conceptual contribution that I view as most exciting and significant.”\nThis research is supported, in part, by the NASA University Leadership Initiative and the Natural Sciences and Engineering Research Council of Canada.\n",
    "published": "2023-07-26",
    "timestamp": "2023-08-30T13:26:06.933916"
  },
  {
    "unique_id": "a20d48f2-53eb-5f8c-96bc-d4397043a884",
    "title": "How to help high schoolers prepare for the rise of artificial intelligence",
    "description": "A one-week summer program aims to foster a deeper understanding of machine-learning approaches in health among curious young minds. ",
    "link": "https://news.mit.edu/2023/how-to-help-high-schoolers-prepare-rise-of-artificial-intelligence-0824",
    "blog_text": "Should artificial intelligence be allowed to make care decisions for patients? Though the future of AI may conjure up doomsday visions of robots and computers intent on rendering human existence superfluous, the MIT Abdul Latif Jameel Clinic for Machine Learning in Health (Jameel Clinic) addressed questions surrounding the use of AI in health through their inaugural summer program focused on educating high school students. \nThe Jameel Clinic Summer Program, which took place July 10-21, accepted a total of 51 students from primarily Boston-area schools, with a commitment to reaching students from diverse backgrounds.  \nThe program, which split students up into two cohorts of 25 students for each week, had core offerings including courses like “Intro to Python,” “Intro to Clinical AI,” and “Intro to Drug Discovery” while also facilitating trips to various local institutions such as the Museum of Science Boston, Massachusetts General Hospital, Janssen Pharmaceuticals, and Amgen. \n“Organizing this boot camp had a personal significance to me. When my family immigrated to Israel, it was tough — my parents and I worked minimum wage jobs to survive,” School of Engineering Distinguished Professor and Jameel Clinic AI faculty lead Regina Barzilay recalls. “Going to university transformed my life. Many of the students in the program have similar backgrounds. I hope that exposing them to exciting science at MIT will open new opportunities for them.” \n“I’m not supposed to be here today,” stated Collin Stultz, the Nina T. and Robert H. Rubin Professor at MIT and Jameel Clinic principal investigator, on becoming both a computer scientist and cardiologist. In his lecture, Stultz spoke of the hardships his parents endured after immigrating to New York from Jamaica. He emphasized that he and his family members had never thought to apply to schools like Harvard University, thinking of it as a school for “people like the Kennedys” until Stultz got the idea to apply from a classmate who was planning to apply.  \n“It is my hope that the interactions between students in the Jameel Clinic Summer Program and MIT faculty will highlight the wealth of opportunities available at the intersection of computer science and medicine,” Stultz says. \nAs a result of a generous gift from Joseph Bates and Kristin Loeffler through their AI for Humanity Foundation, the Jameel Clinic was able to offer the summer program at no cost and reduce the financial barriers for students from under-resourced backgrounds. Bates shared that at the age of 13 he was discovered by a psychology professor at Johns Hopkins University and became the first teenager to enter the university. “I had been doing an adequate, but not good, job in a dangerous Baltimore City public junior high school,” Bates says. “Being at Hopkins was wonderful, socially and intellectually, and it led me to a computer science PhD at Cornell University, then CS professor at Carnegie Mellon University. Someone taking an interest really mattered, and it changed my life.” \nAccording to the National Science Foundation, the U.S. STEM workforce gradually diversified between 2011 and 2021, with increased representation of women and underrepresented students of color. But in the college-educated workforce, a 2021 report showed that just 16 percent of engineers were women and 16 percent of underrepresented students of color — Hispanic, Black, and American Indian or Indigenous Alaskan individuals — were employed in science and engineering occupations with at least a bachelor’s degree. \nAngely Mejia Martinez, a rising junior at Chelsea High School and aspiring doctor, highlighted Jameel Clinic chair and MIT Institute Professor Phillip Sharp’s talk as one of her favorites. Sharp spoke about growing up on a small farm in rural Kentucky before setting off on his career in science, which eventually led to his 1993 Nobel Prize in Physiology and Medicine. “I really got inspired by that because when I was little, many people would say ‘I don’t think you can do this,’ and I was always like ‘I can do this,’” Martinez says. “I think I can achieve anything I set my mind into.” \n“It was very surreal because I didn’t think I’d be here,” Priyani Rawal, a rising junior studying information technology at Essex North Shore Agricultural and Technical School, says. Rawal’s favorite class was Barzilay’s Intro to AI/ML lecture. “I was so amazed by what we were learning ... it made me inspired to go into [the machine learning] field.” \nAdam Nouri, a rising senior at Pioneer Charter School II, signed up for the program after receiving an email from his computer science teacher. Before applying, Nouri had considered enrolling in a summer course for programming at Bunker Hill Community College, an option typically offered at no cost to Pioneer students. However, Nouri quickly realized that free enrollment was only available during the school year and says it would have cost around $800 for him to enroll in the summer. If he hadn’t gotten into the Jameel Clinic Summer Program, Nouri believes he would have continued working at his part-time service job for the rest of the summer while trying to code a game or build a computer with his friends in his free time. “When I got into the [Jameel Clinic Summer Program], I was actually really excited,” Nouri recalls. “Now I feel like I have a clearer path I want to pursue.” \nAs part of their final group project presentations given on the last day of the program, students were assigned AI tools used in clinical settings or drug discovery, like PathAI or AlphaFold2, and asked to explain their assigned tool along with its potential benefits and risks to a target audience of their choice. \n“There is a heavy emphasis placed not only on innovation in science, health care and technology, but also on collaboration across disciplines,” Jay Ananth, a rising junior at Troy High School, says. “During the summer program, I was taught AI and health care not as a high school student, but as a peer — a fellow researcher — who has the ability to innovate and make a change.” \nSerena Hu, a rising junior at Lincoln Sudbury High School, felt less uncertainty about her future after attending the program. “I always wanted to try new things so that I could find something that I love to do, but I can pretty confidently say that I found it here,” Hu says. “They’re not just teaching you the material — they’re also inspiring you.” \nThe Jameel Clinic Summer Program was organized by Ignacio Fuentes, Alex Ouyang, and Marinalva Smith. Maggie Wang, Antonella Catanzaro, Ciarra Brodie, Rohan Kundargi, and Christina Moscat helped to oversee and contribute to the success of the program. Instructors included Pulkit Agrawal, Sharifa Alghowinem, Shrooq Alsenan, Manisha Bahl, Regina Barzilay, Rebecca Boiarsky, Felix Faltings, Florian Fintelmann, Marzyeh Ghassemi, Susan Hockfield, Insoo Hyun, Noah Jones, Ila Kumar, Peter Mikhael, Carles Monterrubio, Tiffany Pereira Portela, Phillip Sharp, Hannes Stärk, Vinith Suriyakumar, Oliver Thiel, Randi Williams, Jeremy Wohlwend, and Rachel Wu.\n",
    "published": "2023-08-24",
    "timestamp": "2023-08-30T13:26:06.903667"
  },
  {
    "unique_id": "a3f439db-fbcb-5c73-879c-413384a07a74",
    "title": "Supporting sustainability, digital health, and the future of work",
    "description": "The MIT and Accenture Convergence Initiative for Industry and Technology selects three new research projects to support. ",
    "link": "https://news.mit.edu/2023/supporting-sustainability-digital-health-future-work-0824",
    "blog_text": "The MIT and Accenture Convergence Initiative for Industry and Technology has selected three new research projects that will receive support from the initiative. The research projects aim to accelerate progress in meeting complex societal needs through new business convergence insights in technology and innovation.\nEstablished in MIT’s School of Engineering and now in its third year, the MIT and Accenture Convergence Initiative is furthering its mission to bring together technological experts from across business and academia to share insights and learn from one another. Recently, Thomas W. Malone, the Patrick J. McGovern (1959) Professor of Management, joined the initiative as its first-ever faculty lead. The research projects relate to three of the initiative’s key focus areas: sustainability, digital health, and the future of work.\n“The solutions these research teams are developing have the potential to have tremendous impact,” says Anantha Chandrakasan, dean of the School of Engineering and the Vannevar Bush Professor of Electrical Engineering and Computer Science. “They embody the initiative’s focus on advancing data-driven research that addresses technology and industry convergence.”\n“The convergence of science and technology driven by advancements in generative AI, digital twins, quantum computing, and other technologies makes this an especially exciting time for Accenture and MIT to be undertaking this joint research,” says Kenneth Munie, senior managing director at Accenture Strategy, Life Sciences. “Our three new research projects focusing on sustainability, digital health, and the future of work have the potential to help guide and shape future innovations that will benefit the way we work and live.”\nThe MIT and Accenture Convergence Initiative charter project researchers are described below.\nAccelerating the journey to net zero with industrial clusters\nJessika Trancik is a professor at the Institute for Data, Systems, and Society (IDSS). Trancik’s research examines the dynamic costs, performance, and environmental impacts of energy systems to inform climate policy and accelerate beneficial and equitable technology innovation. Trancik’s project aims to identify how industrial clusters can enable companies to derive greater value from decarbonization, potentially making companies more willing to invest in the clean energy transition.\nTo meet the ambitious climate goals that have been set by countries around the world, rising greenhouse gas emissions trends must be rapidly reversed. Industrial clusters — geographically co-located or otherwise-aligned groups of companies representing one or more industries — account for a significant portion of greenhouse gas emissions globally. With major energy consumers “clustered” in proximity, industrial clusters provide a potential platform to scale low-carbon solutions by enabling the aggregation of demand and the coordinated investment in physical energy supply infrastructure.\nIn addition to Trancik, the research team working on this project will include Aliza Khurram, a postdoc in IDSS; Micah Ziegler, an IDSS research scientist; Melissa Stark, global energy transition services lead at Accenture; Laura Sanderfer, strategy consulting manager at Accenture; and Maria De Miguel, strategy senior analyst at Accenture.\nEliminating childhood obesity\nAnette \"Peko\" Hosoi is the Neil and Jane Pappalardo Professor of Mechanical Engineering. A common theme in her work is the fundamental study of shape, kinematic, and rheological optimization of biological systems with applications to the emergent field of soft robotics. Her project will use both data from existing studies and synthetic data to create a return-on-investment (ROI) calculator for childhood obesity interventions so that companies can identify earlier returns on their investment beyond reduced health-care costs.\nChildhood obesity is too prevalent to be solved by a single company, industry, drug, application, or program. In addition to the physical and emotional impact on children, society bears a cost through excess health care spending, lost workforce productivity, poor school performance, and increased family trauma. Meaningful solutions require multiple organizations, representing different parts of society, working together with a common understanding of the problem, the economic benefits, and the return on investment. ROI is particularly difficult to defend for any single organization because investment and return can be separated by many years and involve asymmetric investments, returns, and allocation of risk. Hosoi’s project will consider the incentives for a particular entity to invest in programs in order to reduce childhood obesity.\nHosoi will be joined by graduate students Pragya Neupane and Rachael Kha, both of IDSS, as well a team from Accenture that includes Kenneth Munie, senior managing director at Accenture Strategy, Life Sciences; Kaveh Safavi, senior managing director in Accenture Health Industry; and Elizabeth Naik, global health and public service research lead.\nGenerating innovative organizational configurations and algorithms for dealing with the problem of post-pandemic employment\nThomas Malone is the Patrick J. McGovern (1959) Professor of Management at the MIT Sloan School of Management and the founding director of the MIT Center for Collective Intelligence. His research focuses on how new organizations can be designed to take advantage of the possibilities provided by information technology. Malone will be joined in this project by John Horton, the Richard S. Leghorn (1939) Career Development Professor at the MIT Sloan School of Management, whose research focuses on the intersection of labor economics, market design, and information systems. Malone and Horton’s project will look to reshape the future of work with the help of lessons learned in the wake of the pandemic.\nThe Covid-19 pandemic has been a major disrupter of work and employment, and it is not at all obvious how governments, businesses, and other organizations should manage the transition to a desirable state of employment as the pandemic recedes. Using natural language processing algorithms such as GPT-4, this project will look to identify new ways that companies can use AI to better match applicants to necessary jobs, create new types of jobs, assess skill training needed, and identify interventions to help include women and other groups whose employment was disproportionately affected by the pandemic.\nIn addition to Malone and Horton, the research team will include Rob Laubacher, associate director and research scientist at the MIT Center for Collective Intelligence, and Kathleen Kennedy, executive director at the MIT Center for Collective Intelligence and senior director at MIT Horizon. The team will also include Nitu Nivedita, managing director of artificial intelligence at Accenture, and Thomas Hancock, data science senior manager at Accenture.\n",
    "published": "2023-08-24",
    "timestamp": "2023-08-30T13:26:06.905969"
  },
  {
    "unique_id": "a8149e62-2699-53ca-93c0-a3f93c7a14fc",
    "title": "Machine-learning system based on light could yield more powerful, efficient large language models",
    "description": "MIT system demonstrates greater than 100-fold improvement in energy efficiency and a 25-fold improvement in compute density compared with current systems.",
    "link": "https://news.mit.edu/2023/system-could-yield-more-powerful-efficient-llms-0822",
    "blog_text": "ChatGPT has made headlines around the world with its ability to write essays, email, and computer code based on a few prompts from a user. Now an MIT-led team reports a system that could lead to machine-learning programs several orders of magnitude more powerful than the one behind ChatGPT. The system they developed could also use several orders of magnitude less energy than the state-of-the-art supercomputers behind the machine-learning models of today.\nIn the July 17 issue of Nature Photonics, the researchers report the first experimental demonstration of the new system, which performs its computations based on the movement of light, rather than electrons, using hundreds of micron-scale lasers. With the new system, the team reports a greater than 100-fold improvement in energy efficiency and a 25-fold improvement in compute density, a measure of the power of a system, over state-of-the-art digital computers for machine learning. \nToward the future\nIn the paper, the team also cites “substantially several more orders of magnitude for future improvement.” As a result, the authors continue, the technique “opens an avenue to large-scale optoelectronic processors to accelerate machine-learning tasks from data centers to decentralized edge devices.” In other words, cellphones and other small devices could become capable of running programs that can currently only be computed at large data centers.\nFurther, because the components of the system can be created using fabrication processes already in use today, “we expect that it could be scaled for commercial use in a few years. For example, the laser arrays involved are widely used in cell-phone face ID and data communication,” says Zaijun Chen, first author, who conducted the work while a postdoc at MIT in the Research Laboratory of Electronics (RLE) and is now an assistant professor at the University of Southern California.\nSays Dirk Englund, an associate professor in MIT’s Department of Electrical Engineering and Computer Science and leader of the work, “ChatGPT is limited in its size by the power of today’s supercomputers. It’s just not economically viable to train models that are much bigger. Our new technology could make it possible to leapfrog to machine-learning models that otherwise would not be reachable in the near future.”\nHe continues, “We don’t know what capabilities the next-generation ChatGPT will have if it is 100 times more powerful, but that’s the regime of discovery that this kind of technology can allow.” Englund is also leader of MIT’s Quantum Photonics Laboratory and is affiliated with the RLE and the Materials Research Laboratory.\nA drumbeat of progress\nThe current work is the latest achievement in a drumbeat of progress over the last few years by Englund and many of the same colleagues. For example, in 2019 an Englund team reported the theoretical work that led to the current demonstration. The first author of that paper, Ryan Hamerly, now of RLE and NTT Research Inc., is also an author of the current paper.\nAdditional coauthors of the current Nature Photonics paper are Alexander Sludds, Ronald Davis, Ian Christen, Liane Bernstein, and Lamia Ateshian, all of RLE; and Tobias Heuser, Niels Heermeier, James A. Lott, and Stephan Reitzensttein of Technische Universitat Berlin.\nDeep neural networks (DNNs) like the one behind ChatGPT are based on huge machine-learning models that simulate how the brain processes information. However, the digital technologies behind today’s DNNs are reaching their limits even as the field of machine learning is growing. Further, they require huge amounts of energy and are largely confined to large data centers. That is motivating the development of new computing paradigms.\nUsing light rather than electrons to run DNN computations has the potential to break through the current bottlenecks. Computations using optics, for example, have the potential to use far less energy than those based on electronics. Further, with optics, “you can have much larger bandwidths,” or compute densities, says Chen. Light can transfer much more information over a much smaller area.\nBut current optical neural networks (ONNs) have significant challenges. For example, they use a great deal of energy because they are inefficient at converting incoming data based on electrical energy into light. Further, the components involved are bulky and take up significant space. And while ONNs are quite good at linear calculations like adding, they are not great at nonlinear calculations like multiplication and “if” statements.\nIn the current work the researchers introduce a compact architecture that, for the first time, solves all of these challenges and two more simultaneously. That architecture is based on state-of-the-art arrays of vertical surface-emitting lasers (VCSELs), a relatively new technology used in applications including lidar remote sensing and laser printing. The particular VCELs reported in the Nature Photonics paper were developed by the Reitzenstein group at Technische Universitat Berlin. “This was a collaborative project that would not have been possible without them,” Hamerly says.\nLogan Wright, an assistant professor at Yale University who was not involved in the current research, comments, “The work by Zaijun Chen et al. is inspiring, encouraging me and likely many other researchers in this area that systems based on modulated VCSEL arrays could be a viable route to large-scale, high-speed optical neural networks. Of course, the state of the art here is still far from the scale and cost that would be necessary for practically useful devices, but I am optimistic about what can be realized in the next few years, especially given the potential these systems have to accelerate the very large-scale, very expensive AI systems like those used in popular textual ‘GPT’ systems like ChatGPT.”\nChen, Hamerly, and Englund have filed for a patent on the work, which was sponsored by the U.S. Army Research Office, NTT Research, the U.S. National Defense Science and Engineering Graduate Fellowship Program, the U.S. National Science Foundation, the Natural Sciences and Engineering Research Council of Canada, and the Volkswagen Foundation.\n",
    "published": "2023-08-22",
    "timestamp": "2023-08-30T13:26:06.914337"
  },
  {
    "unique_id": "b8997185-05cd-5189-830e-cd842484c903",
    "title": "Computer vision system marries image recognition and generation ",
    "description": "MAGE merges the two key tasks of image generation and recognition, typically trained separately, into a single system.",
    "link": "https://news.mit.edu/2023/computer-vision-system-marries-image-recognition-generation-0628",
    "blog_text": "Computers possess two remarkable capabilities with respect to images: They can both identify them and generate them anew. Historically, these functions have stood separate, akin to the disparate acts of a chef who is good at creating dishes (generation), and a connoisseur who is good at tasting dishes (recognition).\nYet, one can’t help but wonder: What would it take to orchestrate a harmonious union between these two distinctive capacities? Both chef and connoisseur share a common understanding in the taste of the food. Similarly, a unified vision system requires a deep understanding of the visual world.\nNow, researchers in MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) have trained a system to infer the missing parts of an image, a task that requires deep comprehension of the image's content. In successfully filling in the blanks, the system, known as the Masked Generative Encoder (MAGE), achieves two goals at the same time: accurately identifying images and creating new ones with striking resemblance to reality. \nThis dual-purpose system enables myriad potential applications, like object identification and classification within images, swift learning from minimal examples, the creation of images under specific conditions like text or class, and enhancing existing images.\nUnlike other techniques, MAGE doesn't work with raw pixels. Instead, it converts images into what’s called “semantic tokens,” which are compact, yet abstracted, versions of an image section. Think of these tokens as mini jigsaw puzzle pieces, each representing a 16x16 patch of the original image. Just as words form sentences, these tokens create an abstracted version of an image that can be used for complex processing tasks, while preserving the information in the original image. Such a tokenization step can be trained within a self-supervised framework, allowing it to pre-train on large image datasets without labels. \nNow, the magic begins when MAGE uses “masked token modeling.” It randomly hides some of these tokens, creating an incomplete puzzle, and then trains a neural network to fill in the gaps. This way, it learns to both understand the patterns in an image (image recognition) and generate new ones (image generation).\n“One remarkable part of MAGE is its variable masking strategy during pre-training, allowing it to train for either task, image generation or recognition, within the same system,” says Tianhong Li, a PhD student in electrical engineering and computer science at MIT, a CSAIL affiliate, and the lead author on a paper about the research. “MAGE's ability to work in the ‘token space’ rather than ‘pixel space’ results in clear, detailed, and high-quality image generation, as well as semantically rich image representations. This could hopefully pave the way for advanced and integrated computer vision models.” \nApart from its ability to generate realistic images from scratch, MAGE also allows for conditional image generation. Users can specify certain criteria for the images they want MAGE to generate, and the tool will cook up the appropriate image. It’s also capable of image editing tasks, such as removing elements from an image while maintaining a realistic appearance.\nRecognition tasks are another strong suit for MAGE. With its ability to pre-train on large unlabeled datasets, it can classify images using only the learned representations. Moreover, it excels at few-shot learning, achieving impressive results on large image datasets like ImageNet with only a handful of labeled examples.\nThe validation of MAGE's performance has been impressive. On one hand, it set new records in generating new images, outperforming previous models with a significant improvement. On the other hand, MAGE topped in recognition tasks, achieving an 80.9 percent accuracy in linear probing and a 71.9 percent 10-shot accuracy on ImageNet (this means it correctly identified images in 71.9 percent of cases where it had only 10 labeled examples from each class).\nDespite its strengths, the research team acknowledges that MAGE is a work in progress. The process of converting images into tokens inevitably leads to some loss of information. They are keen to explore ways to compress images without losing important details in future work. The team also intends to test MAGE on larger datasets. Future exploration might include training MAGE on larger unlabeled datasets, potentially leading to even better performance. \n“It has been a long dream to achieve image generation and image recognition in one single system. MAGE is a groundbreaking research which successfully harnesses the synergy of these two tasks and achieves the state-of-the-art of them in one single system,” says Huisheng Wang, senior staff software engineer of humans and interactions in the Research and Machine Intelligence division at Google, who was not involved in the work. “This innovative system has wide-ranging applications, and has the potential to inspire many future works in the field of computer vision.” \nLi wrote the paper along with Dina Katabi, the Thuan and Nicole Pham Professor in the MIT Department of Electrical Engineering and Computer Science and a CSAIL principal investigator; Huiwen Chang, a senior research scientist at Google; Shlok Kumar Mishra, a University of Maryland PhD student and Google Research intern; Han Zhang, a senior research scientist at Google; and Dilip Krishnan, a staff research scientist at Google. Computational resources were provided by Google Cloud Platform and the MIT-IBM Watson AI Lab. The team's research was presented at the 2023 Conference on Computer Vision and Pattern Recognition.\n",
    "published": "2023-06-28",
    "timestamp": "2023-08-30T13:26:06.976624"
  },
  {
    "unique_id": "b8ece89c-c979-5993-ba1b-a2c8733dae8a",
    "title": "A new way to look at data privacy",
    "description": "Researchers create a privacy technique that protects sensitive data while maintaining a machine-learning model’s performance.",
    "link": "https://news.mit.edu/2023/new-way-look-data-privacy-0714",
    "blog_text": "Imagine that a team of scientists has developed a machine-learning model that can predict whether a patient has cancer from lung scan images. They want to share this model with hospitals around the world so clinicians can start using it in diagnosis.\nBut there’s a problem. To teach their model how to predict cancer, they showed it millions of real lung scan images, a process called training. Those sensitive data, which are now encoded into the inner workings of the model, could potentially be extracted by a malicious agent. The scientists can prevent this by adding noise, or more generic randomness, to the model that makes it harder for an adversary to guess the original data. However, perturbation reduces a model’s accuracy, so the less noise one can add, the better. \nMIT researchers have developed a technique that enables the user to potentially add the smallest amount of noise possible, while still ensuring the sensitive data are protected.\nThe researchers created a new privacy metric, which they call Probably Approximately Correct (PAC) Privacy, and built a framework based on this metric that can automatically determine the minimal amount of noise that needs to be added. Moreover, this framework does not need knowledge of the inner workings of a model or its training process, which makes it easier to use for different types of models and applications.\nIn several cases, the researchers show that the amount of noise required to protect sensitive data from adversaries is far less with PAC Privacy than with other approaches. This could help engineers create machine-learning models that provably hide training data, while maintaining accuracy in real-world settings.\n“PAC Privacy exploits the uncertainty or entropy of the sensitive data in a meaningful way,  and this allows us to add, in many cases, an order of magnitude less noise. This framework allows us to understand the characteristics of arbitrary data processing and privatize it automatically without artificial modifications. While we are in the early days and we are doing simple examples, we are excited about the promise of this technique,” says Srini Devadas, the Edwin Sibley Webster Professor of Electrical Engineering and co-author of a new paper on PAC Privacy.\nDevadas wrote the paper with lead author Hanshen Xiao, an electrical engineering and computer science graduate student. The research will be presented at the International Cryptography Conference (Crypto 2023).\nDefining privacy\nA fundamental question in data privacy is: How much sensitive data could an adversary recover from a machine-learning model with noise added to it?\nDifferential Privacy, one popular privacy definition, says privacy is achieved if an adversary who observes the released model cannot infer whether an arbitrary individual’s data is used for the training processing. But provably preventing an adversary from distinguishing data usage often requires large amounts of noise to obscure it. This noise reduces the model’s accuracy.\nPAC Privacy looks at the problem a bit differently. It characterizes how hard it would be for an adversary to reconstruct any part of randomly sampled or generated sensitive data after noise has been added, rather than only focusing on the distinguishability problem.\nFor instance, if the sensitive data are images of human faces, differential privacy would focus on whether the adversary can tell if someone’s face was in the dataset. PAC Privacy, on the other hand, could look at whether an adversary could extract a silhouette — an approximation — that someone could recognize as a particular individual’s face.\nOnce they established the definition of PAC Privacy, the researchers created an algorithm that automatically tells the user how much noise to add to a model to prevent an adversary from confidently reconstructing a close approximation of the sensitive data. This algorithm guarantees privacy even if the adversary has infinite computing power, Xiao says.\nTo find the optimal amount of noise, the PAC Privacy algorithm relies on the uncertainty, or entropy, in the original data from the viewpoint of the adversary.\nThis automatic technique takes samples randomly from a data distribution or a large data pool and runs the user’s machine-learning training algorithm on that subsampled data to produce an output learned model. It does this many times on different subsamplings and compares the variance across all outputs. This variance determines how much noise one must add — a smaller variance means less noise is needed.\nAlgorithm advantages\nDifferent from other privacy approaches, the PAC Privacy algorithm does not need knowledge of the inner workings of a model, or the training process.\nWhen implementing PAC Privacy, a user can specify their desired level of confidence at the outset. For instance, perhaps the user wants a guarantee that an adversary will not be more than 1 percent confident that they have successfully reconstructed the sensitive data to within 5 percent of its actual value. The PAC Privacy algorithm automatically tells the user the optimal amount of noise that needs to be added to the output model before it is shared publicly, in order to achieve those goals.\n“The noise is optimal, in the sense that if you add less than we tell you, all bets could be off. But the effect of adding noise to neural network parameters is complicated, and we are making no promises on the utility drop the model may experience with the added noise,” Xiao says.\nThis points to one limitation of PAC Privacy — the technique does not tell the user how much accuracy the model will lose once the noise is added. PAC Privacy also involves repeatedly training a machine-learning model on many subsamplings of data, so it can be computationally expensive.  \nTo improve PAC Privacy, one approach is to modify a user’s machine-learning training process so it is more stable, meaning that the output model it produces does not change very much when the input data is subsampled from a data pool.  This stability would create smaller variances between subsample outputs, so not only would the PAC Privacy algorithm need to be run fewer times to identify the optimal amount of noise, but it would also need to add less noise.\nAn added benefit of stabler models is that they often have less generalization error, which means they can make more accurate predictions on previously unseen data, a win-win situation between machine learning and privacy, Devadas adds.\n“In the next few years, we would love to look a little deeper into this relationship between stability and privacy, and the relationship between privacy and generalization error. We are knocking on a door here, but it is not clear yet where the door leads,” he says.\n“Obfuscating the usage of an individual’s data in a model is paramount to protecting their privacy. However, to do so can come at the cost of the datas’ and therefore model’s utility,” says Jeremy Goodsitt, senior machine learning engineer at Capital One, who was not involved with this research. “PAC provides an empirical, black-box solution, which can reduce the added noise compared to current practices while maintaining equivalent privacy guarantees. In addition, its empirical approach broadens its reach to more data consuming applications.”\nThis research is funded, in part, by DSTA Singapore, Cisco Systems, Capital One, and a MathWorks Fellowship.\n",
    "published": "2023-07-14",
    "timestamp": "2023-08-30T13:26:06.950469"
  },
  {
    "unique_id": "c709b2dc-8d59-5dce-85c6-b3642aa7c54b",
    "title": "Three Spanish MIT physics postdocs receive Botton Foundation fellowships",
    "description": "Recipients Luis Antonio Benítez, Carolina Cuesta-Lazaro, and Fernando Romero López receive support for their scientific research.",
    "link": "https://news.mit.edu/2023/botton-foundation-fellowships-spanish-mit-physics-postdocs-0609",
    "blog_text": "Three Spanish MIT postdocs, Luis Antonio Benítez, Carolina Cuesta-Lazaro, and Fernando Romero López, were chosen by the Department of Physics as the first cohort of Mauricio and Carlota Botton Foundation Fellows.\nThis year’s recipients are provided with a one-year stipend and a research fund to pursue their research interests; they will visit the Botton Foundation in Madrid this summer.\nL. Antonio Benítez\nA dual citizen of Spain and Colombia, L. Antonio Benítez is an MIT postdoc whose research focuses on the investigation of the electronic properties of novel quantum materials, with a particular emphasis on two-dimensional materials like graphene and transition metal dichalcogenides. His work aims to push the boundaries of our knowledge of these materials and unlock their full potential for future technologies. Benítez received his PhD in physics from the Autonomous University of Barcelona, where he specialized in the spin and electronic properties of these materials, developing a deep understanding of their unique characteristics and behavior.  \nCarolina Cuesta-Lazaro \nCarolina Cuestra-Lazaro’s main research interests lie on the intersection of cosmology and artificial intelligence. She is interested in developing robust and interpretable machine-learning models for advancement in physics, especially for developing techniques for cosmological inference to understand the accelerated expansion of the universe. She received her PhD in astronomy and astrophysics at the Institute for Computational Cosmology, and now holds a shared position between MIT’s Institute for Artificial Intelligence and Fundamental Interactions and Harvard University’s Institute for Theory and Computation at the Center for Astrophysics. Cuestra-Lazaro hails from Cuenca, where she says “You can find some of the best Manchego cheese.”\nFernando Romero López  \nRomero-López completed his PhD in 2021 at the University of Valencia. As a postdoc, his research focuses on understanding the strong interactions among quarks and gluons, described by quantum chromodynamics (QCD). By combining effective field theories with numerical simulations of quantum field theories (lattice QCD) and machine-learning tools, he is seeking a better understanding of the mechanisms of confinement, how protons, neutrons, and other hadrons are formed, the properties of atomic nuclei, and the nature of exotic hadrons that have been detected at the Large Hadron Collider.\nThe foundation also recently funded scholarships for two PhD physics students at MIT: Oriol Rubies Bigorda, who is researching the physics of interacting quantum particles and their applications in future quantum technologies, and Miguel Calvo Carrera, who is interested in the application of physics to develop renewable energy sources.\nEstablished in 2017, the Mauricio and Carlota Botton Foundation supports scientific research, including the training of young physicists in the most prestigious universities in the world, and to provide support for conferences that bring world experts in the frontier fields of physics to Spain.\n",
    "published": "2023-06-09",
    "timestamp": "2023-08-30T13:26:07.003240"
  },
  {
    "unique_id": "cf2d0b97-5bce-574a-9d07-7aa590925f4a",
    "title": "Using AI to protect against AI image manipulation ",
    "description": "“PhotoGuard,” developed by MIT CSAIL researchers, prevents unauthorized image manipulation, safeguarding authenticity in the era of advanced generative models.",
    "link": "https://news.mit.edu/2023/using-ai-protect-against-ai-image-manipulation-0731",
    "blog_text": "As we enter a new era where technologies powered by artificial intelligence can craft and manipulate images with a precision that blurs the line between reality and fabrication, the specter of misuse looms large. Recently, advanced generative models such as DALL-E and Midjourney, celebrated for their impressive precision and user-friendly interfaces, have made the production of hyper-realistic images relatively effortless. With the barriers of entry lowered, even inexperienced users can generate and manipulate high-quality images from simple text descriptions — ranging from innocent image alterations to malicious changes. Techniques like watermarking pose a promising solution, but misuse requires a preemptive (as opposed to only post hoc) measure. \nIn the quest to create such a new measure, researchers from MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) developed “PhotoGuard,” a technique that uses perturbations — minuscule alterations in pixel values invisible to the human eye but detectable by computer models — that effectively disrupt the model’s ability to manipulate the image.\nPhotoGuard uses two different “attack” methods to generate these perturbations. The more straightforward “encoder” attack targets the image’s latent representation in the AI model, causing the model to perceive the image as a random entity. The more sophisticated “diffusion” one defines a target image and optimizes the perturbations to make the final image resemble the target as closely as possible.\n“Consider the possibility of fraudulent propagation of fake catastrophic events, like an explosion at a significant landmark. This deception can manipulate market trends and public sentiment, but the risks are not limited to the public sphere. Personal images can be inappropriately altered and used for blackmail, resulting in significant financial implications when executed on a large scale,” says Hadi Salman, an MIT graduate student in electrical engineering and computer science (EECS), affiliate of MIT CSAIL, and lead author of a new paper about PhotoGuard. \n“In more extreme scenarios, these models could simulate voices and images for staging false crimes, inflicting psychological distress and financial loss. The swift nature of these actions compounds the problem. Even when the deception is eventually uncovered, the damage — whether reputational, emotional, or financial — has often already happened. This is a reality for victims at all levels, from individuals bullied at school to society-wide manipulation.”\nPhotoGuard in practice\nAI models view an image differently from how humans do. It sees an image as a complex set of mathematical data points that describe every pixel's color and position — this is the image's latent representation. The encoder attack introduces minor adjustments into this mathematical representation, causing the AI model to perceive the image as a random entity. As a result, any attempt to manipulate the image using the model becomes nearly impossible. The changes introduced are so minute that they are invisible to the human eye, thus preserving the image's visual integrity while ensuring its protection.\nThe second and decidedly more intricate “diffusion” attack strategically targets the entire diffusion model end-to-end. This involves determining a desired target image, and then initiating an optimization process with the intention of closely aligning the generated image with this preselected target.\nIn implementing, the team created perturbations within the input space of the original image. These perturbations are then used during the inference stage, and applied to the images, offering a robust defense against unauthorized manipulation.\n“The progress in AI that we are witnessing is truly breathtaking, but it enables beneficial and malicious uses of AI alike,” says MIT professor of EECS and CSAIL principal investigator Aleksander Madry, who is also an author on the paper. “It is thus urgent that we work towards identifying and mitigating the latter. I view PhotoGuard as our small contribution to that important effort.”\nThe diffusion attack is more computationally intensive than its simpler sibling, and requires significant GPU memory. The team says that approximating the diffusion process with fewer steps mitigates the issue, thus making the technique more practical.\nTo better illustrate the attack, consider an art project, for example. The original image is a drawing, and the target image is another drawing that’s completely different. The diffusion attack is like making tiny, invisible changes to the first drawing so that, to an AI model, it begins to resemble the second drawing. However, to the human eye, the original drawing remains unchanged.\nBy doing this, any AI model attempting to modify the original image will now inadvertently make changes as if dealing with the target image, thereby protecting the original image from intended manipulation. The result is a picture that remains visually unaltered for human observers, but protects against unauthorized edits by AI models.\nAs far as a real example with PhotoGuard, consider an image with multiple faces. You could mask any faces you don’t want to modify, and then prompt with “two men attending a wedding.” Upon submission, the system will adjust the image accordingly, creating a plausible depiction of two men participating in a wedding ceremony.\nNow, consider safeguarding the image from being edited; adding perturbations to the image before upload can immunize it against modifications. In this case, the final output will lack realism compared to the original, non-immunized image.\nAll hands on deck\nKey allies in the fight against image manipulation are the creators of the image-editing models, says the team. For PhotoGuard to be effective, an integrated response from all stakeholders is necessary. “Policymakers should consider implementing regulations that mandate companies to protect user data from such manipulations. Developers of these AI models could design APIs that automatically add perturbations to users’ images, providing an added layer of protection against unauthorized edits,” says Salman.\nDespite PhotoGuard’s promise, it’s not a panacea. Once an image is online, individuals with malicious intent could attempt to reverse engineer the protective measures by applying noise, cropping, or rotating the image. However, there is plenty of previous work from the adversarial examples literature that can be utilized here to implement robust perturbations that resist common image manipulations.\n“A collaborative approach involving model developers, social media platforms, and policymakers presents a robust defense against unauthorized image manipulation. Working on this pressing issue is of paramount importance today,” says Salman. “And while I am glad to contribute towards this solution, much work is needed to make this protection practical. Companies that develop these models need to invest in engineering robust immunizations against the possible threats posed by these AI tools. As we tread into this new era of generative models, let’s strive for potential and protection in equal measures.”\n“The prospect of using attacks on machine learning to protect us from abusive uses of this technology is very compelling,” says Florian Tramèr, an assistant professor at ETH Zürich. “The paper has a nice insight that the developers of generative AI models have strong incentives to provide such immunization protections to their users, which could even be a legal requirement in the future. However, designing image protections that effectively resist circumvention attempts is a challenging problem: Once the generative AI company commits to an immunization mechanism and people start applying it to their online images, we need to ensure that this protection will work against motivated adversaries who might even use better generative AI models developed in the near future. Designing such robust protections is a hard open problem, and this paper makes a compelling case that generative AI companies should be working on solving it.”\nSalman wrote the paper alongside fellow lead authors Alaa Khaddaj and Guillaume Leclerc MS ’18, as well as Andrew Ilyas ’18, MEng ’18; all three are EECS graduate students and MIT CSAIL affiliates. The team’s work was partially done on the MIT Supercloud compute cluster, supported by U.S. National Science Foundation grants and Open Philanthropy, and based upon work supported by the U.S. Defense Advanced Research Projects Agency. It was presented at the International Conference on Machine Learning this July.\n",
    "published": "2023-07-31",
    "timestamp": "2023-08-30T13:26:06.931078"
  },
  {
    "unique_id": "d0ce8a20-45ad-5474-8bab-abbb78f59b4d",
    "title": "A more effective way to train machines for uncertain, real-world situations",
    "description": "Researchers develop an algorithm that decides when a “student” machine should follow its teacher, and when it should learn on its own.",
    "link": "https://news.mit.edu/2023/more-effective-train-machines-uncertain-real-world-situations-0531",
    "blog_text": "Someone learning to play tennis might hire a teacher to help them learn faster. Because this teacher is (hopefully) a great tennis player, there are times when trying to exactly mimic the teacher won’t help the student learn. Perhaps the teacher leaps high into the air to deftly return a volley. The student, unable to copy that, might instead try a few other moves on her own until she has mastered the skills she needs to return volleys.\n\nComputer scientists can also use “teacher” systems to train another machine to complete a task. But just like with human learning, the student machine faces a dilemma of knowing when to follow the teacher and when to explore on its own. To this end, researchers from MIT and Technion, the Israel Institute of Technology, have developed an algorithm that automatically and independently determines when the student should mimic the teacher (known as imitation learning) and when it should instead learn through trial and error (known as reinforcement learning).\n\nTheir dynamic approach allows the student to diverge from copying the teacher when the teacher is either too good or not good enough, but then return to following the teacher at a later point in the training process if doing so would achieve better results and faster learning.\n\nWhen the researchers tested this approach in simulations, they found that their combination of trial-and-error learning and imitation learning enabled students to learn tasks more effectively than methods that used only one type of learning.\n\nThis method could help researchers improve the training process for machines that will be deployed in uncertain real-world situations, like a robot being trained to navigate inside a building it has never seen before.\n\n“This combination of learning by trial-and-error and following a teacher is very powerful. It gives our algorithm the ability to solve very difficult tasks that cannot be solved by using either technique individually,” says Idan Shenfeld an electrical engineering and computer science (EECS) graduate student and lead author of a paper on this technique.\n\nShenfeld wrote the paper with coauthors Zhang-Wei Hong, an EECS graduate student; Aviv Tamar; assistant professor of electrical engineering and computer science at Technion; and senior author Pulkit Agrawal, director of Improbable AI Lab and an assistant professor in the Computer Science and Artificial Intelligence Laboratory. The research will be presented at the International Conference on Machine Learning.\n\nStriking a balance\n\nMany existing methods that seek to strike a balance between imitation learning and reinforcement learning do so through brute force trial-and-error. Researchers pick a weighted combination of the two learning methods, run the entire training procedure, and then repeat the process until they find the optimal balance. This is inefficient and often so computationally expensive it isn’t even feasible.\n\n“We want algorithms that are principled, involve tuning of as few knobs as possible, and achieve high performance — these principles have driven our research,” says Agrawal.\n\nTo achieve this, the team approached the problem differently than prior work. Their solution involves training two students: one with a weighted combination of reinforcement learning and imitation learning, and a second that can only use reinforcement learning to learn the same task.\n\nThe main idea is to automatically and dynamically adjust the weighting of the reinforcement and imitation learning objectives of the first student. Here is where the second student comes into play. The researchers’ algorithm continually compares the two students. If the one using the teacher is doing better, the algorithm puts more weight on imitation learning to train the student, but if the one using only trial and error is starting to get better results, it will focus more on learning from reinforcement learning.\n\nBy dynamically determining which method achieves better results, the algorithm is adaptive and can pick the best technique throughout the training process. Thanks to this innovation, it is able to more effectively teach students than other methods that aren’t adaptive, Shenfeld says.\n\n“One of the main challenges in developing this algorithm was that it took us some time to realize that we should not train the two students independently. It became clear that we needed to connect the agents to make them share information, and then find the right way to technically ground this intuition,” Shenfeld says.\n\nSolving tough problems\n\nTo test their approach, the researchers set up many simulated teacher-student training experiments, such as navigating through a maze of lava to reach the other corner of a grid. In this case, the teacher has a map of the entire grid while the student can only see a patch in front of it. Their algorithm achieved an almost perfect success rate across all testing environments, and was much faster than other methods.\n\nTo give their algorithm an even more difficult test, they set up a simulation involving a robotic hand with touch sensors but no vision, that must reorient a pen to the correct pose. The teacher had access to the actual orientation of the pen, while the student could only use touch sensors to determine the pen’s orientation.\n\nTheir method outperformed others that used either only imitation learning or only reinforcement learning.\n\nReorienting objects is one among many manipulation tasks that a future home robot would need to perform, a vision that the Improbable AI lab is working toward, Agrawal adds.\n\nTeacher-student learning has successfully been applied to train robots to perform complex object manipulation and locomotion in simulation and then transfer the learned skills into the real-world. In these methods, the teacher has privileged information accessible from the simulation that the student won’t have when it is deployed in the real world. For example, the teacher will know the detailed map of a building that the student robot is being trained to navigate using only images captured by its camera.\n\n“Current methods for student-teacher learning in robotics don’t account for the inability of the student to mimic the teacher and thus are performance-limited. The new method paves a path for building superior robots,” says Agrawal.\n\nApart from better robots, the researchers believe their algorithm has the potential to improve performance in diverse applications where imitation or reinforcement learning is being used. For example, large language models such as GPT-4 are very good at accomplishing a wide range of tasks, so perhaps one could use the large model as a teacher to train a smaller, student model to be even “better” at one particular task. Another exciting direction is to investigate the similarities and differences between machines and humans learning from their respective teachers. Such analysis might help improve the learning experience, the researchers say.\n\n“What's interesting about this approach compared to related methods is how robust it seems to various parameter choices, and the variety of domains it shows promising results in,” says Abhishek Gupta, an assistant professor at the University of Washington, who was not involved with this work. “While the current set of results are largely in simulation, I am very excited about the future possibilities of applying this work to problems involving memory and reasoning with different modalities such as tactile sensing.” \n\n“This work presents an interesting approach to reuse prior computational work in reinforcement learning. Particularly, their proposed method can leverage suboptimal teacher policies as a guide while avoiding careful hyperparameter schedules required by prior methods for balancing the objectives of mimicking the teacher versus optimizing the task reward,” adds Rishabh Agarwal, a senior research scientist at Google Brain, who was also not involved in this research. “Hopefully, this work would make reincarnating reinforcement learning with learned policies less cumbersome.”\nThis research was supported, in part, by the MIT-IBM Watson AI Lab, Hyundai Motor Company, the DARPA Machine Common Sense Program, and the Office of Naval Research.\n",
    "published": "2023-05-31",
    "timestamp": "2023-08-30T13:26:07.026279"
  },
  {
    "unique_id": "d80dd34d-0eba-5f56-b66d-d945fc6a7ed1",
    "title": "Researchers teach an AI to write better chart captions",
    "description": "A new dataset can help scientists develop automatic systems that generate richer, more descriptive captions for online charts.",
    "link": "https://news.mit.edu/2023/researchers-chart-captions-ai-vistext-0630",
    "blog_text": "Chart captions that explain complex trends and patterns are important for improving a reader’s ability to comprehend and retain the data being presented. And for people with visual disabilities, the information in a caption often provides their only means of understanding the chart.\nBut writing effective, detailed captions is a labor-intensive process. While autocaptioning techniques can alleviate this burden, they often struggle to describe cognitive features that provide additional context.\nTo help people author high-quality chart captions, MIT researchers have developed a dataset to improve automatic captioning systems. Using this tool, researchers could teach a machine-learning model to vary the level of complexity and type of content included in a chart caption based on the needs of users.\nThe MIT researchers found that machine-learning models trained for autocaptioning with their dataset consistently generated captions that were precise, semantically rich, and described data trends and complex patterns. Quantitative and qualitative analyses revealed that their models captioned charts more effectively than other autocaptioning systems.  \nThe team’s goal is to provide the dataset, called VisText, as a tool researchers can use as they work on the thorny problem of chart autocaptioning. These automatic systems could help provide captions for uncaptioned online charts and improve accessibility for people with visual disabilities, says co-lead author Angie Boggust, a graduate student in electrical engineering and computer science at MIT and member of the Visualization Group in the Computer Science and Artificial Intelligence Laboratory (CSAIL).\n“We’ve tried to embed a lot of human values into our dataset so that when we and other researchers are building automatic chart-captioning systems, we don’t end up with models that aren’t what people want or need,” she says.\nBoggust is joined on the paper by co-lead author and fellow graduate student Benny J. Tang and senior author Arvind Satyanarayan, associate professor of computer science at MIT who leads the Visualization Group in CSAIL. The research will be presented at the Annual Meeting of the Association for Computational Linguistics.\nHuman-centered analysis\nThe researchers were inspired to develop VisText from prior work in the Visualization Group that explored what makes a good chart caption. In that study, researchers found that sighted users and blind or low-vision users had different preferences for the complexity of semantic content in a caption. \nThe group wanted to bring that human-centered analysis into autocaptioning research. To do that, they developed VisText, a dataset of charts and associated captions that could be used to train machine-learning models to generate accurate, semantically rich, customizable captions.\nDeveloping effective autocaptioning systems is no easy task. Existing machine-learning methods often try to caption charts the way they would an image, but people and models interpret natural images differently from how we read charts. Other techniques skip the visual content entirely and caption a chart using its underlying data table. However, such data tables are often not available after charts are published.\nGiven the shortfalls of using images and data tables, VisText also represents charts as scene graphs. Scene graphs, which can be extracted from a chart image, contain all the chart data but also include additional image context.\n“A scene graph is like the best of both worlds — it contains almost all the information present in an image while being easier to extract from images than data tables. As it’s also text, we can leverage advances in modern large language models for captioning,” Tang explains.\nThey compiled a dataset that contains more than 12,000 charts — each represented as a data table, image, and scene graph — as well as associated captions. Each chart has two separate captions: a low-level caption that describes the chart’s construction (like its axis ranges) and a higher-level caption that describes statistics, relationships in the data, and complex trends.\nThe researchers generated low-level captions using an automated system and crowdsourced higher-level captions from human workers.\n“Our captions were informed by two key pieces of prior research: existing guidelines on accessible descriptions of visual media and a conceptual model from our group for categorizing semantic content. This ensured that our captions featured important low-level chart elements like axes, scales, and units for readers with visual disabilities, while retaining human variability in how captions can be written,” says Tang.\nTranslating charts\nOnce they had gathered chart images and captions, the researchers used VisText to train five machine-learning models for autocaptioning. They wanted to see how each representation — image, data table, and scene graph — and combinations of the representations affected the quality of the caption.\n“You can think about a chart captioning model like a model for language translation. But instead of saying, translate this German text to English, we are saying translate this ‘chart language’ to English,” Boggust says.\nTheir results showed that models trained with scene graphs performed as well or better than those trained using data tables. Since scene graphs are easier to extract from existing charts, the researchers argue that they might be a more useful representation.\nThey also trained models with low-level and high-level captions separately. This technique, known as semantic prefix tuning, enabled them to teach the model to vary the complexity of the caption’s content.\nIn addition, they conducted a qualitative examination of captions produced by their best-performing method and categorized six types of common errors. For instance, a directional error occurs if a model says a trend is decreasing when it is actually increasing.\nThis fine-grained, robust qualitative evaluation was important for understanding how the model was making its errors. For example, using quantitative methods, a directional error might incur the same penalty as a repetition error, where the model repeats the same word or phrase. But a directional error could be more misleading to a user than a repetition error. The qualitative analysis helped them understand these types of subtleties, Boggust says.\nThese sorts of errors also expose limitations of current models and raise ethical considerations that researchers must consider as they work to develop autocaptioning systems, she adds.\nGenerative machine-learning models, such as those that power ChatGPT, have been shown to hallucinate or give incorrect information that can be misleading. While there is a clear benefit to using these models for autocaptioning existing charts, it could lead to the spread of misinformation if charts are captioned incorrectly.\n“Maybe this means that we don’t just caption everything in sight with AI. Instead, perhaps we provide these autocaptioning systems as authorship tools for people to edit. It is important to think about these ethical implications throughout the research process, not just at the end when we have a model to deploy,” she says.\nBoggust, Tang, and their colleagues want to continue optimizing the models to reduce some common errors. They also want to expand the VisText dataset to include more charts, and more complex charts, such as those with stacked bars or multiple lines. And they would also like to gain insights into what these autocaptioning models are actually learning about chart data.\nThis research was supported, in part, by a Google Research Scholar Award, the National Science Foundation, the MLA@CSAIL Initiative, and the United States Air Force Research Laboratory.\n",
    "published": "2023-06-30",
    "timestamp": "2023-08-30T13:26:06.974337"
  },
  {
    "unique_id": "e7a612ec-0088-5be9-ac17-eb7b75c3595e",
    "title": "Day of AI curriculum meets the moment",
    "description": "Global participation in MIT RAISE’s free K-12 program more than doubles in its second year.",
    "link": "https://news.mit.edu/2023/day-ai-curriculum-meets-moment-0626",
    "blog_text": "MIT Responsible AI for Social Empowerment and Education (RAISE) recently celebrated the second annual Day of AI with two flagship local events. The Edward M. Kennedy Institute for the U.S. Senate in Boston hosted a human rights and data policy-focused event that was streamed worldwide. Dearborn STEM Academy in Roxbury, Massachusetts, hosted a student workshop in collaboration with Amazon Future Engineer. With over 8,000 registrations across all 50 U.S. states and 108 countries in 2023, participation in Day of AI has more than doubled since its inaugural year.\nDay of AI is a free curriculum of lessons and hands-on activities designed to teach kids of all ages and backgrounds the basics and responsible use of artificial intelligence, designed by researchers at MIT RAISE. This year, resources were available for educators to run at any time and in any increments they chose. The curriculum included five new modules to address timely topics like ChatGPT in School, Teachable Machines, AI and Social Media, Data Science and Me, and more. A collaboration with the International Society for Technology in Education also introduced modules for early elementary students. Educators across the world shared photos, videos, and stories of their students’ engagement, expressing excitement and even relief over the accessible lessons.\nProfessor Cynthia Breazeal, director of RAISE, dean for digital learning at MIT, and head of the MIT Media Lab’s Personal Robots research group, said, “It’s been a year of extraordinary advancements in AI, and with that comes necessary conversations and concerns about who and what this technology is for. With our Day of AI events, we want to celebrate the teachers and students who are putting in the work to make sure that AI is for everyone.”\nReflecting community values and protecting digital citizens\nMIT President Sally Kornbluth welcomed students from Warren Prescott Middle School and New Mission High School to the Day of AI program at the Edward M. Kennedy Institute for the United States Senate. Kornbluth reflected on the exciting potential of AI, along with the ethical considerations society needs to be responsible for.\n“AI has the potential to do all kinds of fantastic things, including driving a car, helping us with the climate crisis, improving health care, and designing apps that we can’t even imagine yet. But what we have to make sure it doesn’t do is cause harm to individuals, to communities, to us — society as a whole,” she said.\nThis theme resonated with each of the event speakers, whose jobs spanned the sectors of education, government, and business. Yo Deshpande, technologist for the public realm, and Michael Lawrence Evans, program director of new urban mechanics from the Boston Mayor’s Office, shared how Boston thinks about using AI to improve city life in ways that are “equitable, accessible, and delightful.” Deshpande said, “We have the opportunity to explore not only how AI works, but how using AI can line up with our values, the way we want to be in the world, and the way we want to be in our community.”\nAdam L’Italien, chief innovation officer at Liberty Mutual Insurance (one of Day of AI’s founding sponsors), compared our present moment with AI technologies to the early days of personal computers and internet connection. “Exposure to emerging technologies can accelerate progress in the world and in your own lives,” L’Italien said, while recognizing that the AI development process needs to be inclusive and mitigate biases.\nHuman policies for artificial intelligence\nSo how does society address these human rights concerns about AI? Marc Aidinoff ’21, former White House Office of Science and Technology Policy chief of staff, led a discussion on how government policy can influence the parameters of how technology is developed and used, like the Blueprint for an AI Bill of Rights. Aidinoff said, “The work of building the world you want to see is far harder than building the technical AI system … How do you work with other people and create a collective vision for what we want to do?” Warren Prescott Middle School students described how AI could be used to solve problems that humans couldn’t. But they also shared their concerns that AI could affect data privacy, learning deficits, social media addiction, job displacement, and propaganda.\nIn a mock U.S. Senate trial activity designed by Daniella DiPaola, PhD student at the MIT Media Lab, the middle schoolers investigated what rights might be undermined by AI in schools, hospitals, law enforcement, and corporations. Meanwhile, New Mission High School students workshopped the ideas behind bill S.2314, the Social Media Addiction Reduction Technology (SMART) Act, in an activity designed by Raechel Walker, graduate research assistant in the Personal Robots Group, and Matt Taylor, research assistant at the Media Lab. They discussed what level of control could or should be introduced at the parental, educational, and governmental levels to reduce the risks of internet addiction.\n“Alexa, how do I program AI?”\nAt Dearborn STEM Academy, Amazon Future Engineer helped students work through the Intro to Voice AI curriculum module in real-time. Students used MIT App Inventor to code basic commands for Alexa. In an interview with WCVB, Principal Darlene Marcano said, “It’s important that we expose our students to as many different experiences as possible. The students that are participating are on track to be future computer scientists and engineers.”\nBreazeal told Dearborn students, “We want you to have an informed voice about how you want AI to be used in society. We want you to feel empowered that you can shape the world. You can make things with AI to help make a better world and a better community.”\nRohit Prasad ’08, senior vice president and head scientist for Alexa at Amazon, and Victor Reinoso ’97, global director of philanthropic education initiatives at Amazon, also joined the event. “Amazon and MIT share a commitment to helping students discover a world of possibilities through STEM and AI education,” said Reinoso. \"There’s a lot of current excitement around the technological revolution with generative AI and large language models, so we’re excited to help students explore careers of the future and navigate the pathways available to them.” To highlight their continued investment in the local community and the school program, Amazon donated a $25,000 Innovation and Early College Pathways Program Grant to the Boston Public School system.\nDay of AI down under\nNot only was the Day of AI program widely adopted across the globe, Australian educators were inspired to adapt their own regionally specific curriculum. An estimated 161,000 AI professionals will be needed in Australia by 2030, according to the National Artificial Intelligence Center in the Commonwealth Scientific and Industrial Research Organization (CSIRO), an Australian government agency and Day of AI Australia project partner. CSIRO worked with the University of New South Wales to develop supplementary educational resources on AI ethics and machine learning. Day of AI Australia reached 85,000 students at 400-plus secondary schools this year, sparking curiosity in the next generation of AI experts.\nThe interest in AI is accelerating as fast as the technology is being developed. Day of AI offers a unique opportunity for K-12 students to shape our world’s digital future and their own.\n“I hope that some of you will decide to be part of this bigger effort to help us figure out the best possible answers to questions that are raised by AI,” Kornbluth told students at the Edward M. Kennedy Institute. “We’re counting on you, the next generation, to learn how AI works and help make sure it’s for everyone.”\n",
    "published": "2023-06-26",
    "timestamp": "2023-08-30T13:26:06.982921"
  },
  {
    "unique_id": "e871e154-b34a-5c49-85aa-88729729054f",
    "title": "AI models are powerful, but are they biologically plausible?",
    "description": "A new study bridging neuroscience and machine learning offers insights into the potential role of astrocytes in the human brain.",
    "link": "https://news.mit.edu/2023/ai-models-astrocytes-role-brain-0815",
    "blog_text": "Artificial neural networks, ubiquitous machine-learning models that can be trained to complete many tasks, are so called because their architecture is inspired by the way biological neurons process information in the human brain.\nAbout six years ago, scientists discovered a new type of more powerful neural network model known as a transformer. These models can achieve unprecedented performance, such as by generating text from prompts with near-human-like accuracy. A transformer underlies AI systems such as ChatGPT and Bard, for example. While incredibly effective, transformers are also mysterious: Unlike with other brain-inspired neural network models, it hasn’t been clear how to build them using biological components.\nNow, researchers from MIT, the MIT-IBM Watson AI Lab, and Harvard Medical School have produced a hypothesis that may explain how a transformer could be built using biological elements in the brain. They suggest that a biological network composed of neurons and other brain cells called astrocytes could perform the same core computation as a transformer.\nRecent research has shown that astrocytes, non-neuronal cells that are abundant in the brain, communicate with neurons and play a role in some physiological processes, like regulating blood flow. But scientists still lack a clear understanding of what these cells do computationally.\nWith the new study, published this week in open-access format in the Proceedings of the National Academy of Sciences, the researchers explored the role astrocytes play in the brain from a computational perspective, and crafted a mathematical model that shows how they could be used, along with neurons, to build a biologically plausible transformer.\nTheir hypothesis provides insights that could spark future neuroscience research into how the human brain works. At the same time, it could help machine-learning researchers explain why transformers are so successful across a diverse set of complex tasks.\n“The brain is far superior to even the best artificial neural networks that we have developed, but we don’t really know exactly how the brain works. There is scientific value in thinking about connections between biological hardware and large-scale artificial intelligence networks. This is neuroscience for AI and AI for neuroscience,” says Dmitry Krotov, a research staff member at the MIT-IBM Watson AI Lab and senior author of the research paper.\nJoining Krotov on the paper are lead author Leo Kozachkov, a postdoc in the MIT Department of Brain and Cognitive Sciences; and Ksenia V. Kastanenka, an assistant professor of neurobiology at Harvard Medical School and an assistant investigator at the Massachusetts General Research Institute.  \nA biological impossibility becomes plausible\nTransformers operate differently than other neural network models. For instance, a recurrent neural network trained for natural language processing would compare each word in a sentence to an internal state determined by the previous words. A transformer, on the other hand, compares all the words in the sentence at once to generate a prediction, a process called self-attention.\nFor self-attention to work, the transformer must keep all the words ready in some form of memory, Krotov explains, but this didn’t seem biologically possible due to the way neurons communicate.\nHowever, a few years ago scientists studying a slightly different type of machine-learning model (known as a Dense Associated Memory) realized that this self-attention mechanism could occur in the brain, but only if there were communication between at least three neurons.\n“The number three really popped out to me because it is known in neuroscience that these cells called astrocytes, which are not neurons, form three-way connections with neurons, what are called tripartite synapses,” Kozachkov says.\nWhen two neurons communicate, a presynaptic neuron sends chemicals called neurotransmitters across the synapse that connects it to a postsynaptic neuron. Sometimes, an astrocyte is also connected — it wraps a long, thin tentacle around the synapse, creating a tripartite (three-part) synapse. One astrocyte may form millions of tripartite synapses.\nThe astrocyte collects some neurotransmitters that flow through the synaptic junction. At some point, the astrocyte can signal back to the neurons. Because astrocytes operate on a much longer time scale than neurons — they create signals by slowly elevating their calcium response and then decreasing it — these cells can hold and integrate information communicated to them from neurons. In this way, astrocytes can form a type of memory buffer, Krotov says.\n“If you think about it from that perspective, then astrocytes are extremely natural for precisely the computation we need to perform the attention operation inside transformers,” he adds.\nBuilding a neuron-astrocyte network\nWith this insight, the researchers formed their hypothesis that astrocytes could play a role in how transformers compute. Then they set out to build a mathematical model of a neuron-astrocyte network that would operate like a transformer.\nThey took the core mathematics that comprise a transformer and developed simple biophysical models of what astrocytes and neurons do when they communicate in the brain, based on a deep dive into the literature and guidance from neuroscientist collaborators.\nThen they combined the models in certain ways until they arrived at an equation of a neuron-astrocyte network that describes a transformer’s self-attention.\n“Sometimes, we found that certain things we wanted to be true couldn’t be plausibly implemented. So, we had to think of workarounds. There are some things in the paper that are very careful approximations of the transformer architecture to be able to match it in a biologically plausible way,” Kozachkov says.\nThrough their analysis, the researchers showed that their biophysical neuron-astrocyte network theoretically matches a transformer. In addition, they conducted numerical simulations by feeding images and paragraphs of text to transformer models and comparing the responses to those of their simulated neuron-astrocyte network. Both responded to the prompts in similar ways, confirming their theoretical model.\n“Having remained electrically silent for over a century of brain recordings, astrocytes are one of the most abundant, yet less explored, cells in the brain. The potential of unleashing the computational power of the other half of our brain is enormous,” says Konstantinos Michmizos, associate professor of computer science at Rutgers University, who was not involved with this work. “This study opens up a fascinating iterative loop, from understanding how intelligent behavior may truly emerge in the brain, to translating disruptive hypotheses into new tools that exhibit human-like intelligence.”\nThe next step for the researchers is to make the leap from theory to practice. They hope to compare the model’s predictions to those that have been observed in biological experiments, and use this knowledge to refine, or possibly disprove, their hypothesis.\nIn addition, one implication of their study is that astrocytes may be involved in long-term memory, since the network needs to store information to be able act on it in the future. Additional research could investigate this idea further, Krotov says.\n“For a lot of reasons, astrocytes are extremely important for cognition and behavior, and they operate in fundamentally different ways from neurons. My biggest hope for this paper is that it catalyzes a bunch of research in computational neuroscience toward glial cells, and in particular, astrocytes,” adds Kozachkov.\nThis research was supported, in part, by the BrightFocus Foundation and the National Institute of Health.\n",
    "published": "2023-08-15",
    "timestamp": "2023-08-30T13:26:06.924037"
  },
  {
    "unique_id": "ecd4efda-8dc8-53ed-8c69-95b83fd7c568",
    "title": "David Autor named NOMIS 2023 Distinguished Scientist",
    "description": "NOMIS Foundation honors the Ford Professor of Economics for his contributions to understanding the effects of technological change and globalization on jobs and earnings prospects for workers.",
    "link": "https://news.mit.edu/2023/david-autor-named-nomis-distinguished-scientist-0614",
    "blog_text": "David H. Autor, Ford Professor of Economics at MIT, has been recognized as one of two 2023 NOMIS Distinguished Scientists for his significant contributions and ongoing research work to understand the effects of technological change and globalization on jobs and earnings prospects for workers. Anne Brunet of Stanford University is the other winner for this year.\nThe NOMIS Distinguished Scientist and Scholar Award is presented by the NOMIS Foundation to researchers who, through their innovative, groundbreaking research, have made a significant contribution to their respective fields, and who inspire the world around them.\nIn addition to co-directing the MIT Shaping the Future of Work Initiative, MIT Blueprint Labs, and the National Bureau of Economic Research's Labor Studies Program, Autor will lead the NOMIS Foundation's Expertise project, which seeks to answer a looming question about the impact of new technologies like generative AI: “Will new technologies complement or commodify expertise?”\nJoshua Angrist, Ford Professor of Economics at MIT, recipient of the 2021 Nobel Prize in Economics, and co-founder and director of MIT Blueprint Labs, responded to the announcement of the award with praise for Autor’s ongoing research: “David’s labor market research has produced unparalleled insights into how our world works. His careful scholarship documents the consequences of major new forces, such as the China trade shock, and slowly evolving trends, like automation. He melds careful theoretical reasoning and state-of-the-art causal inference tools with unmatched skill to address big economic questions. David’s answers, always explained with grace and humor, reassure those inclined to panic in the face of the unknown, while also helping us all prepare for a changed future. It’s a privilege to be his colleague, coauthor, and friend.”\n“This is a moment of celebration for the very well-deserved recognition of David Autor's enormous contributions to labor economics and our understanding of some of the most important trends of the age,” says Daron Acemolgu, MIT Institute professor and Autor’s fellow faculty co-director of the MIT Shaping the Future of Work Initiative. “It is not an exaggeration to say that David has revolutionized our thinking on not just one, but several topics, including the effects of imports from China, the inequality and job implications of computers and automation, and the consequences of new human resource practices and work arrangements. David has been more than a trailblazing researcher. He is also a dedicated advisor and an amazing, generous colleague. The entire labor economics profession, and especially our community at MIT, is fortunate to have had an opportunity to interact with him and learn from him. I am personally looking forward to doing so for many more years.”\nThe 2023 NOMIS Awards will be presented in Zurich, Switzerland, this October. \n",
    "published": "2023-06-14",
    "timestamp": "2023-08-30T13:26:06.995674"
  },
  {
    "unique_id": "edb1375e-6042-5269-a2c8-ec6005731bcf",
    "title": "AI model can help determine where a patient’s cancer arose",
    "description": "Predictions from the OncoNPC model could enable doctors to choose targeted treatments for difficult-to-treat tumors.",
    "link": "https://news.mit.edu/2023/ai-model-can-help-determine-where-patients-cancer-arose-0807",
    "blog_text": "For a small percentage of cancer patients, doctors are unable to determine where their cancer originated. This makes it much more difficult to choose a treatment for those patients, because many cancer drugs are typically developed for specific cancer types.\nA new approach developed by researchers at MIT and Dana-Farber Cancer Institute may make it easier to identify the sites of origin for those enigmatic cancers. Using machine learning, the researchers created a computational model that can analyze the sequence of about 400 genes and use that information to predict where a given tumor originated in the body.\nUsing this model, the researchers showed that they could accurately classify at least 40 percent of tumors of unknown origin with high confidence, in a dataset of about 900 patients. This approach enabled a 2.2-fold increase in the number of patients who could have been eligible for a genomically guided, targeted treatment, based on where their cancer originated.\n“That was the most important finding in our paper, that this model could be potentially used to aid treatment decisions, guiding doctors toward personalized treatments for patients with cancers of unknown primary origin,” says Intae Moon, an MIT graduate student in electrical engineering and computer science who is the lead author of the new study.\nAlexander Gusev, an associate professor of medicine at Harvard Medical School and Dana-Farber Cancer Institute, is the senior author of the paper, which appears today in Nature Medicine.\nMysterious origins\nIn 3 to 5 percent of cancer patients, particularly in cases where tumors have metastasized throughout the body, oncologists don’t have an easy way to determine where the cancer originated. These tumors are classified as cancers of unknown primary (CUP).\nThis lack of knowledge often prevents doctors from being able to give patients “precision” drugs, which are typically approved for specific cancer types where they are known to work. These targeted treatments tend to be more effective and have fewer side effects than treatments that are used for a broad spectrum of cancers, which are commonly prescribed to CUP patients.\n“A sizeable number of individuals develop these cancers of unknown primary every year, and because most therapies are approved in a site-specific way, where you have to know the primary site to deploy them, they have very limited treatment options,” Gusev says.\nMoon, an affiliate of the Computer Science and Artificial Intelligence Laboratory who is co-advised by Gusev, decided to analyze genetic data that is routinely collected at Dana-Farber to see if it could be used to predict cancer type. The data consist of genetic sequences for about 400 genes that are often mutated in cancer. The researchers trained a machine-learning model on data from nearly 30,000 patients who had been diagnosed with one of 22 known cancer types. That set of data included patients from Memorial Sloan Kettering Cancer Center and Vanderbilt-Ingram Cancer Center, as well as Dana-Farber.\nThe researchers then tested the resulting model on about 7,000 tumors that it hadn’t seen before, but whose site of origin was known. The model, which the researchers named OncoNPC, was able to predict their origins with about 80 percent accuracy. For tumors with high-confidence predictions, which constituted about 65 percent of the total, its accuracy rose to roughly 95 percent.\nAfter those encouraging results, the researchers used the model to analyze a set of about 900 tumors from patients with CUP, which were all from Dana-Farber. They found that for 40 percent of these tumors, the model was able to make high-confidence predictions.\nThe researchers then compared the model’s predictions with an analysis of the germline, or inherited, mutations in a subset of tumors with available data, which can reveal whether the patients have a genetic predisposition to develop a particular type of cancer. The researchers found that the model’s predictions were much more likely to match the type of cancer most strongly predicted by the germline mutations than any other type of cancer.\nGuiding drug decisions \nTo further validate the model’s predictions, the researchers compared data on the CUP patients’ survival time with the typical prognosis for the type of cancer that the model predicted. They found that CUP patients who were predicted to have cancer with a poor prognosis, such as pancreatic cancer, showed correspondingly shorter survival times. Meanwhile, CUP patients who were predicted to have cancers that typically have better prognoses, such as neuroendocrine tumors, had longer survival times.\nAnother indication that the model’s predictions could be useful came from looking at the types of treatments that CUP patients analyzed in the study had received. About 10 percent of these patients had received a targeted treatment, based on their oncologists’ best guess about where their cancer had originated. Among those patients, those who received a treatment consistent with the type of cancer that the model predicted for them fared better than patients who received a treatment typically given for a different type of cancer than what the model predicted for them.\nUsing this model, the researchers also identified an additional 15 percent of patients (2.2-fold increase) who could have received an existing targeted treatment, if their cancer type had been known. Instead, those patients ended up receiving more general chemotherapy drugs.\n“That potentially makes these findings more clinically actionable because we’re not requiring a new drug to be approved. What we’re saying is that this population can now be eligible for precision treatments that already exist,” Gusev says.\nThe researchers now hope to expand their model to include other types of data, such as pathology images and radiology images, to provide a more comprehensive prediction using multiple data modalities. This would also provide the model with a comprehensive perspective of tumors, enabling it to predict not just the type of tumor and patient outcome, but potentially even the optimal treatment.\nThe research was funded by the National Institutes of Health, the Louis B. Mayer Foundation, the Doris Duke Charitable Foundation, the Phi Beta Psi Sorority, and the Emerson Collective.\n",
    "published": "2023-08-07",
    "timestamp": "2023-08-30T13:26:06.926837"
  },
  {
    "unique_id": "eed8df01-052f-5157-8a7f-499eacd4b58b",
    "title": "AI helps household robots cut planning time in half",
    "description": "PIGINet leverages machine learning to streamline and enhance household robots' task and motion planning, by assessing and filtering feasible solutions in complex environments.",
    "link": "https://news.mit.edu/2023/ai-helps-household-robots-cut-planning-time-half-0714",
    "blog_text": "Your brand new household robot is delivered to your house, and you ask it to make you a cup of coffee. Although it knows some basic skills from previous practice in simulated kitchens, there are way too many actions it could possibly take — turning on the faucet, flushing the toilet, emptying out the flour container, and so on. But there’s a tiny number of actions that could possibly be useful. How is the robot to figure out what steps are sensible in a new situation?\nIt could use PIGINet, a new system that aims to efficiently enhance the problem-solving capabilities of household robots. Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) are using machine learning to cut down on the typical iterative process of task planning that considers all possible actions. PIGINet eliminates task plans that can’t satisfy collision-free requirements, and reduces planning time by 50-80 percent when trained on only 300-500 problems. \nTypically, robots attempt various task plans and iteratively refine their moves until they find a feasible solution, which can be inefficient and time-consuming, especially when there are movable and articulated obstacles. Maybe after cooking, for example, you want to put all the sauces in the cabinet. That problem might take two to eight steps depending on what the world looks like at that moment. Does the robot need to open multiple cabinet doors, or are there any obstacles inside the cabinet that need to be relocated in order to make space? You don’t want your robot to be annoyingly slow — and it will be worse if it burns dinner while it’s thinking.\nHousehold robots are usually thought of as following predefined recipes for performing tasks, which isn’t always suitable for diverse or changing environments. So, how does PIGINet avoid those predefined rules? PIGINet is a neural network that takes in “Plans, Images, Goal, and Initial facts,” then predicts the probability that a task plan can be refined to find feasible motion plans. In simple terms, it employs a transformer encoder, a versatile and state-of-the-art model designed to operate on data sequences. The input sequence, in this case, is information about which task plan it is considering, images of the environment, and symbolic encodings of the initial state and the desired goal. The encoder combines the task plans, image, and text to generate a prediction regarding the feasibility of the selected task plan. \nKeeping things in the kitchen, the team created hundreds of simulated environments, each with different layouts and specific tasks that require objects to be rearranged among counters, fridges, cabinets, sinks, and cooking pots. By measuring the time taken to solve problems, they compared PIGINet against prior approaches. One correct task plan may include opening the left fridge door, removing a pot lid, moving the cabbage from pot to fridge, moving a potato to the fridge, picking up the bottle from the sink, placing the bottle in the sink, picking up the tomato, or placing the tomato. PIGINet significantly reduced planning time by 80 percent in simpler scenarios and 20-50 percent in more complex scenarios that have longer plan sequences and less training data.\n“Systems such as PIGINet, which use the power of data-driven methods to handle familiar cases efficiently, but can still fall back on “first-principles” planning methods to verify learning-based suggestions and solve novel problems, offer the best of both worlds, providing reliable and efficient general-purpose solutions to a wide variety of problems,” says MIT Professor and CSAIL Principal Investigator Leslie Pack Kaelbling.\n\nPIGINet's use of multimodal embeddings in the input sequence allowed for better representation and understanding of complex geometric relationships. Using image data helped the model to grasp spatial arrangements and object configurations without knowing the object 3D meshes for precise collision checking, enabling fast decision-making in different environments. \nOne of the major challenges faced during the development of PIGINet was the scarcity of good training data, as all feasible and infeasible plans need to be generated by traditional planners, which is slow in the first place. However, by using pretrained vision language models and data augmentation tricks, the team was able to address this challenge, showing impressive plan time reduction not only on problems with seen objects, but also zero-shot generalization to previously unseen objects.\n“Because everyone’s home is different, robots should be adaptable problem-solvers instead of just recipe followers. Our key idea is to let a general-purpose task planner generate candidate task plans and use a deep learning model to select the promising ones. The result is a more efficient, adaptable, and practical household robot, one that can nimbly navigate even complex and dynamic environments. Moreover, the practical applications of PIGINet are not confined to households,” says Zhutian Yang, MIT CSAIL PhD student and lead author on the work. “Our future aim is to further refine PIGINet to suggest alternate task plans after identifying infeasible actions, which will further speed up the generation of feasible task plans without the need of big datasets for training a general-purpose planner from scratch. We believe that this could revolutionize the way robots are trained during development and then applied to everyone’s homes.” \n“This paper addresses the fundamental challenge in implementing a general-purpose robot: how to learn from past experience to speed up the decision-making process in unstructured environments filled with a large number of articulated and movable obstacles,” says Beomjoon Kim PhD ’20, assistant professor in the Graduate School of AI at Korea Advanced Institute of Science and Technology (KAIST). “The core bottleneck in such problems is how to determine a high-level task plan such that there exists a low-level motion plan that realizes the high-level plan. Typically, you have to oscillate between motion and task planning, which causes significant computational inefficiency. Zhutian's work tackles this by using learning to eliminate infeasible task plans, and is a step in a promising direction.”\nYang wrote the paper with NVIDIA research scientist Caelan Garrett SB ’15, MEng ’15, PhD ’21; MIT Department of Electrical Engineering and Computer Science professors and CSAIL members Tomás Lozano-Pérez and Leslie Kaelbling; and Senior Director of Robotics Research at NVIDIA and University of Washington Professor Dieter Fox. The team was supported by AI Singapore and grants from National Science Foundation, the Air Force Office of Scientific Research, and the Army Research Office. This project was partially conducted while Yang was an intern at NVIDIA Research. Their research will be presented in July at the conference Robotics: Science and Systems.\n",
    "published": "2023-07-14",
    "timestamp": "2023-08-30T13:26:06.945165"
  },
  {
    "unique_id": "f642d2f1-aa7e-578e-9f9a-e0493ad2cb72",
    "title": "Learning the language of molecules to predict their properties",
    "description": "This AI system only needs a small amount of data to predict molecular properties, which could speed up drug discovery and material development.",
    "link": "https://news.mit.edu/2023/learning-language-molecules-predict-properties-0707",
    "blog_text": "Discovering new materials and drugs typically involves a manual, trial-and-error process that can take decades and cost millions of dollars. To streamline this process, scientists often use machine learning to predict molecular properties and narrow down the molecules they need to synthesize and test in the lab.\nResearchers from MIT and the MIT-IBM Watson AI Lab have developed a new, unified framework that can simultaneously predict molecular properties and generate new molecules much more efficiently than these popular deep-learning approaches.\nTo teach a machine-learning model to predict a molecule’s biological or mechanical properties, researchers must show it millions of labeled molecular structures — a process known as training. Due to the expense of discovering molecules and the challenges of hand-labeling millions of structures, large training datasets are often hard to come by, which limits the effectiveness of machine-learning approaches.\nBy contrast, the system created by the MIT researchers can effectively predict molecular properties using only a small amount of data. Their system has an underlying understanding of the rules that dictate how building blocks combine to produce valid molecules. These rules capture the similarities between molecular structures, which helps the system generate new molecules and predict their properties in a data-efficient manner.\nThis method outperformed other machine-learning approaches on both small and large datasets, and was able to accurately predict molecular properties and generate viable molecules when given a dataset with fewer than 100 samples.\n“Our goal with this project is to use some data-driven methods to speed up the discovery of new molecules, so you can train a model to do the prediction without all of these cost-heavy experiments,” says lead author Minghao Guo, a computer science and electrical engineering (EECS) graduate student.\nGuo’s co-authors include MIT-IBM Watson AI Lab research staff members Veronika Thost, Payel Das, and Jie Chen; recent MIT graduates Samuel Song ’23 and Adithya Balachandran ’23; and senior author Wojciech Matusik, a professor of electrical engineering and computer science and a member of the MIT-IBM Watson AI Lab, who leads the Computational Design and Fabrication Group within the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). The research will be presented at the International Conference for Machine Learning.\nLearning the language of molecules\nTo achieve the best results with machine-learning models, scientists need training datasets with millions of molecules that have similar properties to those they hope to discover. In reality, these domain-specific datasets are usually very small. So, researchers use models that have been pretrained on large datasets of general molecules, which they apply to a much smaller, targeted dataset. However, because these models haven’t acquired much domain-specific knowledge, they tend to perform poorly.\nThe MIT team took a different approach. They created a machine-learning system that automatically learns the “language” of molecules — what is known as a molecular grammar — using only a small, domain-specific dataset. It uses this grammar to construct viable molecules and predict their properties.\nIn language theory, one generates words, sentences, or paragraphs based on a set of grammar rules. You can think of a molecular grammar the same way. It is a set of production rules that dictate how to generate molecules or polymers by combining atoms and substructures.\nJust like a language grammar, which can generate a plethora of sentences using the same rules, one molecular grammar can represent a vast number of molecules. Molecules with similar structures use the same grammar production rules, and the system learns to understand these similarities.\nSince structurally similar molecules often have similar properties, the system uses its underlying knowledge of molecular similarity to predict properties of new molecules more efficiently. \n“Once we have this grammar as a representation for all the different molecules, we can use it to boost the process of property prediction,” Guo says.\nThe system learns the production rules for a molecular grammar using reinforcement learning — a trial-and-error process where the model is rewarded for behavior that gets it closer to achieving a goal.\nBut because there could be billions of ways to combine atoms and substructures, the process to learn grammar production rules would be too computationally expensive for anything but the tiniest dataset.\nThe researchers decoupled the molecular grammar into two parts. The first part, called a metagrammar, is a general, widely applicable grammar they design manually and give the system at the outset. Then it only needs to learn a much smaller, molecule-specific grammar from the domain dataset. This hierarchical approach speeds up the learning process.\nBig results, small datasets\nIn experiments, the researchers’ new system simultaneously generated viable molecules and polymers, and predicted their properties more accurately than several popular machine-learning approaches, even when the domain-specific datasets had only a few hundred samples. Some other methods also required a costly pretraining step that the new system avoids.\nThe technique was especially effective at predicting physical properties of polymers, such as the glass transition temperature, which is the temperature required for a material to transition from solid to liquid. Obtaining this information manually is often extremely costly because the experiments require extremely high temperatures and pressures.\nTo push their approach further, the researchers cut one training set down by more than half — to just 94 samples. Their model still achieved results that were on par with methods trained using the entire dataset.\n“This grammar-based representation is very powerful. And because the grammar itself is a very general representation, it can be deployed to different kinds of graph-form data. We are trying to identify other applications beyond chemistry or material science,” Guo says.\nIn the future, they also want to extend their current molecular grammar to include the 3D geometry of molecules and polymers, which is key to understanding the interactions between polymer chains. They are also developing an interface that would show a user the learned grammar production rules and solicit feedback to correct rules that may be wrong, boosting the accuracy of the system.\nThis work is funded, in part, by the MIT-IBM Watson AI Lab and its member company, Evonik.\n",
    "published": "2023-07-07",
    "timestamp": "2023-08-30T13:26:06.964372"
  },
  {
    "unique_id": "f7031fbd-8590-5c68-afd9-01cb58e953c0",
    "title": "MIT researchers combine deep learning and physics to fix motion-corrupted MRI scans",
    "description": "The challenge involves more than just a blurry JPEG. Fixing motion artifacts in medical imaging requires a more sophisticated approach.",
    "link": "https://news.mit.edu/2023/mit-researchers-combine-deep-learning-physics-fix-motion-corrupted-MRI-scans-0817",
    "blog_text": "Compared to other imaging modalities like X-rays or CT scans, MRI scans provide high-quality soft tissue contrast. Unfortunately, MRI is highly sensitive to motion, with even the smallest of movements resulting in image artifacts. These artifacts put patients at risk of misdiagnoses or inappropriate treatment when critical details are obscured from the physician. But researchers at MIT may have developed a deep learning model capable of motion correction in brain MRI.\n“Motion is a common problem in MRI,” explains Nalini Singh, an Abdul Latif Jameel Clinic for Machine Learning in Health (Jameel Clinic)-affiliated PhD student in the Harvard-MIT Program in Health Sciences and Technology (HST) and lead author of the paper. “It’s a pretty slow imaging modality.”\nMRI sessions can take anywhere from a few minutes to an hour, depending on the type of images required. Even during the shortest scans, small movements can have dramatic effects on the resulting image. Unlike camera imaging, where motion typically manifests as a localized blur, motion in MRI often results in artifacts that can corrupt the whole image. Patients may be anesthetized or requested to limit deep breathing in order to minimize motion. However, these measures often cannot be taken in populations particularly susceptible to motion, including children and patients with psychiatric disorders. \nThe paper, titled “Data Consistent Deep Rigid MRI Motion Correction,” was recently awarded best oral presentation at the Medical Imaging with Deep Learning conference (MIDL) in Nashville, Tennessee. The method computationally constructs a motion-free image from motion-corrupted data without changing anything about the scanning procedure. “Our aim was to combine physics-based modeling and deep learning to get the best of both worlds,” Singh says.\nThe importance of this combined approach lies within ensuring consistency between the image output and the actual measurements of what is being depicted, otherwise the model creates “hallucinations” — images that appear realistic, but are physically and spatially inaccurate, potentially worsening outcomes when it comes to diagnoses.\nProcuring an MRI free of motion artifacts, particularly from patients with neurological disorders that cause involuntary movement, such as Alzheimer’s or Parkinson’s disease, would benefit more than just patient outcomes. A study from the University of Washington Department of Radiology estimated that motion affects 15 percent of brain MRIs. Motion in all types of MRI that leads to repeated scans or imaging sessions to obtain images with sufficient quality for diagnosis results in approximately $115,000 in hospital expenditures per scanner on an annual basis.\nAccording to Singh, future work could explore more sophisticated types of head motion as well as motion in other body parts. For instance, fetal MRI suffers from rapid, unpredictable motion that cannot be modeled only by simple translations and rotations. \n“This line of work from Singh and company is the next step in MRI motion correction. Not only is it excellent research work, but I believe these methods will be used in all kinds of clinical cases: children and older folks who can't sit still in the scanner, pathologies which induce motion, studies of moving tissue, even healthy patients will move in the magnet,” says Daniel Moyer, an assistant professor at Vanderbilt University. “In the future, I think that it likely will be standard practice to process images with something directly descended from this research.”\nCo-authors of this paper include Nalini Singh, Neel Dey, Malte Hoffmann, Bruce Fischl, Elfar Adalsteinsson, Robert Frost, Adrian Dalca and Polina Golland. This research was supported in part by GE Healthcare and by computational hardware provided by the Massachusetts Life Sciences Center. The research team thanks Steve Cauley for helpful discussions. Additional support was provided by NIH NIBIB, NIA, NIMH, NINDS, the Blueprint for Neuroscience Research, part of the multi-institutional Human Connectome Project, the BRAIN Initiative Cell Census Network, and a Google PhD Fellowship.\n",
    "published": "2023-08-17",
    "timestamp": "2023-08-30T13:26:06.919370"
  }
];